{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CDA_env_disc_RLlib.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9Toun0Aj3tS",
        "colab_type": "code",
        "outputId": "96e3dfa2-f959-4621-b248-3cc51f3514b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "#!pip install ray[rllib]==0.7.3 # Originally developed with ver 0.7.3\n",
        "!pip install ray[rllib]\n",
        "#!pip install ray[debug]==0.7.3\n",
        "#!pip install ray[debug]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ray[rllib] in /usr/local/lib/python3.6/dist-packages (0.8.2)\n",
            "Requirement already satisfied: redis>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (3.4.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (1.17.5)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (3.6.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (3.6.2)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (0.3.3)\n",
            "Requirement already satisfied: funcsigs in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (1.0.2)\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (2.0.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (2.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (20.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (0.4.3)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (3.10.0)\n",
            "Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (1.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (3.0.12)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (1.27.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (7.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (3.13)\n",
            "Requirement already satisfied: tensorboardX; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (2.0)\n",
            "Requirement already satisfied: gym[atari]; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (0.15.6)\n",
            "Requirement already satisfied: lz4; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (3.0.2)\n",
            "Requirement already satisfied: scipy; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (1.4.1)\n",
            "Requirement already satisfied: tabulate; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (0.8.6)\n",
            "Requirement already satisfied: opencv-python-headless; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (4.2.0.32)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray[rllib]) (8.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->ray[rllib]) (45.1.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray[rllib]) (1.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->ray[rllib]) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray[rllib]) (19.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray[rllib]) (1.8.1)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]) (3.0.4)\n",
            "Requirement already satisfied: multidict<5.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]) (4.7.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]) (3.6.6)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]) (3.0.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]) (1.4.2)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]) (1.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google->ray[rllib]) (4.6.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->ray[rllib]) (2.4.6)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]; extra == \"rllib\"->ray[rllib]) (1.4.10)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]; extra == \"rllib\"->ray[rllib]) (4.1.2.30)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]; extra == \"rllib\"->ray[rllib]) (0.2.6)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]; extra == \"rllib\"->ray[rllib]) (6.2.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.6/dist-packages (from yarl<2.0,>=1.0->aiohttp->ray[rllib]) (2.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]; extra == \"rllib\"->ray[rllib]) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_GHZ31rWbYh",
        "colab_type": "code",
        "outputId": "cea90fce-c172-4f27-d225-2e0ad215610e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "!pip show tensorflow\n",
        "!pip show ray"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 1.15.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: opt-einsum, tensorflow-estimator, wheel, keras-preprocessing, numpy, six, keras-applications, google-pasta, grpcio, termcolor, absl-py, wrapt, gast, tensorboard, protobuf, astor\n",
            "Required-by: stable-baselines, magenta, fancyimpute\n",
            "Name: ray\n",
            "Version: 0.8.2\n",
            "Summary: A system for parallel and distributed Python that unifies the ML ecosystem.\n",
            "Home-page: https://github.com/ray-project/ray\n",
            "Author: Ray Team\n",
            "Author-email: ray-dev@googlegroups.com\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: pytest, filelock, numpy, click, redis, jsonschema, google, pyyaml, six, py-spy, packaging, protobuf, aiohttp, grpcio, funcsigs, colorama, cloudpickle\n",
            "Required-by: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAqVG2cqjLXM",
        "colab_type": "code",
        "outputId": "92cd3fbb-b0b9-44a6-e792-525947933752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Needed to switch directory in Google drive so as to import MARL env.\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd \"/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction/\"\n",
        "#%cd \"/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction/gym_continuousDoubleAuction/\"\n",
        "\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction\n",
            "/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UW3INjDipTC",
        "colab_type": "code",
        "outputId": "d5e4cce8-eb88-43a0-e323-aaf9ea1bff67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "#CDA_env_disc_RLlib\n",
        "\n",
        "# https://github.com/ray-project/ray/blob/master/python/ray/rllib/examples/multiagent_cartpole.py\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\"\"\"Simple example of setting up a multi-agent policy mapping.\n",
        "Control the number of agents and policies via --num-agents and --num-policies.\n",
        "This works with hundreds of agents and policies, but note that initializing\n",
        "many TF policies will take some time.\n",
        "Also, TF evals might slow down with large numbers of policies. To debug TF\n",
        "execution, set the TF_TIMELINE_DIR environment variable.\n",
        "\"\"\"\n",
        "\n",
        "# rllib rollout ~/ray_results/PPO/PPO_MMenv-v0_0_2019-08-09_00-06-10t4b1lscr/checkpoint-1 --run PPO --env MMenv-v0 --steps 100\n",
        "\n",
        "import os\n",
        "os.environ['RAY_DEBUG_DISABLE_MEMORY_MONITOR'] = \"True\"\n",
        "\n",
        "import argparse\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.models import Model, ModelCatalog\n",
        "\n",
        "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
        "from ray.rllib.models.tf.fcnet_v2 import FullyConnectedNetwork\n",
        "\n",
        "from ray.tune.registry import register_env\n",
        "from ray.rllib.utils import try_import_tf\n",
        "\n",
        "\n",
        "from ray.rllib.policy.policy import Policy\n",
        "#from ray.rllib.policy.tf_policy import TFPolicy\n",
        "#from ray.rllib.policy import Policy\n",
        "\n",
        "\n",
        "#from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
        "\n",
        "\n",
        "import sys\n",
        "if \"../\" not in sys.path:\n",
        "    sys.path.append(\"../\")\n",
        "#from exchg.x.y import z\n",
        "\n",
        "#from envs.continuousDoubleAuction_env import continuousDoubleAuctionEnv\n",
        "from gym_continuousDoubleAuction.envs.continuousDoubleAuction_env import continuousDoubleAuctionEnv\n",
        "\n",
        "tf = try_import_tf()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFWgG3bhiwux",
        "colab_type": "code",
        "outputId": "117b3325-f4be-43b4-bddd-e078cfbe961b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--num-agents\", type=int, default=4)\n",
        "parser.add_argument(\"--num-policies\", type=int, default=4)\n",
        "parser.add_argument(\"--num-iters\", type=int, default=10)\n",
        "parser.add_argument(\"--simple\", action=\"store_true\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreTrueAction(option_strings=['--simple'], dest='simple', nargs=0, const=True, default=False, type=None, choices=None, help=None, metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIYT1UTxiQcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomModel_disc(Model):\n",
        "    def _lstm(self, Inputs, cell_size):\n",
        "        s = tf.expand_dims(Inputs, axis=1, name='time_major')  # [time_step, feature] => [time_step, batch, feature]\n",
        "        lstm_cell = tf.nn.rnn_cell.LSTMCell(cell_size)\n",
        "        self.init_state = lstm_cell.zero_state(batch_size=1, dtype=tf.float32)\n",
        "        # time_major means [time_step, batch, feature] while batch major means [batch, time_step, feature]\n",
        "        outputs, self.final_state = tf.nn.dynamic_rnn(cell=lstm_cell, inputs=s, initial_state=self.init_state, time_major=True)\n",
        "        lstm_out = tf.reshape(outputs, [-1, cell_size], name='flatten_rnn_outputs')  # joined state representation\n",
        "        return lstm_out\n",
        "\n",
        "    def _build_layers_v2(self, input_dict, num_outputs, options):\n",
        "        hidden = 512\n",
        "        cell_size = 256\n",
        "        #S = input_dict[\"obs\"]\n",
        "        S = tf.layers.flatten(input_dict[\"obs\"])\n",
        "        with tf.variable_scope(tf.VariableScope(tf.AUTO_REUSE, \"shared\"),\n",
        "                               reuse=tf.AUTO_REUSE,\n",
        "                               auxiliary_name_scope=False):\n",
        "            last_layer = tf.layers.dense(S, hidden, activation=tf.nn.relu, name=\"fc1\")\n",
        "        last_layer = tf.layers.dense(last_layer, hidden, activation=tf.nn.relu, name=\"fc2\")\n",
        "        last_layer = tf.layers.dense(last_layer, hidden, activation=tf.nn.relu, name=\"fc3\")\n",
        "\n",
        "        last_layer = self._lstm(last_layer, cell_size)\n",
        "\n",
        "        output = tf.layers.dense(last_layer, num_outputs, activation=tf.nn.softmax, name=\"mu\")\n",
        "\n",
        "        return output, last_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56uO7LyPMQ4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomModel(TFModelV2):\n",
        "    \"\"\"Example of a custom model that just delegates to a fc-net.\"\"\"\n",
        "\n",
        "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
        "        \n",
        "        super(CustomModel, self).__init__(obs_space, action_space, num_outputs, model_config, name)\n",
        "        \n",
        "        self.model = FullyConnectedNetwork(obs_space, action_space, num_outputs, model_config, name)\n",
        "        #print('obs_space', obs_space)\n",
        "\n",
        "        self.register_variables(self.model.variables())\n",
        "\n",
        "    def forward(self, input_dict, state, seq_lens):\n",
        "        return self.model.forward(input_dict, state, seq_lens)\n",
        "\n",
        "    def value_function(self):\n",
        "        return self.model.value_function()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37-b4rcBdg6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyKerasModel(TFModelV2):\n",
        "    \"\"\"Custom model for policy gradient algorithms.\"\"\"\n",
        "\n",
        "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
        "                 name):\n",
        "        super(MyKerasModel, self).__init__(obs_space, action_space,\n",
        "                                           num_outputs, model_config, name)\n",
        "        self.inputs = tf.keras.layers.Input(\n",
        "            shape=obs_space.shape, name=\"observations\")\n",
        "        layer_1 = tf.keras.layers.Dense(\n",
        "            128,\n",
        "            name=\"my_layer1\",\n",
        "            activation=tf.nn.relu,\n",
        "            kernel_initializer=normc_initializer(1.0))(self.inputs)\n",
        "        layer_out = tf.keras.layers.Dense(\n",
        "            num_outputs,\n",
        "            name=\"my_out\",\n",
        "            activation=None,\n",
        "            kernel_initializer=normc_initializer(0.01))(layer_1)\n",
        "        value_out = tf.keras.layers.Dense(\n",
        "            1,\n",
        "            name=\"value_out\",\n",
        "            activation=None,\n",
        "            kernel_initializer=normc_initializer(0.01))(layer_1)\n",
        "        self.base_model = tf.keras.Model(self.inputs, [layer_out, value_out])\n",
        "        self.register_variables(self.base_model.variables)\n",
        "\n",
        "    def forward(self, input_dict, state, seq_lens):\n",
        "        model_out, self._value_out = self.base_model(input_dict[\"obs\"])\n",
        "        return model_out, state\n",
        "\n",
        "    def value_function(self):\n",
        "        return tf.reshape(self._value_out, [-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9ET0_vWehmt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FastModel(Model):\n",
        "    def _build_layers_v2(self, input_dict, num_outputs, options):\n",
        "        bias = tf.get_variable(\n",
        "            dtype=tf.float32,\n",
        "            name=\"bias\",\n",
        "            initializer=tf.zeros_initializer,\n",
        "            shape=())\n",
        "        output = bias + tf.zeros([tf.shape(input_dict[\"obs\"])[0], num_outputs])\n",
        "        return output, output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cijiMv-ti1SK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_RandomPolicy(_seed):\n",
        "\n",
        "    # a hand-coded policy that acts at random in the env (doesn't learn)\n",
        "    class RandomPolicy(Policy):\n",
        "        \"\"\"Hand-coded policy that returns random actions.\"\"\"\n",
        "        def __init__(self, observation_space, action_space, config):\n",
        "            self.observation_space = observation_space\n",
        "            self.action_space = action_space\n",
        "            self.action_space.seed(_seed)\n",
        "\n",
        "        def compute_actions(self,\n",
        "                            obs_batch,\n",
        "                            state_batches,\n",
        "                            prev_action_batch=None,\n",
        "                            prev_reward_batch=None,\n",
        "                            info_batch=None,\n",
        "                            episodes=None,\n",
        "                            **kwargs):\n",
        "            \"\"\"Compute actions on a batch of observations.\"\"\"\n",
        "            return [self.action_space.sample() for _ in obs_batch], [], {}\n",
        "\n",
        "        def learn_on_batch(self, samples):\n",
        "            \"\"\"No learning.\"\"\"\n",
        "            #return {}\n",
        "            pass\n",
        "\n",
        "        def get_weights(self):\n",
        "            pass\n",
        "\n",
        "        def set_weights(self, weights):\n",
        "            pass\n",
        "\n",
        "    return RandomPolicy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2cbF3Inoo2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_agents = 4\n",
        "num_policies = 4\n",
        "num_iters = 3\n",
        "simple = False#store_true"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To995IZanbGx",
        "colab_type": "code",
        "outputId": "c59e47d6-c594-424e-c5d3-464e3fc26e0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "source": [
        "#args = parser.parse_args()\n",
        "\n",
        "#ray.init(ignore_reinit_error=True)\n",
        "#ray.init(ignore_reinit_error=True, num_cpus=2)\n",
        "#ray.init(ignore_reinit_error=True, webui_host='127.0.0.1', num_cpus=2)\n",
        "ray.init(ignore_reinit_error=True, log_to_driver=False, webui_host='127.0.0.1', num_cpus=2)\n",
        "\n",
        "#num_of_traders = args.num_agents\n",
        "num_of_traders = num_agents\n",
        "tape_display_length = 100\n",
        "tick_size = 1\n",
        "init_cash = 1000000\n",
        "max_step = 700 # per episode\n",
        "episode = 9\n",
        "CDA_env = continuousDoubleAuctionEnv(num_of_traders, init_cash, tick_size, tape_display_length, max_step)\n",
        "print('CDA_env:', CDA_env.print_accs())\n",
        "register_env(\"continuousDoubleAuction-v0\", lambda _: continuousDoubleAuctionEnv(num_of_traders, init_cash, tick_size, tape_display_length, max_step))\n",
        "\n",
        "ModelCatalog.register_custom_model(\"model_disc\", CustomModel_disc)\n",
        "#ModelCatalog.register_custom_model(\"model_disc\", CustomModel)\n",
        "#ModelCatalog.register_custom_model(\"model_disc\", MyKerasModel)\n",
        "#ModelCatalog.register_custom_model(\"model_disc\", FastModel)\n",
        "\n",
        "obs_space = CDA_env.observation_space\n",
        "act_space = CDA_env.action_space"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-02-29 19:23:54,331\tWARNING services.py:586 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
            "2020-02-29 19:23:54,334\tINFO resource_spec.py:212 -- Starting Ray with 6.74 GiB memory available for workers and up to 3.37 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
            "2020-02-29 19:23:54,981\tINFO services.py:1078 -- View the Ray dashboard at \u001b[1m\u001b[32m127.0.0.1:8265\u001b[39m\u001b[22m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ID: 0\n",
            "cash: 1000000\n",
            "cash_on_hold: 0\n",
            "position_val: 0\n",
            "prev_nav: 1000000\n",
            "nav: 1000000\n",
            "net_position: 0\n",
            "VWAP: 0\n",
            "profit from trade(tick): 0\n",
            "total_profit: 0\n",
            "num_trades: 0\n",
            "ID: 1\n",
            "cash: 1000000\n",
            "cash_on_hold: 0\n",
            "position_val: 0\n",
            "prev_nav: 1000000\n",
            "nav: 1000000\n",
            "net_position: 0\n",
            "VWAP: 0\n",
            "profit from trade(tick): 0\n",
            "total_profit: 0\n",
            "num_trades: 0\n",
            "ID: 2\n",
            "cash: 1000000\n",
            "cash_on_hold: 0\n",
            "position_val: 0\n",
            "prev_nav: 1000000\n",
            "nav: 1000000\n",
            "net_position: 0\n",
            "VWAP: 0\n",
            "profit from trade(tick): 0\n",
            "total_profit: 0\n",
            "num_trades: 0\n",
            "ID: 3\n",
            "cash: 1000000\n",
            "cash_on_hold: 0\n",
            "position_val: 0\n",
            "prev_nav: 1000000\n",
            "nav: 1000000\n",
            "net_position: 0\n",
            "VWAP: 0\n",
            "profit from trade(tick): 0\n",
            "total_profit: 0\n",
            "num_trades: 0\n",
            "CDA_env: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngepKdXqVZL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Each policy can have a different configuration (including custom model)\n",
        "def gen_policy(i):\n",
        "    config = {\"model\": {\"custom_model\": \"model_disc\"},\n",
        "              \"gamma\": 0.99,}\n",
        "    return (None, obs_space, act_space, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5rZxJ--ndCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def policy_mapper(agent_id):\n",
        "    if agent_id == 0:\n",
        "        return \"policy_0\" # PPO\n",
        "    elif agent_id == 1:\n",
        "        return \"policy_1\" # RandomPolicy\n",
        "    elif agent_id == 2:\n",
        "        return \"policy_2\" # RandomPolicy\n",
        "    else:\n",
        "        return \"policy_3\" # RandomPolicy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQviAEDfgPzq",
        "colab_type": "code",
        "outputId": "3cf51d68-71bd-4fbb-955a-14d499060a7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Setup PPO with an ensemble of `num_policies` different policies\n",
        "\n",
        "# Dictionary of policies\n",
        "#policies = {\"policy_{}\".format(i): gen_policy(i) for i in range(args.num_policies)}\n",
        "policies = {\"policy_{}\".format(i): gen_policy(i) for i in range(num_policies)}\n",
        "\n",
        "# override policy with random policy\n",
        "#policies[\"policy_{}\".format(args.num_policies-3)] = (make_RandomPolicy(1), obs_space, act_space, {}) # random policy stored as the last item in policies dictionary\n",
        "#policies[\"policy_{}\".format(args.num_policies-2)] = (make_RandomPolicy(2), obs_space, act_space, {}) # random policy stored as the last item in policies dictionary\n",
        "#policies[\"policy_{}\".format(args.num_policies-1)] = (make_RandomPolicy(3), obs_space, act_space, {}) # random policy stored as the last item in policies dictionary\n",
        "policies[\"policy_{}\".format(num_policies-3)] = (make_RandomPolicy(1), obs_space, act_space, {}) # random policy stored as the last item in policies dictionary\n",
        "policies[\"policy_{}\".format(num_policies-2)] = (make_RandomPolicy(2), obs_space, act_space, {}) # random policy stored as the last item in policies dictionary\n",
        "policies[\"policy_{}\".format(num_policies-1)] = (make_RandomPolicy(3), obs_space, act_space, {}) # random policy stored as the last item in policies dictionary\n",
        "\n",
        "print('policies:', policies)\n",
        "\n",
        "policy_ids = list(policies.keys())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "policies: {'policy_0': (None, Box(4, 10), Tuple(Discrete(3), Discrete(4), Discrete(100), Discrete(12)), {'model': {'custom_model': 'model_disc'}, 'gamma': 0.99}), 'policy_1': (<class '__main__.make_RandomPolicy.<locals>.RandomPolicy'>, Box(4, 10), Tuple(Discrete(3), Discrete(4), Discrete(100), Discrete(12)), {}), 'policy_2': (<class '__main__.make_RandomPolicy.<locals>.RandomPolicy'>, Box(4, 10), Tuple(Discrete(3), Discrete(4), Discrete(100), Discrete(12)), {}), 'policy_3': (<class '__main__.make_RandomPolicy.<locals>.RandomPolicy'>, Box(4, 10), Tuple(Discrete(3), Discrete(4), Discrete(100), Discrete(12)), {})}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVr7hhEni7u7",
        "colab_type": "code",
        "outputId": "92a96ea6-a1c8-46cf-dcce-c54d23d3dd09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tune.run(#PPOTrainer,\n",
        "          \"PPO\",\n",
        "          #\"PG\",\n",
        "          #queue_trials=False,\n",
        "          #resources_per_trial={\"cpu\": 2,\n",
        "          #                     \"gpu\": 0},\n",
        "         \n",
        "          #stop={\"training_iteration\": args.num_iters},\n",
        "          #stop={\"timesteps_total\": max_step,\n",
        "          #      \"training_iteration\": num_iters},\n",
        "          stop={\"timesteps_total\": max_step * episode},\n",
        "         \n",
        "          config={\"env\": \"continuousDoubleAuction-v0\",\n",
        "                  \n",
        "                  #\"log_level\": \"DEBUG\",\n",
        "                  #\"simple_optimizer\": args.simple,\n",
        "                  #\"simple_optimizer\": True,\n",
        "                  #\"num_sgd_iter\": 10,\n",
        "                  \n",
        "                  #\"gamma\": 0.9,\n",
        "\n",
        "                  # Number of rollout worker actors to create for parallel sampling.                   \n",
        "                  # Setting to 0 will force rollouts to be done in the trainer actor.\n",
        "                  \"num_workers\": 1, # Colab (only 2 CPUs or 1 GPU)                  \n",
        "                  \"num_envs_per_worker\": 2, #4\n",
        "\n",
        "                  #\"timesteps_per_iteration\": max_step,\n",
        "\n",
        "                  # https://github.com/ray-project/ray/issues/4628\n",
        "                  \"sample_batch_size\": 32, # number of environment steps sampled from each environment\n",
        "                  \"train_batch_size\": 128, # minibatch size must be >= 128, number of environment steps sampled from all available environments\n",
        "                                    \n",
        "                  \"multiagent\": {\"policies_to_train\": [\"policy_0\"],\n",
        "                                 \"policies\": policies,\n",
        "                                 #\"policy_mapping_fn\": tune.function(lambda agent_id: random.choice(policy_ids)),\n",
        "                                 #\"policy_mapping_fn\": tune.function(policy_mapper),\n",
        "                                 \"policy_mapping_fn\": policy_mapper,\n",
        "                                },                                 \n",
        "                  },\n",
        "          )\n",
        "\n",
        "#ray.shutdown()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 1.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc  </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>     </td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-24-34\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 15971.467\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.20000000298023224\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.564391136169434\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.010105578228831291\n",
            "        policy_loss: -0.05712352693080902\n",
            "        total_loss: 389637.3125\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 389637.34375\n",
            "    load_time_ms: 148.68\n",
            "    num_steps_sampled: 128\n",
            "    num_steps_trained: 128\n",
            "    sample_time_ms: 884.733\n",
            "    update_time_ms: 2026.962\n",
            "  iterations_since_restore: 1\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.70000000000002\n",
            "    ram_util_percent: 17.685714285714283\n",
            "  pid: 12191\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 19.189515590667725\n",
            "  time_this_iter_s: 19.189515590667725\n",
            "  time_total_s: 19.189515590667725\n",
            "  timestamp: 1583004274\n",
            "  timesteps_since_restore: 128\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 128\n",
            "  training_iteration: 1\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         19.1895</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">     1</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-24-50\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 15407.081\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.20000000298023224\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.554363250732422\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.01714286580681801\n",
            "        policy_loss: -0.05367262661457062\n",
            "        total_loss: 4390097.5\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 4390097.0\n",
            "    load_time_ms: 74.942\n",
            "    num_steps_sampled: 256\n",
            "    num_steps_trained: 256\n",
            "    sample_time_ms: 803.601\n",
            "    update_time_ms: 1029.599\n",
            "  iterations_since_restore: 2\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.31739130434785\n",
            "    ram_util_percent: 17.800000000000004\n",
            "  pid: 12191\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 34.79183292388916\n",
            "  time_this_iter_s: 15.602317333221436\n",
            "  time_total_s: 34.79183292388916\n",
            "  timestamp: 1583004290\n",
            "  timesteps_since_restore: 256\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 256\n",
            "  training_iteration: 2\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         34.7918</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">     2</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-25-06\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 15282.836\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.20000000298023224\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.56192398071289\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.007960289716720581\n",
            "        policy_loss: -0.05319364741444588\n",
            "        total_loss: 1505663.375\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 1505663.375\n",
            "    load_time_ms: 50.303\n",
            "    num_steps_sampled: 384\n",
            "    num_steps_trained: 384\n",
            "    sample_time_ms: 775.078\n",
            "    update_time_ms: 694.827\n",
            "  iterations_since_restore: 3\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.7\n",
            "    ram_util_percent: 17.89999999999999\n",
            "  pid: 12191\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 50.57471942901611\n",
            "  time_this_iter_s: 15.782886505126953\n",
            "  time_total_s: 50.57471942901611\n",
            "  timestamp: 1583004306\n",
            "  timesteps_since_restore: 384\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 384\n",
            "  training_iteration: 3\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         50.5747</td><td style=\"text-align: right;\"> 384</td><td style=\"text-align: right;\">     3</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-25-22\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 15186.904\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.20000000298023224\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.568567276000977\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.003529150504618883\n",
            "        policy_loss: -0.054904304444789886\n",
            "        total_loss: 40027288.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 40027288.0\n",
            "    load_time_ms: 37.965\n",
            "    num_steps_sampled: 512\n",
            "    num_steps_trained: 512\n",
            "    sample_time_ms: 752.31\n",
            "    update_time_ms: 527.819\n",
            "  iterations_since_restore: 4\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.45454545454545\n",
            "    ram_util_percent: 17.995454545454546\n",
            "  pid: 12191\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 66.1896870136261\n",
            "  time_this_iter_s: 15.614967584609985\n",
            "  time_total_s: 66.1896870136261\n",
            "  timestamp: 1583004322\n",
            "  timesteps_since_restore: 512\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 512\n",
            "  training_iteration: 4\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         66.1897</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">     4</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-25-37\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 15147.721\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.10000000149011612\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.565532684326172\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.007386608514934778\n",
            "        policy_loss: -0.05016940087080002\n",
            "        total_loss: 21862318.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 21862318.0\n",
            "    load_time_ms: 30.702\n",
            "    num_steps_sampled: 640\n",
            "    num_steps_trained: 640\n",
            "    sample_time_ms: 746.077\n",
            "    update_time_ms: 426.948\n",
            "  iterations_since_restore: 5\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.55217391304348\n",
            "    ram_util_percent: 18.0\n",
            "  pid: 12191\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 81.93097424507141\n",
            "  time_this_iter_s: 15.741287231445312\n",
            "  time_total_s: 81.93097424507141\n",
            "  timestamp: 1583004337\n",
            "  timesteps_since_restore: 640\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 640\n",
            "  training_iteration: 5\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">          81.931</td><td style=\"text-align: right;\"> 640</td><td style=\"text-align: right;\">     5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-25-53\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 15125.767\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.10000000149011612\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.544785499572754\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.025905989110469818\n",
            "        policy_loss: -0.12051631510257721\n",
            "        total_loss: 2855499.5\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 2855499.5\n",
            "    load_time_ms: 25.754\n",
            "    num_steps_sampled: 768\n",
            "    num_steps_trained: 768\n",
            "    sample_time_ms: 763.462\n",
            "    update_time_ms: 360.481\n",
            "  iterations_since_restore: 6\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 97.11304347826086\n",
            "    ram_util_percent: 18.10000000000001\n",
            "  pid: 12191\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 97.83156514167786\n",
            "  time_this_iter_s: 15.900590896606445\n",
            "  time_total_s: 97.83156514167786\n",
            "  timestamp: 1583004353\n",
            "  timesteps_since_restore: 768\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 768\n",
            "  training_iteration: 6\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         97.8316</td><td style=\"text-align: right;\"> 768</td><td style=\"text-align: right;\">     6</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-26-11\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 15346.719\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.15000000596046448\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.53975772857666\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.026583898812532425\n",
            "        policy_loss: -0.13854968547821045\n",
            "        total_loss: 6834028.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 6834028.0\n",
            "    load_time_ms: 22.223\n",
            "    num_steps_sampled: 896\n",
            "    num_steps_trained: 896\n",
            "    sample_time_ms: 760.379\n",
            "    update_time_ms: 314.502\n",
            "  iterations_since_restore: 7\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 97.036\n",
            "    ram_util_percent: 18.2\n",
            "  pid: 12191\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 115.29195928573608\n",
            "  time_this_iter_s: 17.460394144058228\n",
            "  time_total_s: 115.29195928573608\n",
            "  timestamp: 1583004371\n",
            "  timesteps_since_restore: 896\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 896\n",
            "  training_iteration: 7\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         115.292</td><td style=\"text-align: right;\"> 896</td><td style=\"text-align: right;\">     7</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-26-27\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 15297.906\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.22499999403953552\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.544628143310547\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.016027841717004776\n",
            "        policy_loss: -0.09708276391029358\n",
            "        total_loss: 14565382.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 14565381.0\n",
            "    load_time_ms: 19.594\n",
            "    num_steps_sampled: 1024\n",
            "    num_steps_trained: 1024\n",
            "    sample_time_ms: 754.095\n",
            "    update_time_ms: 280.168\n",
            "  iterations_since_restore: 8\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.5590909090909\n",
            "    ram_util_percent: 18.300000000000004\n",
            "  pid: 12191\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 131.00495052337646\n",
            "  time_this_iter_s: 15.71299123764038\n",
            "  time_total_s: 131.00495052337646\n",
            "  timestamp: 1583004387\n",
            "  timesteps_since_restore: 1024\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1024\n",
            "  training_iteration: 8\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         131.005</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">     8</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-26-42\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 15280.585\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.22499999403953552\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.540206909179688\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.023744748905301094\n",
            "        policy_loss: -0.10929825901985168\n",
            "        total_loss: 7821138.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 7821138.0\n",
            "    load_time_ms: 17.546\n",
            "    num_steps_sampled: 1152\n",
            "    num_steps_trained: 1152\n",
            "    sample_time_ms: 749.837\n",
            "    update_time_ms: 252.057\n",
            "  iterations_since_restore: 9\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.52173913043477\n",
            "    ram_util_percent: 18.39999999999999\n",
            "  pid: 12191\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 146.89583587646484\n",
            "  time_this_iter_s: 15.890885353088379\n",
            "  time_total_s: 146.89583587646484\n",
            "  timestamp: 1583004402\n",
            "  timesteps_since_restore: 1152\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1152\n",
            "  training_iteration: 9\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         146.896</td><td style=\"text-align: right;\">1152</td><td style=\"text-align: right;\">     9</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-26-58\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 15267.605\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.3375000059604645\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.542168617248535\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.021723398938775063\n",
            "        policy_loss: -0.10712334513664246\n",
            "        total_loss: 80612304.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 80612304.0\n",
            "    load_time_ms: 15.911\n",
            "    num_steps_sampled: 1280\n",
            "    num_steps_trained: 1280\n",
            "    sample_time_ms: 747.608\n",
            "    update_time_ms: 230.233\n",
            "  iterations_since_restore: 10\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.70869565217392\n",
            "    ram_util_percent: 18.5\n",
            "  pid: 12191\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 162.81327867507935\n",
            "  time_this_iter_s: 15.917442798614502\n",
            "  time_total_s: 162.81327867507935\n",
            "  timestamp: 1583004418\n",
            "  timesteps_since_restore: 1280\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1280\n",
            "  training_iteration: 10\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         162.813</td><td style=\"text-align: right;\">1280</td><td style=\"text-align: right;\">    10</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-27-14\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 2\n",
            "  episodes_total: 2\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 15175.21\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.5062500238418579\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.555044174194336\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.011898262426257133\n",
            "        policy_loss: -0.06856460869312286\n",
            "        total_loss: 70933720.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 70933720.0\n",
            "    load_time_ms: 1.241\n",
            "    num_steps_sampled: 1408\n",
            "    num_steps_trained: 1408\n",
            "    sample_time_ms: 734.97\n",
            "    update_time_ms: 30.261\n",
            "  iterations_since_restore: 11\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.39090909090909\n",
            "    ram_util_percent: 18.60000000000001\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 9275.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 377.0\n",
            "    policy_3: -30.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 7466.0\n",
            "    policy_1: -6040.0\n",
            "    policy_2: -198.5\n",
            "    policy_3: -1227.5\n",
            "  policy_reward_min:\n",
            "    policy_0: 5657.0\n",
            "    policy_1: -7227.0\n",
            "    policy_2: -774.0\n",
            "    policy_3: -2425.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.101005527144628\n",
            "    mean_inference_ms: 3.1049353011110994\n",
            "    mean_processing_ms: 1.1031877909991759\n",
            "  time_since_restore: 178.6531946659088\n",
            "  time_this_iter_s: 15.839915990829468\n",
            "  time_total_s: 178.6531946659088\n",
            "  timestamp: 1583004434\n",
            "  timesteps_since_restore: 1408\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1408\n",
            "  training_iteration: 11\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         178.653</td><td style=\"text-align: right;\">1408</td><td style=\"text-align: right;\">    11</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-27-30\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 15194.217\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.5062500238418579\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.559427261352539\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.009522691369056702\n",
            "        policy_loss: -0.0817519947886467\n",
            "        total_loss: 1967355.25\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 1967355.5\n",
            "    load_time_ms: 1.258\n",
            "    num_steps_sampled: 1536\n",
            "    num_steps_trained: 1536\n",
            "    sample_time_ms: 730.079\n",
            "    update_time_ms: 30.024\n",
            "  iterations_since_restore: 12\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.66521739130434\n",
            "    ram_util_percent: 18.60000000000001\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 9275.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 377.0\n",
            "    policy_3: -30.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 7466.0\n",
            "    policy_1: -6040.0\n",
            "    policy_2: -198.5\n",
            "    policy_3: -1227.5\n",
            "  policy_reward_min:\n",
            "    policy_0: 5657.0\n",
            "    policy_1: -7227.0\n",
            "    policy_2: -774.0\n",
            "    policy_3: -2425.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.101005527144628\n",
            "    mean_inference_ms: 3.1049353011110994\n",
            "    mean_processing_ms: 1.1031877909991759\n",
            "  time_since_restore: 194.3971939086914\n",
            "  time_this_iter_s: 15.743999242782593\n",
            "  time_total_s: 194.3971939086914\n",
            "  timestamp: 1583004450\n",
            "  timesteps_since_restore: 1536\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1536\n",
            "  training_iteration: 12\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         194.397</td><td style=\"text-align: right;\">1536</td><td style=\"text-align: right;\">    12</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-27-46\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 15206.633\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.5062500238418579\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.54718017578125\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.01619338057935238\n",
            "        policy_loss: -0.10483800619840622\n",
            "        total_loss: 3169794.5\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 3169794.5\n",
            "    load_time_ms: 1.271\n",
            "    num_steps_sampled: 1664\n",
            "    num_steps_trained: 1664\n",
            "    sample_time_ms: 732.808\n",
            "    update_time_ms: 31.57\n",
            "  iterations_since_restore: 13\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.91739130434784\n",
            "    ram_util_percent: 18.699999999999996\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 9275.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 377.0\n",
            "    policy_3: -30.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 7466.0\n",
            "    policy_1: -6040.0\n",
            "    policy_2: -198.5\n",
            "    policy_3: -1227.5\n",
            "  policy_reward_min:\n",
            "    policy_0: 5657.0\n",
            "    policy_1: -7227.0\n",
            "    policy_2: -774.0\n",
            "    policy_3: -2425.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.101005527144628\n",
            "    mean_inference_ms: 3.1049353011110994\n",
            "    mean_processing_ms: 1.1031877909991759\n",
            "  time_since_restore: 210.34689736366272\n",
            "  time_this_iter_s: 15.949703454971313\n",
            "  time_total_s: 210.34689736366272\n",
            "  timestamp: 1583004466\n",
            "  timesteps_since_restore: 1664\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1664\n",
            "  training_iteration: 13\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         210.347</td><td style=\"text-align: right;\">1664</td><td style=\"text-align: right;\">    13</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-28-02\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 15231.636\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.5062500238418579\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.546603202819824\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.013653052970767021\n",
            "        policy_loss: -0.08352692425251007\n",
            "        total_loss: 7715961.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 7715961.0\n",
            "    load_time_ms: 1.312\n",
            "    num_steps_sampled: 1792\n",
            "    num_steps_trained: 1792\n",
            "    sample_time_ms: 742.675\n",
            "    update_time_ms: 31.461\n",
            "  iterations_since_restore: 14\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.42608695652173\n",
            "    ram_util_percent: 18.800000000000004\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 9275.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 377.0\n",
            "    policy_3: -30.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 7466.0\n",
            "    policy_1: -6040.0\n",
            "    policy_2: -198.5\n",
            "    policy_3: -1227.5\n",
            "  policy_reward_min:\n",
            "    policy_0: 5657.0\n",
            "    policy_1: -7227.0\n",
            "    policy_2: -774.0\n",
            "    policy_3: -2425.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.101005527144628\n",
            "    mean_inference_ms: 3.1049353011110994\n",
            "    mean_processing_ms: 1.1031877909991759\n",
            "  time_since_restore: 226.30979442596436\n",
            "  time_this_iter_s: 15.962897062301636\n",
            "  time_total_s: 226.30979442596436\n",
            "  timestamp: 1583004482\n",
            "  timesteps_since_restore: 1792\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1792\n",
            "  training_iteration: 14\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">          226.31</td><td style=\"text-align: right;\">1792</td><td style=\"text-align: right;\">    14</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-28-18\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 15222.103\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.5062500238418579\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.535966873168945\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.01594112068414688\n",
            "        policy_loss: -0.09020920097827911\n",
            "        total_loss: 68511272.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 68511272.0\n",
            "    load_time_ms: 1.278\n",
            "    num_steps_sampled: 1920\n",
            "    num_steps_trained: 1920\n",
            "    sample_time_ms: 756.906\n",
            "    update_time_ms: 33.03\n",
            "  iterations_since_restore: 15\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 97.05\n",
            "    ram_util_percent: 18.89999999999999\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 9275.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 377.0\n",
            "    policy_3: -30.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 7466.0\n",
            "    policy_1: -6040.0\n",
            "    policy_2: -198.5\n",
            "    policy_3: -1227.5\n",
            "  policy_reward_min:\n",
            "    policy_0: 5657.0\n",
            "    policy_1: -7227.0\n",
            "    policy_2: -774.0\n",
            "    policy_3: -2425.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.101005527144628\n",
            "    mean_inference_ms: 3.1049353011110994\n",
            "    mean_processing_ms: 1.1031877909991759\n",
            "  time_since_restore: 242.11406254768372\n",
            "  time_this_iter_s: 15.80426812171936\n",
            "  time_total_s: 242.11406254768372\n",
            "  timestamp: 1583004498\n",
            "  timesteps_since_restore: 1920\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1920\n",
            "  training_iteration: 15\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         242.114</td><td style=\"text-align: right;\">1920</td><td style=\"text-align: right;\">    15</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-28-34\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 15238.766\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.5062500238418579\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.56242561340332\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.009774105623364449\n",
            "        policy_loss: -0.07475034147500992\n",
            "        total_loss: 163072736.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 163072736.0\n",
            "    load_time_ms: 1.311\n",
            "    num_steps_sampled: 2048\n",
            "    num_steps_trained: 2048\n",
            "    sample_time_ms: 743.663\n",
            "    update_time_ms: 33.074\n",
            "  iterations_since_restore: 16\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.7695652173913\n",
            "    ram_util_percent: 19.0\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 9275.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 377.0\n",
            "    policy_3: -30.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 7466.0\n",
            "    policy_1: -6040.0\n",
            "    policy_2: -198.5\n",
            "    policy_3: -1227.5\n",
            "  policy_reward_min:\n",
            "    policy_0: 5657.0\n",
            "    policy_1: -7227.0\n",
            "    policy_2: -774.0\n",
            "    policy_3: -2425.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.101005527144628\n",
            "    mean_inference_ms: 3.1049353011110994\n",
            "    mean_processing_ms: 1.1031877909991759\n",
            "  time_since_restore: 258.05087971687317\n",
            "  time_this_iter_s: 15.936817169189453\n",
            "  time_total_s: 258.05087971687317\n",
            "  timestamp: 1583004514\n",
            "  timesteps_since_restore: 2048\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2048\n",
            "  training_iteration: 16\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         258.051</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">    16</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-28-50\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 15061.38\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.5062500238418579\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.547430992126465\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.017790265381336212\n",
            "        policy_loss: -0.10576411336660385\n",
            "        total_loss: 203351840.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 203351840.0\n",
            "    load_time_ms: 1.311\n",
            "    num_steps_sampled: 2176\n",
            "    num_steps_trained: 2176\n",
            "    sample_time_ms: 744.133\n",
            "    update_time_ms: 31.921\n",
            "  iterations_since_restore: 17\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.11818181818181\n",
            "    ram_util_percent: 19.0\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 9275.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 377.0\n",
            "    policy_3: -30.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 7466.0\n",
            "    policy_1: -6040.0\n",
            "    policy_2: -198.5\n",
            "    policy_3: -1227.5\n",
            "  policy_reward_min:\n",
            "    policy_0: 5657.0\n",
            "    policy_1: -7227.0\n",
            "    policy_2: -774.0\n",
            "    policy_3: -2425.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.101005527144628\n",
            "    mean_inference_ms: 3.1049353011110994\n",
            "    mean_processing_ms: 1.1031877909991759\n",
            "  time_since_restore: 273.7293577194214\n",
            "  time_this_iter_s: 15.678478002548218\n",
            "  time_total_s: 273.7293577194214\n",
            "  timestamp: 1583004530\n",
            "  timesteps_since_restore: 2176\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2176\n",
            "  training_iteration: 17\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         273.729</td><td style=\"text-align: right;\">2176</td><td style=\"text-align: right;\">    17</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-29-05\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 15038.217\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.5062500238418579\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.546329498291016\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.017361009493470192\n",
            "        policy_loss: -0.09717047959566116\n",
            "        total_loss: 48656824.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 48656824.0\n",
            "    load_time_ms: 1.342\n",
            "    num_steps_sampled: 2304\n",
            "    num_steps_trained: 2304\n",
            "    sample_time_ms: 751.269\n",
            "    update_time_ms: 31.492\n",
            "  iterations_since_restore: 18\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.48695652173912\n",
            "    ram_util_percent: 19.10000000000001\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 9275.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 377.0\n",
            "    policy_3: -30.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 7466.0\n",
            "    policy_1: -6040.0\n",
            "    policy_2: -198.5\n",
            "    policy_3: -1227.5\n",
            "  policy_reward_min:\n",
            "    policy_0: 5657.0\n",
            "    policy_1: -7227.0\n",
            "    policy_2: -774.0\n",
            "    policy_3: -2425.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.101005527144628\n",
            "    mean_inference_ms: 3.1049353011110994\n",
            "    mean_processing_ms: 1.1031877909991759\n",
            "  time_since_restore: 289.2780747413635\n",
            "  time_this_iter_s: 15.548717021942139\n",
            "  time_total_s: 289.2780747413635\n",
            "  timestamp: 1583004545\n",
            "  timesteps_since_restore: 2304\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2304\n",
            "  training_iteration: 18\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         289.278</td><td style=\"text-align: right;\">2304</td><td style=\"text-align: right;\">    18</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-29-21\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 15027.669\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.5062500238418579\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.523727416992188\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.017220165580511093\n",
            "        policy_loss: -0.09951585531234741\n",
            "        total_loss: 178126240.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 178126240.0\n",
            "    load_time_ms: 1.344\n",
            "    num_steps_sampled: 2432\n",
            "    num_steps_trained: 2432\n",
            "    sample_time_ms: 768.726\n",
            "    update_time_ms: 33.563\n",
            "  iterations_since_restore: 19\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.67727272727271\n",
            "    ram_util_percent: 19.199999999999996\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 9275.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 377.0\n",
            "    policy_3: -30.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 7466.0\n",
            "    policy_1: -6040.0\n",
            "    policy_2: -198.5\n",
            "    policy_3: -1227.5\n",
            "  policy_reward_min:\n",
            "    policy_0: 5657.0\n",
            "    policy_1: -7227.0\n",
            "    policy_2: -774.0\n",
            "    policy_3: -2425.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.101005527144628\n",
            "    mean_inference_ms: 3.1049353011110994\n",
            "    mean_processing_ms: 1.1031877909991759\n",
            "  time_since_restore: 305.2590265274048\n",
            "  time_this_iter_s: 15.98095178604126\n",
            "  time_total_s: 305.2590265274048\n",
            "  timestamp: 1583004561\n",
            "  timesteps_since_restore: 2432\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2432\n",
            "  training_iteration: 19\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         305.259</td><td style=\"text-align: right;\">2432</td><td style=\"text-align: right;\">    19</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-29-37\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 15021.518\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.5062500238418579\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.532682418823242\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.019240055233240128\n",
            "        policy_loss: -0.102362260222435\n",
            "        total_loss: 88432248.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 88432248.0\n",
            "    load_time_ms: 1.327\n",
            "    num_steps_sampled: 2560\n",
            "    num_steps_trained: 2560\n",
            "    sample_time_ms: 771.243\n",
            "    update_time_ms: 33.315\n",
            "  iterations_since_restore: 20\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.08260869565218\n",
            "    ram_util_percent: 19.295652173913048\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 9275.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 377.0\n",
            "    policy_3: -30.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 7466.0\n",
            "    policy_1: -6040.0\n",
            "    policy_2: -198.5\n",
            "    policy_3: -1227.5\n",
            "  policy_reward_min:\n",
            "    policy_0: 5657.0\n",
            "    policy_1: -7227.0\n",
            "    policy_2: -774.0\n",
            "    policy_3: -2425.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.101005527144628\n",
            "    mean_inference_ms: 3.1049353011110994\n",
            "    mean_processing_ms: 1.1031877909991759\n",
            "  time_since_restore: 321.1378893852234\n",
            "  time_this_iter_s: 15.878862857818604\n",
            "  time_total_s: 321.1378893852234\n",
            "  timestamp: 1583004577\n",
            "  timesteps_since_restore: 2560\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2560\n",
            "  training_iteration: 20\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         321.138</td><td style=\"text-align: right;\">2560</td><td style=\"text-align: right;\">    20</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-29-53\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 15019.929\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.5062500238418579\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.536666870117188\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.01359753031283617\n",
            "        policy_loss: -0.08225895464420319\n",
            "        total_loss: 446579520.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 446579520.0\n",
            "    load_time_ms: 1.26\n",
            "    num_steps_sampled: 2688\n",
            "    num_steps_trained: 2688\n",
            "    sample_time_ms: 767.768\n",
            "    update_time_ms: 33.507\n",
            "  iterations_since_restore: 21\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.58695652173913\n",
            "    ram_util_percent: 19.39999999999999\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 9275.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 377.0\n",
            "    policy_3: -30.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 7466.0\n",
            "    policy_1: -6040.0\n",
            "    policy_2: -198.5\n",
            "    policy_3: -1227.5\n",
            "  policy_reward_min:\n",
            "    policy_0: 5657.0\n",
            "    policy_1: -7227.0\n",
            "    policy_2: -774.0\n",
            "    policy_3: -2425.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.101005527144628\n",
            "    mean_inference_ms: 3.1049353011110994\n",
            "    mean_processing_ms: 1.1031877909991759\n",
            "  time_since_restore: 336.9282259941101\n",
            "  time_this_iter_s: 15.790336608886719\n",
            "  time_total_s: 336.9282259941101\n",
            "  timestamp: 1583004593\n",
            "  timesteps_since_restore: 2688\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2688\n",
            "  training_iteration: 21\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         336.928</td><td style=\"text-align: right;\">2688</td><td style=\"text-align: right;\">    21</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-30-09\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 2\n",
            "  episodes_total: 4\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14992.713\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.5062500238418579\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.545783042907715\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.020926252007484436\n",
            "        policy_loss: -0.1060573011636734\n",
            "        total_loss: 681936128.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 681936128.0\n",
            "    load_time_ms: 1.242\n",
            "    num_steps_sampled: 2816\n",
            "    num_steps_trained: 2816\n",
            "    sample_time_ms: 771.759\n",
            "    update_time_ms: 33.235\n",
            "  iterations_since_restore: 22\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.5\n",
            "    ram_util_percent: 19.5\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 21058.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 4506.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 6840.0\n",
            "    policy_1: -16057.0\n",
            "    policy_2: -5749.25\n",
            "    policy_3: 14966.25\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -2425.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.184184657174825\n",
            "    mean_inference_ms: 3.0788493678827678\n",
            "    mean_processing_ms: 1.123558713074646\n",
            "  time_since_restore: 352.43506050109863\n",
            "  time_this_iter_s: 15.506834506988525\n",
            "  time_total_s: 352.43506050109863\n",
            "  timestamp: 1583004609\n",
            "  timesteps_since_restore: 2816\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2816\n",
            "  training_iteration: 22\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         352.435</td><td style=\"text-align: right;\">2816</td><td style=\"text-align: right;\">    22</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-30-24\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14985.573\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.551548957824707\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.00907808169722557\n",
            "        policy_loss: -0.054147981107234955\n",
            "        total_loss: 1208222.25\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 1208222.25\n",
            "    load_time_ms: 1.289\n",
            "    num_steps_sampled: 2944\n",
            "    num_steps_trained: 2944\n",
            "    sample_time_ms: 768.216\n",
            "    update_time_ms: 31.678\n",
            "  iterations_since_restore: 23\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.28260869565216\n",
            "    ram_util_percent: 19.5\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 21058.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 4506.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 6840.0\n",
            "    policy_1: -16057.0\n",
            "    policy_2: -5749.25\n",
            "    policy_3: 14966.25\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -2425.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.184184657174825\n",
            "    mean_inference_ms: 3.0788493678827678\n",
            "    mean_processing_ms: 1.123558713074646\n",
            "  time_since_restore: 368.2632851600647\n",
            "  time_this_iter_s: 15.828224658966064\n",
            "  time_total_s: 368.2632851600647\n",
            "  timestamp: 1583004624\n",
            "  timesteps_since_restore: 2944\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2944\n",
            "  training_iteration: 23\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         368.263</td><td style=\"text-align: right;\">2944</td><td style=\"text-align: right;\">    23</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-30-40\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14959.016\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.542715072631836\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.015237816609442234\n",
            "        policy_loss: -0.07317061722278595\n",
            "        total_loss: 446808.8125\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 446808.84375\n",
            "    load_time_ms: 1.279\n",
            "    num_steps_sampled: 3072\n",
            "    num_steps_trained: 3072\n",
            "    sample_time_ms: 763.628\n",
            "    update_time_ms: 31.816\n",
            "  iterations_since_restore: 24\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.42727272727271\n",
            "    ram_util_percent: 19.60000000000001\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 21058.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 4506.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 6840.0\n",
            "    policy_1: -16057.0\n",
            "    policy_2: -5749.25\n",
            "    policy_3: 14966.25\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -2425.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.184184657174825\n",
            "    mean_inference_ms: 3.0788493678827678\n",
            "    mean_processing_ms: 1.123558713074646\n",
            "  time_since_restore: 383.9159560203552\n",
            "  time_this_iter_s: 15.652670860290527\n",
            "  time_total_s: 383.9159560203552\n",
            "  timestamp: 1583004640\n",
            "  timesteps_since_restore: 3072\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3072\n",
            "  training_iteration: 24\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         383.916</td><td style=\"text-align: right;\">3072</td><td style=\"text-align: right;\">    24</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-30-56\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14964.108\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.551318168640137\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.011402109637856483\n",
            "        policy_loss: -0.07247921079397202\n",
            "        total_loss: 8089804.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 8089804.0\n",
            "    load_time_ms: 1.257\n",
            "    num_steps_sampled: 3200\n",
            "    num_steps_trained: 3200\n",
            "    sample_time_ms: 747.325\n",
            "    update_time_ms: 30.644\n",
            "  iterations_since_restore: 25\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.45454545454548\n",
            "    ram_util_percent: 19.699999999999996\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 21058.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 4506.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 6840.0\n",
            "    policy_1: -16057.0\n",
            "    policy_2: -5749.25\n",
            "    policy_3: 14966.25\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -2425.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.184184657174825\n",
            "    mean_inference_ms: 3.0788493678827678\n",
            "    mean_processing_ms: 1.123558713074646\n",
            "  time_since_restore: 399.5961558818817\n",
            "  time_this_iter_s: 15.68019986152649\n",
            "  time_total_s: 399.5961558818817\n",
            "  timestamp: 1583004656\n",
            "  timesteps_since_restore: 3200\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3200\n",
            "  training_iteration: 25\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         399.596</td><td style=\"text-align: right;\">3200</td><td style=\"text-align: right;\">    25</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-31-12\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14934.652\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.547499656677246\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.017552264034748077\n",
            "        policy_loss: -0.07706727832555771\n",
            "        total_loss: 57412056.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 57412056.0\n",
            "    load_time_ms: 1.241\n",
            "    num_steps_sampled: 3328\n",
            "    num_steps_trained: 3328\n",
            "    sample_time_ms: 744.236\n",
            "    update_time_ms: 30.601\n",
            "  iterations_since_restore: 26\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.67826086956521\n",
            "    ram_util_percent: 19.800000000000004\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 21058.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 4506.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 6840.0\n",
            "    policy_1: -16057.0\n",
            "    policy_2: -5749.25\n",
            "    policy_3: 14966.25\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -2425.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.184184657174825\n",
            "    mean_inference_ms: 3.0788493678827678\n",
            "    mean_processing_ms: 1.123558713074646\n",
            "  time_since_restore: 415.2063820362091\n",
            "  time_this_iter_s: 15.610226154327393\n",
            "  time_total_s: 415.2063820362091\n",
            "  timestamp: 1583004672\n",
            "  timesteps_since_restore: 3328\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3328\n",
            "  training_iteration: 26\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         415.206</td><td style=\"text-align: right;\">3328</td><td style=\"text-align: right;\">    26</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-31-27\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14940.813\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.548578262329102\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.018533235415816307\n",
            "        policy_loss: -0.10023588687181473\n",
            "        total_loss: 72917600.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 72917600.0\n",
            "    load_time_ms: 1.356\n",
            "    num_steps_sampled: 3456\n",
            "    num_steps_trained: 3456\n",
            "    sample_time_ms: 737.213\n",
            "    update_time_ms: 30.451\n",
            "  iterations_since_restore: 27\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.2181818181818\n",
            "    ram_util_percent: 19.89999999999999\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 21058.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 4506.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 6840.0\n",
            "    policy_1: -16057.0\n",
            "    policy_2: -5749.25\n",
            "    policy_3: 14966.25\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -2425.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.184184657174825\n",
            "    mean_inference_ms: 3.0788493678827678\n",
            "    mean_processing_ms: 1.123558713074646\n",
            "  time_since_restore: 430.8761456012726\n",
            "  time_this_iter_s: 15.669763565063477\n",
            "  time_total_s: 430.8761456012726\n",
            "  timestamp: 1583004687\n",
            "  timesteps_since_restore: 3456\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3456\n",
            "  training_iteration: 27\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         430.876</td><td style=\"text-align: right;\">3456</td><td style=\"text-align: right;\">    27</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-31-43\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14945.505\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.556697845458984\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.012655419297516346\n",
            "        policy_loss: -0.06952691078186035\n",
            "        total_loss: 99799352.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 99799352.0\n",
            "    load_time_ms: 1.305\n",
            "    num_steps_sampled: 3584\n",
            "    num_steps_trained: 3584\n",
            "    sample_time_ms: 728.339\n",
            "    update_time_ms: 29.713\n",
            "  iterations_since_restore: 28\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.57272727272726\n",
            "    ram_util_percent: 20.0\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 21058.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 4506.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 6840.0\n",
            "    policy_1: -16057.0\n",
            "    policy_2: -5749.25\n",
            "    policy_3: 14966.25\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -2425.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.184184657174825\n",
            "    mean_inference_ms: 3.0788493678827678\n",
            "    mean_processing_ms: 1.123558713074646\n",
            "  time_since_restore: 446.37491369247437\n",
            "  time_this_iter_s: 15.498768091201782\n",
            "  time_total_s: 446.37491369247437\n",
            "  timestamp: 1583004703\n",
            "  timesteps_since_restore: 3584\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3584\n",
            "  training_iteration: 28\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         446.375</td><td style=\"text-align: right;\">3584</td><td style=\"text-align: right;\">    28</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-31-58\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14923.306\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.563679695129395\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.010961301624774933\n",
            "        policy_loss: -0.08025716245174408\n",
            "        total_loss: 112344024.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 112344024.0\n",
            "    load_time_ms: 1.318\n",
            "    num_steps_sampled: 3712\n",
            "    num_steps_trained: 3712\n",
            "    sample_time_ms: 709.693\n",
            "    update_time_ms: 27.823\n",
            "  iterations_since_restore: 29\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.24999999999999\n",
            "    ram_util_percent: 20.09545454545455\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 21058.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 4506.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 6840.0\n",
            "    policy_1: -16057.0\n",
            "    policy_2: -5749.25\n",
            "    policy_3: 14966.25\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -2425.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.184184657174825\n",
            "    mean_inference_ms: 3.0788493678827678\n",
            "    mean_processing_ms: 1.123558713074646\n",
            "  time_since_restore: 461.9287209510803\n",
            "  time_this_iter_s: 15.553807258605957\n",
            "  time_total_s: 461.9287209510803\n",
            "  timestamp: 1583004718\n",
            "  timesteps_since_restore: 3712\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3712\n",
            "  training_iteration: 29\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.6/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         461.929</td><td style=\"text-align: right;\">3712</td><td style=\"text-align: right;\">    29</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-32-14\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14873.597\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.568429946899414\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.006268490571528673\n",
            "        policy_loss: -0.06256236135959625\n",
            "        total_loss: 61317668.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 61317668.0\n",
            "    load_time_ms: 1.334\n",
            "    num_steps_sampled: 3840\n",
            "    num_steps_trained: 3840\n",
            "    sample_time_ms: 712.711\n",
            "    update_time_ms: 29.162\n",
            "  iterations_since_restore: 30\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.67826086956521\n",
            "    ram_util_percent: 20.10000000000001\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 21058.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 4506.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 6840.0\n",
            "    policy_1: -16057.0\n",
            "    policy_2: -5749.25\n",
            "    policy_3: 14966.25\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -2425.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.184184657174825\n",
            "    mean_inference_ms: 3.0788493678827678\n",
            "    mean_processing_ms: 1.123558713074646\n",
            "  time_since_restore: 477.3538386821747\n",
            "  time_this_iter_s: 15.42511773109436\n",
            "  time_total_s: 477.3538386821747\n",
            "  timestamp: 1583004734\n",
            "  timesteps_since_restore: 3840\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3840\n",
            "  training_iteration: 30\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.6/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         477.354</td><td style=\"text-align: right;\">3840</td><td style=\"text-align: right;\">    30</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-32-29\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14859.21\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.569364547729492\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.006740286480635405\n",
            "        policy_loss: -0.05366244539618492\n",
            "        total_loss: 37291160.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 37291160.0\n",
            "    load_time_ms: 1.358\n",
            "    num_steps_sampled: 3968\n",
            "    num_steps_trained: 3968\n",
            "    sample_time_ms: 708.755\n",
            "    update_time_ms: 29.074\n",
            "  iterations_since_restore: 31\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.33636363636361\n",
            "    ram_util_percent: 20.199999999999996\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 21058.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 4506.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 6840.0\n",
            "    policy_1: -16057.0\n",
            "    policy_2: -5749.25\n",
            "    policy_3: 14966.25\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -2425.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.184184657174825\n",
            "    mean_inference_ms: 3.0788493678827678\n",
            "    mean_processing_ms: 1.123558713074646\n",
            "  time_since_restore: 492.9694399833679\n",
            "  time_this_iter_s: 15.615601301193237\n",
            "  time_total_s: 492.9694399833679\n",
            "  timestamp: 1583004749\n",
            "  timesteps_since_restore: 3968\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3968\n",
            "  training_iteration: 31\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.6/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         492.969</td><td style=\"text-align: right;\">3968</td><td style=\"text-align: right;\">    31</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-32-45\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14869.03\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.56714916229248\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.01333294715732336\n",
            "        policy_loss: -0.06733853369951248\n",
            "        total_loss: 42537648.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 42537648.0\n",
            "    load_time_ms: 1.344\n",
            "    num_steps_sampled: 4096\n",
            "    num_steps_trained: 4096\n",
            "    sample_time_ms: 714.282\n",
            "    update_time_ms: 30.061\n",
            "  iterations_since_restore: 32\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.43181818181819\n",
            "    ram_util_percent: 20.300000000000004\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 21058.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 4506.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 6840.0\n",
            "    policy_1: -16057.0\n",
            "    policy_2: -5749.25\n",
            "    policy_3: 14966.25\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -2425.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.184184657174825\n",
            "    mean_inference_ms: 3.0788493678827678\n",
            "    mean_processing_ms: 1.123558713074646\n",
            "  time_since_restore: 508.64169120788574\n",
            "  time_this_iter_s: 15.672251224517822\n",
            "  time_total_s: 508.64169120788574\n",
            "  timestamp: 1583004765\n",
            "  timesteps_since_restore: 4096\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 4096\n",
            "  training_iteration: 32\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.6/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         508.642</td><td style=\"text-align: right;\">4096</td><td style=\"text-align: right;\">    32</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-33-01\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 2\n",
            "  episodes_total: 6\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14832.338\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.558279037475586\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.013009306974709034\n",
            "        policy_loss: -0.08816071599721909\n",
            "        total_loss: 126985440.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 126985440.0\n",
            "    load_time_ms: 1.313\n",
            "    num_steps_sampled: 4224\n",
            "    num_steps_trained: 4224\n",
            "    sample_time_ms: 718.634\n",
            "    update_time_ms: 32.173\n",
            "  iterations_since_restore: 33\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.45217391304348\n",
            "    ram_util_percent: 20.39999999999999\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 22611.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 67126.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 11278.666666666666\n",
            "    policy_1: -18119.166666666668\n",
            "    policy_2: 9485.166666666666\n",
            "    policy_3: -2644.6666666666665\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -61879.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.188596591632244\n",
            "    mean_inference_ms: 3.0402833422100684\n",
            "    mean_processing_ms: 1.1241805271632481\n",
            "  time_since_restore: 524.1679499149323\n",
            "  time_this_iter_s: 15.526258707046509\n",
            "  time_total_s: 524.1679499149323\n",
            "  timestamp: 1583004781\n",
            "  timesteps_since_restore: 4224\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 4224\n",
            "  training_iteration: 33\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.6/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         524.168</td><td style=\"text-align: right;\">4224</td><td style=\"text-align: right;\">    33</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-33-17\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 6\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14856.493\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.559807777404785\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.012666773051023483\n",
            "        policy_loss: -0.0947626605629921\n",
            "        total_loss: 29750464.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 29750464.0\n",
            "    load_time_ms: 1.298\n",
            "    num_steps_sampled: 4352\n",
            "    num_steps_trained: 4352\n",
            "    sample_time_ms: 714.48\n",
            "    update_time_ms: 32.724\n",
            "  iterations_since_restore: 34\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.37727272727274\n",
            "    ram_util_percent: 20.5\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 22611.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 67126.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 11278.666666666666\n",
            "    policy_1: -18119.166666666668\n",
            "    policy_2: 9485.166666666666\n",
            "    policy_3: -2644.6666666666665\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -61879.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.188596591632245\n",
            "    mean_inference_ms: 3.0402833422100684\n",
            "    mean_processing_ms: 1.1241805271632481\n",
            "  time_since_restore: 540.0261788368225\n",
            "  time_this_iter_s: 15.858228921890259\n",
            "  time_total_s: 540.0261788368225\n",
            "  timestamp: 1583004797\n",
            "  timesteps_since_restore: 4352\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 4352\n",
            "  training_iteration: 34\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.6/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         540.026</td><td style=\"text-align: right;\">4352</td><td style=\"text-align: right;\">    34</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-33-32\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 6\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14841.611\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.561220169067383\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.008585321716964245\n",
            "        policy_loss: -0.075234554708004\n",
            "        total_loss: 189901872.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 189901872.0\n",
            "    load_time_ms: 1.323\n",
            "    num_steps_sampled: 4480\n",
            "    num_steps_trained: 4480\n",
            "    sample_time_ms: 715.324\n",
            "    update_time_ms: 32.732\n",
            "  iterations_since_restore: 35\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.34545454545454\n",
            "    ram_util_percent: 20.59545454545455\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 22611.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 67126.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 11278.666666666666\n",
            "    policy_1: -18119.166666666668\n",
            "    policy_2: 9485.166666666666\n",
            "    policy_3: -2644.6666666666665\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -61879.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.188596591632245\n",
            "    mean_inference_ms: 3.0402833422100684\n",
            "    mean_processing_ms: 1.1241805271632481\n",
            "  time_since_restore: 555.566650390625\n",
            "  time_this_iter_s: 15.54047155380249\n",
            "  time_total_s: 555.566650390625\n",
            "  timestamp: 1583004812\n",
            "  timesteps_since_restore: 4480\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 4480\n",
            "  training_iteration: 35\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.6/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         555.567</td><td style=\"text-align: right;\">4480</td><td style=\"text-align: right;\">    35</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-33-48\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 6\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14801.358\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.561738967895508\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.013734973035752773\n",
            "        policy_loss: -0.11868586391210556\n",
            "        total_loss: 189383552.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 189383552.0\n",
            "    load_time_ms: 1.321\n",
            "    num_steps_sampled: 4608\n",
            "    num_steps_trained: 4608\n",
            "    sample_time_ms: 713.15\n",
            "    update_time_ms: 33.035\n",
            "  iterations_since_restore: 36\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.4681818181818\n",
            "    ram_util_percent: 20.60000000000001\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 22611.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 67126.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 11278.666666666666\n",
            "    policy_1: -18119.166666666668\n",
            "    policy_2: 9485.166666666666\n",
            "    policy_3: -2644.6666666666665\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -61879.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.188596591632245\n",
            "    mean_inference_ms: 3.0402833422100684\n",
            "    mean_processing_ms: 1.1241805271632481\n",
            "  time_since_restore: 570.7568278312683\n",
            "  time_this_iter_s: 15.19017744064331\n",
            "  time_total_s: 570.7568278312683\n",
            "  timestamp: 1583004828\n",
            "  timesteps_since_restore: 4608\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 4608\n",
            "  training_iteration: 36\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.6/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         570.757</td><td style=\"text-align: right;\">4608</td><td style=\"text-align: right;\">    36</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-34-04\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 6\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14823.608\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.552724838256836\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.014937568455934525\n",
            "        policy_loss: -0.0949852466583252\n",
            "        total_loss: 624021120.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 624021120.0\n",
            "    load_time_ms: 1.289\n",
            "    num_steps_sampled: 4736\n",
            "    num_steps_trained: 4736\n",
            "    sample_time_ms: 718.738\n",
            "    update_time_ms: 34.908\n",
            "  iterations_since_restore: 37\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.49565217391303\n",
            "    ram_util_percent: 20.699999999999996\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 22611.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 67126.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 11278.666666666666\n",
            "    policy_1: -18119.166666666668\n",
            "    policy_2: 9485.166666666666\n",
            "    policy_3: -2644.6666666666665\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -61879.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.188596591632245\n",
            "    mean_inference_ms: 3.0402833422100684\n",
            "    mean_processing_ms: 1.1241805271632481\n",
            "  time_since_restore: 586.7221856117249\n",
            "  time_this_iter_s: 15.965357780456543\n",
            "  time_total_s: 586.7221856117249\n",
            "  timestamp: 1583004844\n",
            "  timesteps_since_restore: 4736\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 4736\n",
            "  training_iteration: 37\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.6/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         586.722</td><td style=\"text-align: right;\">4736</td><td style=\"text-align: right;\">    37</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-34-19\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 6\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14829.998\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.564440727233887\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.011379528790712357\n",
            "        policy_loss: -0.08958379924297333\n",
            "        total_loss: 126248888.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 126248888.0\n",
            "    load_time_ms: 1.388\n",
            "    num_steps_sampled: 4864\n",
            "    num_steps_trained: 4864\n",
            "    sample_time_ms: 716.601\n",
            "    update_time_ms: 35.541\n",
            "  iterations_since_restore: 38\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.4\n",
            "    ram_util_percent: 20.800000000000004\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 22611.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 67126.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 11278.666666666666\n",
            "    policy_1: -18119.166666666668\n",
            "    policy_2: 9485.166666666666\n",
            "    policy_3: -2644.6666666666665\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -61879.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.188596591632245\n",
            "    mean_inference_ms: 3.0402833422100684\n",
            "    mean_processing_ms: 1.1241805271632481\n",
            "  time_since_restore: 602.2710659503937\n",
            "  time_this_iter_s: 15.548880338668823\n",
            "  time_total_s: 602.2710659503937\n",
            "  timestamp: 1583004859\n",
            "  timesteps_since_restore: 4864\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 4864\n",
            "  training_iteration: 38\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.6/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         602.271</td><td style=\"text-align: right;\">4864</td><td style=\"text-align: right;\">    38</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-34-35\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 6\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14876.385\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.56497573852539\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.012310197576880455\n",
            "        policy_loss: -0.07259421050548553\n",
            "        total_loss: 32681002.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 32681002.0\n",
            "    load_time_ms: 1.37\n",
            "    num_steps_sampled: 4992\n",
            "    num_steps_trained: 4992\n",
            "    sample_time_ms: 719.888\n",
            "    update_time_ms: 36.898\n",
            "  iterations_since_restore: 39\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.83913043478262\n",
            "    ram_util_percent: 20.89999999999999\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 22611.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 67126.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 11278.666666666666\n",
            "    policy_1: -18119.166666666668\n",
            "    policy_2: 9485.166666666666\n",
            "    policy_3: -2644.6666666666665\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -61879.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.188596591632245\n",
            "    mean_inference_ms: 3.0402833422100684\n",
            "    mean_processing_ms: 1.1241805271632481\n",
            "  time_since_restore: 618.3353049755096\n",
            "  time_this_iter_s: 16.064239025115967\n",
            "  time_total_s: 618.3353049755096\n",
            "  timestamp: 1583004875\n",
            "  timesteps_since_restore: 4992\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 4992\n",
            "  training_iteration: 39\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         618.335</td><td style=\"text-align: right;\">4992</td><td style=\"text-align: right;\">    39</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-34-51\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 6\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14940.503\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.559139251708984\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.010569708421826363\n",
            "        policy_loss: -0.1064121425151825\n",
            "        total_loss: 35589532.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 35589532.0\n",
            "    load_time_ms: 1.409\n",
            "    num_steps_sampled: 5120\n",
            "    num_steps_trained: 5120\n",
            "    sample_time_ms: 720.345\n",
            "    update_time_ms: 36.566\n",
            "  iterations_since_restore: 40\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.4391304347826\n",
            "    ram_util_percent: 21.0\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 22611.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 67126.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 11278.666666666666\n",
            "    policy_1: -18119.166666666668\n",
            "    policy_2: 9485.166666666666\n",
            "    policy_3: -2644.6666666666665\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -61879.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.188596591632245\n",
            "    mean_inference_ms: 3.0402833422100684\n",
            "    mean_processing_ms: 1.1241805271632481\n",
            "  time_since_restore: 634.403183221817\n",
            "  time_this_iter_s: 16.067878246307373\n",
            "  time_total_s: 634.403183221817\n",
            "  timestamp: 1583004891\n",
            "  timesteps_since_restore: 5120\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 5120\n",
            "  training_iteration: 40\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         634.403</td><td style=\"text-align: right;\">5120</td><td style=\"text-align: right;\">    40</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-35-07\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 6\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14945.224\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.541845321655273\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.013011161237955093\n",
            "        policy_loss: -0.09919741749763489\n",
            "        total_loss: 6076301.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 6076301.5\n",
            "    load_time_ms: 1.482\n",
            "    num_steps_sampled: 5248\n",
            "    num_steps_trained: 5248\n",
            "    sample_time_ms: 722.439\n",
            "    update_time_ms: 36.966\n",
            "  iterations_since_restore: 41\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.33043478260869\n",
            "    ram_util_percent: 21.10000000000001\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 22611.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 67126.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 11278.666666666666\n",
            "    policy_1: -18119.166666666668\n",
            "    policy_2: 9485.166666666666\n",
            "    policy_3: -2644.6666666666665\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -61879.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.188596591632245\n",
            "    mean_inference_ms: 3.0402833422100684\n",
            "    mean_processing_ms: 1.1241805271632481\n",
            "  time_since_restore: 650.0841031074524\n",
            "  time_this_iter_s: 15.680919885635376\n",
            "  time_total_s: 650.0841031074524\n",
            "  timestamp: 1583004907\n",
            "  timesteps_since_restore: 5248\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 5248\n",
            "  training_iteration: 41\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         650.084</td><td style=\"text-align: right;\">5248</td><td style=\"text-align: right;\">    41</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-35-23\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 6\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14955.658\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.54813289642334\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.013704896904528141\n",
            "        policy_loss: -0.11438899487257004\n",
            "        total_loss: 27999208.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 27999208.0\n",
            "    load_time_ms: 1.54\n",
            "    num_steps_sampled: 5376\n",
            "    num_steps_trained: 5376\n",
            "    sample_time_ms: 721.979\n",
            "    update_time_ms: 37.674\n",
            "  iterations_since_restore: 42\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.69545454545454\n",
            "    ram_util_percent: 21.199999999999996\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 22611.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 67126.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 11278.666666666666\n",
            "    policy_1: -18119.166666666668\n",
            "    policy_2: 9485.166666666666\n",
            "    policy_3: -2644.6666666666665\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -61879.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.188596591632245\n",
            "    mean_inference_ms: 3.0402833422100684\n",
            "    mean_processing_ms: 1.1241805271632481\n",
            "  time_since_restore: 665.8622345924377\n",
            "  time_this_iter_s: 15.778131484985352\n",
            "  time_total_s: 665.8622345924377\n",
            "  timestamp: 1583004923\n",
            "  timesteps_since_restore: 5376\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 5376\n",
            "  training_iteration: 42\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         665.862</td><td style=\"text-align: right;\">5376</td><td style=\"text-align: right;\">    42</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-35-39\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 6\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14946.676\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.5533447265625\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.010108786635100842\n",
            "        policy_loss: -0.08697810769081116\n",
            "        total_loss: 85887456.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 85887456.0\n",
            "    load_time_ms: 1.58\n",
            "    num_steps_sampled: 5504\n",
            "    num_steps_trained: 5504\n",
            "    sample_time_ms: 729.466\n",
            "    update_time_ms: 36.596\n",
            "  iterations_since_restore: 43\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.54347826086955\n",
            "    ram_util_percent: 21.199999999999996\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 22611.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 67126.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 11278.666666666666\n",
            "    policy_1: -18119.166666666668\n",
            "    policy_2: 9485.166666666666\n",
            "    policy_3: -2644.6666666666665\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -61879.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.188596591632245\n",
            "    mean_inference_ms: 3.0402833422100684\n",
            "    mean_processing_ms: 1.1241805271632481\n",
            "  time_since_restore: 681.3642919063568\n",
            "  time_this_iter_s: 15.502057313919067\n",
            "  time_total_s: 681.3642919063568\n",
            "  timestamp: 1583004939\n",
            "  timesteps_since_restore: 5504\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 5504\n",
            "  training_iteration: 43\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         681.364</td><td style=\"text-align: right;\">5504</td><td style=\"text-align: right;\">    43</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-35-54\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 2\n",
            "  episodes_total: 8\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14906.665\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.541428565979004\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.014483047649264336\n",
            "        policy_loss: -0.08690053224563599\n",
            "        total_loss: 130532448.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 130532448.0\n",
            "    load_time_ms: 1.6\n",
            "    num_steps_sampled: 5632\n",
            "    num_steps_trained: 5632\n",
            "    sample_time_ms: 734.637\n",
            "    update_time_ms: 36.711\n",
            "  iterations_since_restore: 44\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.74090909090908\n",
            "    ram_util_percent: 21.300000000000004\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 23209.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 67126.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 12424.875\n",
            "    policy_1: -16124.875\n",
            "    policy_2: 6730.875\n",
            "    policy_3: -3030.875\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -61879.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.190121722809504\n",
            "    mean_inference_ms: 3.0115086642574855\n",
            "    mean_processing_ms: 1.1249815758830333\n",
            "  time_since_restore: 696.8761239051819\n",
            "  time_this_iter_s: 15.511831998825073\n",
            "  time_total_s: 696.8761239051819\n",
            "  timestamp: 1583004954\n",
            "  timesteps_since_restore: 5632\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 5632\n",
            "  training_iteration: 44\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         696.876</td><td style=\"text-align: right;\">5632</td><td style=\"text-align: right;\">    44</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-36-10\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 8\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14912.43\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.544062614440918\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.01602988690137863\n",
            "        policy_loss: -0.10013841837644577\n",
            "        total_loss: 99061.625\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 99061.71875\n",
            "    load_time_ms: 1.568\n",
            "    num_steps_sampled: 5760\n",
            "    num_steps_trained: 5760\n",
            "    sample_time_ms: 732.862\n",
            "    update_time_ms: 36.975\n",
            "  iterations_since_restore: 45\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.27272727272725\n",
            "    ram_util_percent: 21.39999999999999\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 23209.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 67126.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 12424.875\n",
            "    policy_1: -16124.875\n",
            "    policy_2: 6730.875\n",
            "    policy_3: -3030.875\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -61879.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.190121722809504\n",
            "    mean_inference_ms: 3.0115086642574855\n",
            "    mean_processing_ms: 1.1249815758830333\n",
            "  time_since_restore: 712.4584605693817\n",
            "  time_this_iter_s: 15.582336664199829\n",
            "  time_total_s: 712.4584605693817\n",
            "  timestamp: 1583004970\n",
            "  timesteps_since_restore: 5760\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 5760\n",
            "  training_iteration: 45\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         712.458</td><td style=\"text-align: right;\">5760</td><td style=\"text-align: right;\">    45</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-36-26\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 8\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14969.307\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.563232421875\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.010487709194421768\n",
            "        policy_loss: -0.077745720744133\n",
            "        total_loss: 25258092.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 25258092.0\n",
            "    load_time_ms: 1.572\n",
            "    num_steps_sampled: 5888\n",
            "    num_steps_trained: 5888\n",
            "    sample_time_ms: 740.812\n",
            "    update_time_ms: 37.26\n",
            "  iterations_since_restore: 46\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.25652173913043\n",
            "    ram_util_percent: 21.5\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 23209.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 67126.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 12424.875\n",
            "    policy_1: -16124.875\n",
            "    policy_2: 6730.875\n",
            "    policy_3: -3030.875\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -61879.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.190121722809504\n",
            "    mean_inference_ms: 3.0115086642574855\n",
            "    mean_processing_ms: 1.1249815758830333\n",
            "  time_since_restore: 728.2974059581757\n",
            "  time_this_iter_s: 15.838945388793945\n",
            "  time_total_s: 728.2974059581757\n",
            "  timestamp: 1583004986\n",
            "  timesteps_since_restore: 5888\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 5888\n",
            "  training_iteration: 46\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         728.297</td><td style=\"text-align: right;\">5888</td><td style=\"text-align: right;\">    46</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-36-41\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 8\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14951.419\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.563613891601562\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.009684189222753048\n",
            "        policy_loss: -0.08653222769498825\n",
            "        total_loss: 53974352.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 53974352.0\n",
            "    load_time_ms: 1.528\n",
            "    num_steps_sampled: 6016\n",
            "    num_steps_trained: 6016\n",
            "    sample_time_ms: 739.839\n",
            "    update_time_ms: 35.536\n",
            "  iterations_since_restore: 47\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.41363636363636\n",
            "    ram_util_percent: 21.60000000000001\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 23209.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 67126.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 12424.875\n",
            "    policy_1: -16124.875\n",
            "    policy_2: 6730.875\n",
            "    policy_3: -3030.875\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -61879.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.190121722809504\n",
            "    mean_inference_ms: 3.0115086642574855\n",
            "    mean_processing_ms: 1.1249815758830333\n",
            "  time_since_restore: 744.0566358566284\n",
            "  time_this_iter_s: 15.759229898452759\n",
            "  time_total_s: 744.0566358566284\n",
            "  timestamp: 1583005001\n",
            "  timesteps_since_restore: 6016\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 6016\n",
            "  training_iteration: 47\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         744.057</td><td style=\"text-align: right;\">6016</td><td style=\"text-align: right;\">    47</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-36-57\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 8\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14963.754\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.548479080200195\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.008800836279988289\n",
            "        policy_loss: -0.06188404560089111\n",
            "        total_loss: 111989968.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 111989968.0\n",
            "    load_time_ms: 1.435\n",
            "    num_steps_sampled: 6144\n",
            "    num_steps_trained: 6144\n",
            "    sample_time_ms: 739.804\n",
            "    update_time_ms: 35.167\n",
            "  iterations_since_restore: 48\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.7217391304348\n",
            "    ram_util_percent: 21.678260869565214\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 23209.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 67126.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 12424.875\n",
            "    policy_1: -16124.875\n",
            "    policy_2: 6730.875\n",
            "    policy_3: -3030.875\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -61879.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.190121722809504\n",
            "    mean_inference_ms: 3.0115086642574855\n",
            "    mean_processing_ms: 1.1249815758830333\n",
            "  time_since_restore: 759.7230987548828\n",
            "  time_this_iter_s: 15.666462898254395\n",
            "  time_total_s: 759.7230987548828\n",
            "  timestamp: 1583005017\n",
            "  timesteps_since_restore: 6144\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 6144\n",
            "  training_iteration: 48\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         759.723</td><td style=\"text-align: right;\">6144</td><td style=\"text-align: right;\">    48</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-37-13\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 8\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14916.426\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.548850059509277\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.013503413647413254\n",
            "        policy_loss: -0.09687633812427521\n",
            "        total_loss: 83116896.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 83116896.0\n",
            "    load_time_ms: 1.436\n",
            "    num_steps_sampled: 6272\n",
            "    num_steps_trained: 6272\n",
            "    sample_time_ms: 753.353\n",
            "    update_time_ms: 33.844\n",
            "  iterations_since_restore: 49\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.04545454545455\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 23209.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 67126.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 12424.875\n",
            "    policy_1: -16124.875\n",
            "    policy_2: 6730.875\n",
            "    policy_3: -3030.875\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -61879.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.190121722809504\n",
            "    mean_inference_ms: 3.0115086642574855\n",
            "    mean_processing_ms: 1.1249815758830333\n",
            "  time_since_restore: 775.435129404068\n",
            "  time_this_iter_s: 15.71203064918518\n",
            "  time_total_s: 775.435129404068\n",
            "  timestamp: 1583005033\n",
            "  timesteps_since_restore: 6272\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 6272\n",
            "  training_iteration: 49\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>RUNNING </td><td>172.28.0.2:12191</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         775.435</td><td style=\"text-align: right;\">6272</td><td style=\"text-align: right;\">    49</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_07001334:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-02-29_19-37-29\n",
            "  done: true\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 8\n",
            "  experiment_id: 0a648881c4bc4a878ccbbb1728f6c82e\n",
            "  experiment_tag: '0'\n",
            "  hostname: 7e6fe8db4693\n",
            "  info:\n",
            "    grad_time_ms: 14882.699\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.7593749761581421\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 9.55988883972168\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.007329336833208799\n",
            "        policy_loss: -0.07236185669898987\n",
            "        total_loss: 21397754.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 21397754.0\n",
            "    load_time_ms: 1.38\n",
            "    num_steps_sampled: 6400\n",
            "    num_steps_trained: 6400\n",
            "    sample_time_ms: 740.817\n",
            "    update_time_ms: 32.836\n",
            "  iterations_since_restore: 50\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 96.52727272727272\n",
            "    ram_util_percent: 21.800000000000004\n",
            "  pid: 12191\n",
            "  policy_reward_max:\n",
            "    policy_0: 23209.0\n",
            "    policy_1: -4853.0\n",
            "    policy_2: 67126.0\n",
            "    policy_3: 44537.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 12424.875\n",
            "    policy_1: -16124.875\n",
            "    policy_2: 6730.875\n",
            "    policy_3: -3030.875\n",
            "  policy_reward_min:\n",
            "    policy_0: -8630.0\n",
            "    policy_1: -38489.0\n",
            "    policy_2: -27106.0\n",
            "    policy_3: -61879.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 7.190121722809504\n",
            "    mean_inference_ms: 3.0115086642574855\n",
            "    mean_processing_ms: 1.1249815758830333\n",
            "  time_since_restore: 791.0320391654968\n",
            "  time_this_iter_s: 15.596909761428833\n",
            "  time_total_s: 791.0320391654968\n",
            "  timestamp: 1583005049\n",
            "  timesteps_since_restore: 6400\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 6400\n",
            "  training_iteration: 50\n",
            "  trial_id: '07001334'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         791.032</td><td style=\"text-align: right;\">6400</td><td style=\"text-align: right;\">    50</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_07001334</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         791.032</td><td style=\"text-align: right;\">6400</td><td style=\"text-align: right;\">    50</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2020-02-29 19:37:29,221\tINFO tune.py:352 -- Returning an analysis object by default. You can call `analysis.trials` to retrieve a list of trials. This message will be removed in future versions of Tune.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f97f4b600f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eTWYR3dglJ8",
        "colab_type": "code",
        "outputId": "9005bb82-688e-4288-87ad-afec91217847",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "next_states:\n",
        "{\n",
        "    0: [array([ 16., 114., 141.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]), \n",
        "        array([4., 3., 2., 0., 0., 0., 0., 0., 0., 0.]), \n",
        "        array([  -7.,  -60., -135.,  -97.,  -61., -151., -136.,    0.,    0., 0.]), \n",
        "        array([  -7.,   -9.,  -16., -100., -101., -102., -103.,    0.,    0., 0.])], \n",
        " \n",
        "    1: [array([ 16., 114., 141.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]), \n",
        "        array([4., 3., 2., 0., 0., 0., 0., 0., 0., 0.]), \n",
        "        array([  -7.,  -60., -135.,  -97.,  -61., -151., -136.,    0.,    0., 0.]), \n",
        "        array([  -7.,   -9.,  -16., -100., -101., -102., -103.,    0.,    0., 0.])], \n",
        " \n",
        "    2: [array([ 16., 114., 141.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]), \n",
        "        array([4., 3., 2., 0., 0., 0., 0., 0., 0., 0.]), \n",
        "        array([  -7.,  -60., -135.,  -97.,  -61., -151., -136.,    0.,    0., 0.]), \n",
        "        array([  -7.,   -9.,  -16., -100., -101., -102., -103.,    0.,    0., 0.])], \n",
        " \n",
        "    3: [array([ 16., 114., 141.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]), \n",
        "        array([4., 3., 2., 0., 0., 0., 0., 0., 0., 0.]), \n",
        "        array([  -7.,  -60., -135.,  -97.,  -61., -151., -136.,    0.,    0., 0.]), \n",
        "        array([  -7.,   -9.,  -16., -100., -101., -102., -103.,    0.,    0., 0.])]\n",
        " }\n",
        " '''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnext_states:\\n{\\n    0: [array([ 16., 114., 141.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]), \\n        array([4., 3., 2., 0., 0., 0., 0., 0., 0., 0.]), \\n        array([  -7.,  -60., -135.,  -97.,  -61., -151., -136.,    0.,    0., 0.]), \\n        array([  -7.,   -9.,  -16., -100., -101., -102., -103.,    0.,    0., 0.])], \\n \\n    1: [array([ 16., 114., 141.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]), \\n        array([4., 3., 2., 0., 0., 0., 0., 0., 0., 0.]), \\n        array([  -7.,  -60., -135.,  -97.,  -61., -151., -136.,    0.,    0., 0.]), \\n        array([  -7.,   -9.,  -16., -100., -101., -102., -103.,    0.,    0., 0.])], \\n \\n    2: [array([ 16., 114., 141.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]), \\n        array([4., 3., 2., 0., 0., 0., 0., 0., 0., 0.]), \\n        array([  -7.,  -60., -135.,  -97.,  -61., -151., -136.,    0.,    0., 0.]), \\n        array([  -7.,   -9.,  -16., -100., -101., -102., -103.,    0.,    0., 0.])], \\n \\n    3: [array([ 16., 114., 141.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]), \\n        array([4., 3., 2., 0., 0., 0., 0., 0., 0., 0.]), \\n        array([  -7.,  -60., -135.,  -97.,  -61., -151., -136.,    0.,    0., 0.]), \\n        array([  -7.,   -9.,  -16., -100., -101., -102., -103.,    0.,    0., 0.])]\\n }\\n '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}