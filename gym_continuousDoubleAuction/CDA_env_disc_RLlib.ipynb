{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CDA_env_disc_RLlib.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9Toun0Aj3tS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install ray[rllib]==0.7.3 # Originally developed with ver 0.7.3\n",
        "!pip install ray[rllib]\n",
        "#!pip install ray[debug]==0.7.3\n",
        "#!pip install ray[debug]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_GHZ31rWbYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip show tensorflow\n",
        "!pip show ray"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAqVG2cqjLXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Needed to switch directory in Google drive so as to import MARL env.\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd \"/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction/\"\n",
        "#%cd \"/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction/gym_continuousDoubleAuction/\"\n",
        "\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UW3INjDipTC",
        "colab_type": "code",
        "outputId": "ba2efa90-05a8-4fb6-bd7e-18aa90c33cfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "#CDA_env_disc_RLlib\n",
        "\n",
        "# https://github.com/ray-project/ray/blob/master/python/ray/rllib/examples/multiagent_cartpole.py\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\"\"\"Simple example of setting up a multi-agent policy mapping.\n",
        "Control the number of agents and policies via --num-agents and --num-policies.\n",
        "This works with hundreds of agents and policies, but note that initializing\n",
        "many TF policies will take some time.\n",
        "Also, TF evals might slow down with large numbers of policies. To debug TF\n",
        "execution, set the TF_TIMELINE_DIR environment variable.\n",
        "\"\"\"\n",
        "\n",
        "# rllib rollout ~/ray_results/PPO/PPO_MMenv-v0_0_2019-08-09_00-06-10t4b1lscr/checkpoint-1 --run PPO --env MMenv-v0 --steps 100\n",
        "\n",
        "import os\n",
        "os.environ['RAY_DEBUG_DISABLE_MEMORY_MONITOR'] = \"True\"\n",
        "\n",
        "import argparse\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.models import Model, ModelCatalog\n",
        "\n",
        "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
        "from ray.rllib.models.tf.fcnet_v2 import FullyConnectedNetwork\n",
        "\n",
        "from ray.tune.registry import register_env\n",
        "from ray.rllib.utils import try_import_tf\n",
        "\n",
        "\n",
        "from ray.rllib.policy.policy import Policy\n",
        "#from ray.rllib.policy.tf_policy import TFPolicy\n",
        "#from ray.rllib.policy import Policy\n",
        "\n",
        "\n",
        "#from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
        "\n",
        "\n",
        "import sys\n",
        "if \"../\" not in sys.path:\n",
        "    sys.path.append(\"../\")\n",
        "#from exchg.x.y import z\n",
        "\n",
        "#from envs.continuousDoubleAuction_env import continuousDoubleAuctionEnv\n",
        "from gym_continuousDoubleAuction.envs.continuousDoubleAuction_env import continuousDoubleAuctionEnv\n",
        "\n",
        "tf = try_import_tf()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFWgG3bhiwux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--num-agents\", type=int, default=4)\n",
        "parser.add_argument(\"--num-policies\", type=int, default=4)\n",
        "parser.add_argument(\"--num-iters\", type=int, default=10)\n",
        "parser.add_argument(\"--simple\", action=\"store_true\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIYT1UTxiQcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomModel_disc(Model):\n",
        "    def _lstm(self, Inputs, cell_size):\n",
        "        s = tf.expand_dims(Inputs, axis=1, name='time_major')  # [time_step, feature] => [time_step, batch, feature]\n",
        "        lstm_cell = tf.nn.rnn_cell.LSTMCell(cell_size)\n",
        "        self.init_state = lstm_cell.zero_state(batch_size=1, dtype=tf.float32)\n",
        "        # time_major means [time_step, batch, feature] while batch major means [batch, time_step, feature]\n",
        "        outputs, self.final_state = tf.nn.dynamic_rnn(cell=lstm_cell, inputs=s, initial_state=self.init_state, time_major=True)\n",
        "        lstm_out = tf.reshape(outputs, [-1, cell_size], name='flatten_rnn_outputs')  # joined state representation\n",
        "        return lstm_out\n",
        "\n",
        "    def _build_layers_v2(self, input_dict, num_outputs, options):\n",
        "        hidden = 512\n",
        "        cell_size = 256\n",
        "        #S = input_dict[\"obs\"]\n",
        "        S = tf.layers.flatten(input_dict[\"obs\"])\n",
        "        with tf.variable_scope(tf.VariableScope(tf.AUTO_REUSE, \"shared\"),\n",
        "                               reuse=tf.AUTO_REUSE,\n",
        "                               auxiliary_name_scope=False):\n",
        "            last_layer = tf.layers.dense(S, hidden, activation=tf.nn.relu, name=\"fc1\")\n",
        "        last_layer = tf.layers.dense(last_layer, hidden, activation=tf.nn.relu, name=\"fc2\")\n",
        "        last_layer = tf.layers.dense(last_layer, hidden, activation=tf.nn.relu, name=\"fc3\")\n",
        "\n",
        "        last_layer = self._lstm(last_layer, cell_size)\n",
        "\n",
        "        output = tf.layers.dense(last_layer, num_outputs, activation=tf.nn.softmax, name=\"mu\")\n",
        "\n",
        "        return output, last_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56uO7LyPMQ4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomModel(TFModelV2):\n",
        "    \"\"\"Example of a custom model that just delegates to a fc-net.\"\"\"\n",
        "\n",
        "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
        "        \n",
        "        super(CustomModel, self).__init__(obs_space, action_space, num_outputs, model_config, name)\n",
        "        \n",
        "        self.model = FullyConnectedNetwork(obs_space, action_space, num_outputs, model_config, name)\n",
        "        #print('obs_space', obs_space)\n",
        "\n",
        "        self.register_variables(self.model.variables())\n",
        "\n",
        "    def forward(self, input_dict, state, seq_lens):\n",
        "        return self.model.forward(input_dict, state, seq_lens)\n",
        "\n",
        "    def value_function(self):\n",
        "        return self.model.value_function()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cijiMv-ti1SK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_RandomPolicy(_seed):\n",
        "\n",
        "    # a hand-coded policy that acts at random in the env (doesn't learn)\n",
        "    class RandomPolicy(Policy):\n",
        "        \"\"\"Hand-coded policy that returns random actions.\"\"\"\n",
        "        def __init__(self, observation_space, action_space, config):\n",
        "            self.observation_space = observation_space\n",
        "            self.action_space = action_space\n",
        "            self.action_space.seed(_seed)\n",
        "\n",
        "        def compute_actions(self,\n",
        "                            obs_batch,\n",
        "                            state_batches,\n",
        "                            prev_action_batch=None,\n",
        "                            prev_reward_batch=None,\n",
        "                            info_batch=None,\n",
        "                            episodes=None,\n",
        "                            **kwargs):\n",
        "            \"\"\"Compute actions on a batch of observations.\"\"\"\n",
        "            return [self.action_space.sample() for _ in obs_batch], [], {}\n",
        "\n",
        "        def learn_on_batch(self, samples):\n",
        "            \"\"\"No learning.\"\"\"\n",
        "            #return {}\n",
        "            pass\n",
        "\n",
        "        def get_weights(self):\n",
        "            pass\n",
        "\n",
        "        def set_weights(self, weights):\n",
        "            pass\n",
        "\n",
        "    return RandomPolicy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7ASMamrPoh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#args = parser.parse_args()\n",
        "\n",
        "# set log_to_driver=False to off render output so as to prevent browser from hanging.\n",
        "#ray.init(ignore_reinit_error=True)\n",
        "#ray.init(ignore_reinit_error=True, num_cpus=2)\n",
        "#ray.init(ignore_reinit_error=True, webui_host='127.0.0.1', num_cpus=2)\n",
        "ray.init(ignore_reinit_error=True, log_to_driver=False, webui_host='127.0.0.1', num_cpus=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqzjVWUsPykm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_agents = 4\n",
        "num_policies = num_agents\n",
        "num_iters = 3\n",
        "simple = False #store_true\n",
        "#num_of_traders = args.num_agents\n",
        "num_of_traders = num_agents\n",
        "tape_display_length = 10 #100\n",
        "tick_size = 1\n",
        "init_cash = 1000000\n",
        "max_step = 700 # per episode #700\n",
        "episode = 5 #9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To995IZanbGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "single_CDA_env = continuousDoubleAuctionEnv(num_of_traders, init_cash, tick_size, tape_display_length, max_step)\n",
        "obs_space = single_CDA_env.observation_space\n",
        "act_space = single_CDA_env.action_space\n",
        "register_env(\"continuousDoubleAuction-v0\", lambda _: continuousDoubleAuctionEnv(num_of_traders, init_cash, tick_size, tape_display_length, max_step))\n",
        "ModelCatalog.register_custom_model(\"model_disc\", CustomModel_disc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN5IfMMvP4VA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Each policy can have a different configuration (including custom model)\n",
        "def gen_policy(i):\n",
        "    config = {\"model\": {\"custom_model\": \"model_disc\"},\n",
        "              \"gamma\": 0.99,}\n",
        "    return (None, obs_space, act_space, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7KFjAxGP6en",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "def policy_mapper_0(agent_id):\n",
        "    if agent_id == 0:\n",
        "        return \"policy_0\" # PPO\n",
        "    elif agent_id == 1:\n",
        "        return \"policy_1\" # RandomPolicy\n",
        "    elif agent_id == 2:\n",
        "        return \"policy_2\" # RandomPolicy\n",
        "    else:\n",
        "        return \"policy_3\" # RandomPolicy\n",
        "\"\"\"\n",
        "def policy_mapper(agent_id):\n",
        "    for i in range(num_agents):\n",
        "        if agent_id == i:\n",
        "            return \"policy_{}\".format(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXrTPRQDP8of",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup PPO with an ensemble of `num_policies` different policies\n",
        "\n",
        "# Dictionary of policies\n",
        "#policies = {\"policy_{}\".format(i): gen_policy(i) for i in range(args.num_policies)}\n",
        "policies = {\"policy_{}\".format(i): gen_policy(i) for i in range(num_policies)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsbblOn-P_eA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ce294259-1505-46d7-93be-4629fbf1ca3c"
      },
      "source": [
        "# override policy with random policy\n",
        "\"\"\"\n",
        "policies[\"policy_{}\".format(num_policies-3)] = (make_RandomPolicy(1), obs_space, act_space, {}) # random policy stored as the last item in policies dictionary\n",
        "policies[\"policy_{}\".format(num_policies-2)] = (make_RandomPolicy(2), obs_space, act_space, {}) # random policy stored as the last item in policies dictionary\n",
        "policies[\"policy_{}\".format(num_policies-1)] = (make_RandomPolicy(3), obs_space, act_space, {}) # random policy stored as the last item in policies dictionary\n",
        "\"\"\"\n",
        "def set_RandomPolicy(policies):\n",
        "    for i in range(num_agents):\n",
        "        # random policy stored as the last item in policies dictionary\n",
        "        policies[\"policy_{}\".format(num_policies-1)] = (make_RandomPolicy(3), obs_space, act_space, {})\n",
        "\n",
        "    print('policies:', policies)\n",
        "\n",
        "    return 0\n",
        "set_RandomPolicy(policies)\n",
        "\n",
        "policy_ids = list(policies.keys())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "policies: {'policy_0': (None, Box(4, 10), Tuple(Discrete(3), Discrete(4), Box(1,), Box(1,), Discrete(12)), {'model': {'custom_model': 'model_disc'}, 'gamma': 0.99}), 'policy_1': (None, Box(4, 10), Tuple(Discrete(3), Discrete(4), Box(1,), Box(1,), Discrete(12)), {'model': {'custom_model': 'model_disc'}, 'gamma': 0.99}), 'policy_2': (None, Box(4, 10), Tuple(Discrete(3), Discrete(4), Box(1,), Box(1,), Discrete(12)), {'model': {'custom_model': 'model_disc'}, 'gamma': 0.99}), 'policy_3': (<class '__main__.make_RandomPolicy.<locals>.RandomPolicy'>, Box(4, 10), Tuple(Discrete(3), Discrete(4), Box(1,), Box(1,), Discrete(12)), {})}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVr7hhEni7u7",
        "colab_type": "code",
        "outputId": "d9c62130-200c-4122-e25e-f9173d570bc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tune.run(#PPOTrainer,\n",
        "          \"PPO\",\n",
        "          #\"PG\",\n",
        "          #queue_trials=False,\n",
        "          #resources_per_trial={\"cpu\": 2,\n",
        "          #                     \"gpu\": 0},\n",
        "\n",
        "          #stop={\"training_iteration\": args.num_iters},\n",
        "          #stop={\"timesteps_total\": max_step,\n",
        "          #      \"training_iteration\": num_iters},\n",
        "          stop={\"timesteps_total\": max_step * episode},\n",
        "\n",
        "          config={\"env\": \"continuousDoubleAuction-v0\",\n",
        "\n",
        "                  #\"log_level\": \"DEBUG\",\n",
        "                  #\"simple_optimizer\": args.simple,\n",
        "                  #\"simple_optimizer\": True,\n",
        "                  #\"num_sgd_iter\": 10,\n",
        "\n",
        "                  #\"gamma\": 0.9,\n",
        "\n",
        "                  # Number of rollout worker actors to create for parallel sampling.\n",
        "                  # Setting to 0 will force rollouts to be done in the trainer actor.\n",
        "                  \"num_workers\": 1, # Colab (only 2 CPUs or 1 GPU) #1\n",
        "                  \"num_envs_per_worker\": 2, #2\n",
        "\n",
        "                  #\"timesteps_per_iteration\": max_step,\n",
        "\n",
        "                  # https://github.com/ray-project/ray/issues/4628\n",
        "                  \"sample_batch_size\": 32, # number of environment steps sampled from each environment\n",
        "                  \"train_batch_size\": 128, # minibatch size must be >= 128, number of environment steps sampled from all available environments\n",
        "\n",
        "                  \"multiagent\": {\"policies_to_train\": [\"policy_0\"],\n",
        "                                 \"policies\": policies,\n",
        "                                 #\"policy_mapping_fn\": tune.function(lambda agent_id: random.choice(policy_ids)),\n",
        "                                 #\"policy_mapping_fn\": tune.function(policy_mapper),\n",
        "                                 \"policy_mapping_fn\": policy_mapper,\n",
        "                                },\n",
        "                  },\n",
        "          )\n",
        "\n",
        "#ray.shutdown()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 1.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc  </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>     </td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-15-10\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 21294.795\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.20000000298023224\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.987395286560059\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.02809048257768154\n",
            "        policy_loss: -0.16817311942577362\n",
            "        total_loss: 612441664.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 612441664.0\n",
            "    load_time_ms: 448.169\n",
            "    num_steps_sampled: 128\n",
            "    num_steps_trained: 128\n",
            "    sample_time_ms: 5448.473\n",
            "    update_time_ms: 23597.442\n",
            "  iterations_since_restore: 1\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 82.105\n",
            "    ram_util_percent: 18.078749999999996\n",
            "  pid: 1834\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 51.276058197021484\n",
            "  time_this_iter_s: 51.276058197021484\n",
            "  time_total_s: 51.276058197021484\n",
            "  timestamp: 1583406910\n",
            "  timesteps_since_restore: 128\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 128\n",
            "  training_iteration: 1\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         51.2761</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">     1</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-15-36\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 21256.643\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.9587602615356445\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.013769903220236301\n",
            "        policy_loss: -0.11672718077898026\n",
            "        total_loss: 3418343168.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 3418343168.0\n",
            "    load_time_ms: 224.691\n",
            "    num_steps_sampled: 256\n",
            "    num_steps_trained: 256\n",
            "    sample_time_ms: 5002.762\n",
            "    update_time_ms: 11841.507\n",
            "  iterations_since_restore: 2\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.63243243243244\n",
            "    ram_util_percent: 18.5\n",
            "  pid: 1834\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 77.14352107048035\n",
            "  time_this_iter_s: 25.867462873458862\n",
            "  time_total_s: 77.14352107048035\n",
            "  timestamp: 1583406936\n",
            "  timesteps_since_restore: 256\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 256\n",
            "  training_iteration: 2\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         77.1435</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">     2</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-16-00\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 20864.666\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.904134750366211\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.018707266077399254\n",
            "        policy_loss: -0.12486598640680313\n",
            "        total_loss: 1586682496.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 1586682496.0\n",
            "    load_time_ms: 150.318\n",
            "    num_steps_sampled: 384\n",
            "    num_steps_trained: 384\n",
            "    sample_time_ms: 4704.1\n",
            "    update_time_ms: 7921.611\n",
            "  iterations_since_restore: 3\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.62647058823529\n",
            "    ram_util_percent: 18.700000000000003\n",
            "  pid: 1834\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 101.42166233062744\n",
            "  time_this_iter_s: 24.278141260147095\n",
            "  time_total_s: 101.42166233062744\n",
            "  timestamp: 1583406960\n",
            "  timesteps_since_restore: 384\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 384\n",
            "  training_iteration: 3\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         101.422</td><td style=\"text-align: right;\"> 384</td><td style=\"text-align: right;\">     3</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-16-25\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 20697.245\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.931929588317871\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.008782545104622841\n",
            "        policy_loss: -0.07386830449104309\n",
            "        total_loss: 4409103360.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 4409103360.0\n",
            "    load_time_ms: 113.015\n",
            "    num_steps_sampled: 512\n",
            "    num_steps_trained: 512\n",
            "    sample_time_ms: 4543.786\n",
            "    update_time_ms: 5960.059\n",
            "  iterations_since_restore: 4\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.96857142857145\n",
            "    ram_util_percent: 19.0\n",
            "  pid: 1834\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 125.76042032241821\n",
            "  time_this_iter_s: 24.33875799179077\n",
            "  time_total_s: 125.76042032241821\n",
            "  timestamp: 1583406985\n",
            "  timesteps_since_restore: 512\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 512\n",
            "  training_iteration: 4\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">          125.76</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">     4</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-16-49\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 20588.032\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.91120719909668\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.01039392314851284\n",
            "        policy_loss: -0.08849842101335526\n",
            "        total_loss: 6458265600.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 6458265600.0\n",
            "    load_time_ms: 90.642\n",
            "    num_steps_sampled: 640\n",
            "    num_steps_trained: 640\n",
            "    sample_time_ms: 4481.652\n",
            "    update_time_ms: 4783.058\n",
            "  iterations_since_restore: 5\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.67142857142858\n",
            "    ram_util_percent: 19.202857142857148\n",
            "  pid: 1834\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 150.22494673728943\n",
            "  time_this_iter_s: 24.464526414871216\n",
            "  time_total_s: 150.22494673728943\n",
            "  timestamp: 1583407009\n",
            "  timesteps_since_restore: 640\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 640\n",
            "  training_iteration: 5\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         150.225</td><td style=\"text-align: right;\"> 640</td><td style=\"text-align: right;\">     5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-17-14\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 20519.743\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.90236759185791\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.011624272912740707\n",
            "        policy_loss: -0.10243949294090271\n",
            "        total_loss: 9618319360.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 9618319360.0\n",
            "    load_time_ms: 75.756\n",
            "    num_steps_sampled: 768\n",
            "    num_steps_trained: 768\n",
            "    sample_time_ms: 4431.511\n",
            "    update_time_ms: 3998.754\n",
            "  iterations_since_restore: 6\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 92.03428571428572\n",
            "    ram_util_percent: 19.5\n",
            "  pid: 1834\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 174.66794300079346\n",
            "  time_this_iter_s: 24.44299626350403\n",
            "  time_total_s: 174.66794300079346\n",
            "  timestamp: 1583407034\n",
            "  timesteps_since_restore: 768\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 768\n",
            "  training_iteration: 6\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         174.668</td><td style=\"text-align: right;\"> 768</td><td style=\"text-align: right;\">     6</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-17-38\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 20495.307\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.962882995605469\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.007509632967412472\n",
            "        policy_loss: -0.06798627972602844\n",
            "        total_loss: 21316874240.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 21316874240.0\n",
            "    load_time_ms: 65.145\n",
            "    num_steps_sampled: 896\n",
            "    num_steps_trained: 896\n",
            "    sample_time_ms: 4387.156\n",
            "    update_time_ms: 3441.082\n",
            "  iterations_since_restore: 7\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.96571428571428\n",
            "    ram_util_percent: 19.702857142857148\n",
            "  pid: 1834\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 199.24016785621643\n",
            "  time_this_iter_s: 24.572224855422974\n",
            "  time_total_s: 199.24016785621643\n",
            "  timestamp: 1583407058\n",
            "  timesteps_since_restore: 896\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 896\n",
            "  training_iteration: 7\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">          199.24</td><td style=\"text-align: right;\"> 896</td><td style=\"text-align: right;\">     7</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-18-03\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 20475.158\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.073963165283203\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.03159043565392494\n",
            "        policy_loss: -0.12397880852222443\n",
            "        total_loss: 39984111616.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 39984111616.0\n",
            "    load_time_ms: 57.143\n",
            "    num_steps_sampled: 1024\n",
            "    num_steps_trained: 1024\n",
            "    sample_time_ms: 4357.431\n",
            "    update_time_ms: 3020.465\n",
            "  iterations_since_restore: 8\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.92285714285715\n",
            "    ram_util_percent: 19.985714285714284\n",
            "  pid: 1834\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 223.80561184883118\n",
            "  time_this_iter_s: 24.565443992614746\n",
            "  time_total_s: 223.80561184883118\n",
            "  timestamp: 1583407083\n",
            "  timesteps_since_restore: 1024\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1024\n",
            "  training_iteration: 8\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         223.806</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">     8</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-18-28\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 20434.739\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.99238920211792\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.017367981374263763\n",
            "        policy_loss: -0.0977259948849678\n",
            "        total_loss: 38035361792.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 38035361792.0\n",
            "    load_time_ms: 51.012\n",
            "    num_steps_sampled: 1152\n",
            "    num_steps_trained: 1152\n",
            "    sample_time_ms: 4340.18\n",
            "    update_time_ms: 2693.335\n",
            "  iterations_since_restore: 9\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.93999999999998\n",
            "    ram_util_percent: 20.200000000000003\n",
            "  pid: 1834\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 248.20175218582153\n",
            "  time_this_iter_s: 24.396140336990356\n",
            "  time_total_s: 248.20175218582153\n",
            "  timestamp: 1583407108\n",
            "  timesteps_since_restore: 1152\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1152\n",
            "  training_iteration: 9\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.6/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         248.202</td><td style=\"text-align: right;\">1152</td><td style=\"text-align: right;\">     9</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-18-52\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 20411.645\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.929616928100586\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.015004642307758331\n",
            "        policy_loss: -0.09203654527664185\n",
            "        total_loss: 44127027200.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 44127027200.0\n",
            "    load_time_ms: 46.04\n",
            "    num_steps_sampled: 1280\n",
            "    num_steps_trained: 1280\n",
            "    sample_time_ms: 4331.123\n",
            "    update_time_ms: 2431.52\n",
            "  iterations_since_restore: 10\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.86857142857143\n",
            "    ram_util_percent: 20.399999999999995\n",
            "  pid: 1834\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 272.7356095314026\n",
            "  time_this_iter_s: 24.533857345581055\n",
            "  time_total_s: 272.7356095314026\n",
            "  timestamp: 1583407132\n",
            "  timesteps_since_restore: 1280\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1280\n",
            "  training_iteration: 10\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.6/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         272.736</td><td style=\"text-align: right;\">1280</td><td style=\"text-align: right;\">    10</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-19-16\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 2\n",
            "  episodes_total: 2\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 20287.253\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.972887992858887\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.010261467657983303\n",
            "        policy_loss: -0.06704235821962357\n",
            "        total_loss: 58361610240.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 58361610240.0\n",
            "    load_time_ms: 1.38\n",
            "    num_steps_sampled: 1408\n",
            "    num_steps_trained: 1408\n",
            "    sample_time_ms: 4203.969\n",
            "    update_time_ms: 79.815\n",
            "  iterations_since_restore: 11\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.77142857142859\n",
            "    ram_util_percent: 20.700000000000003\n",
            "  pid: 1834\n",
            "  policy_reward_max:\n",
            "    policy_0: 233985.0\n",
            "    policy_1: -138678.0\n",
            "    policy_2: 257620.0\n",
            "    policy_3: 169934.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 22085.5\n",
            "    policy_1: -188209.0\n",
            "    policy_2: 157081.0\n",
            "    policy_3: 9042.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -189814.0\n",
            "    policy_1: -237740.0\n",
            "    policy_2: 56542.0\n",
            "    policy_3: -151849.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 49.055883394065475\n",
            "    mean_inference_ms: 14.316562219714442\n",
            "    mean_processing_ms: 2.174516096182749\n",
            "  time_since_restore: 297.05029225349426\n",
            "  time_this_iter_s: 24.314682722091675\n",
            "  time_total_s: 297.05029225349426\n",
            "  timestamp: 1583407156\n",
            "  timesteps_since_restore: 1408\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1408\n",
            "  training_iteration: 11\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.6/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">          297.05</td><td style=\"text-align: right;\">1408</td><td style=\"text-align: right;\">    11</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-19-40\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 20139.843\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.962658882141113\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.015129053965210915\n",
            "        policy_loss: -0.1415550410747528\n",
            "        total_loss: 1669381248.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 1669381248.0\n",
            "    load_time_ms: 1.397\n",
            "    num_steps_sampled: 1536\n",
            "    num_steps_trained: 1536\n",
            "    sample_time_ms: 4131.047\n",
            "    update_time_ms: 78.777\n",
            "  iterations_since_restore: 12\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 92.11470588235295\n",
            "    ram_util_percent: 20.9\n",
            "  pid: 1834\n",
            "  policy_reward_max:\n",
            "    policy_0: 233985.0\n",
            "    policy_1: -138678.0\n",
            "    policy_2: 257620.0\n",
            "    policy_3: 169934.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 22085.5\n",
            "    policy_1: -188209.0\n",
            "    policy_2: 157081.0\n",
            "    policy_3: 9042.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -189814.0\n",
            "    policy_1: -237740.0\n",
            "    policy_2: 56542.0\n",
            "    policy_3: -151849.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 49.055883394065475\n",
            "    mean_inference_ms: 14.316562219714442\n",
            "    mean_processing_ms: 2.174516096182749\n",
            "  time_since_restore: 320.7042052745819\n",
            "  time_this_iter_s: 23.653913021087646\n",
            "  time_total_s: 320.7042052745819\n",
            "  timestamp: 1583407180\n",
            "  timesteps_since_restore: 1536\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1536\n",
            "  training_iteration: 12\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         320.704</td><td style=\"text-align: right;\">1536</td><td style=\"text-align: right;\">    12</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-20-04\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 20074.35\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.965651035308838\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.013057589530944824\n",
            "        policy_loss: -0.11964069306850433\n",
            "        total_loss: 1329384960.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 1329384960.0\n",
            "    load_time_ms: 1.366\n",
            "    num_steps_sampled: 1664\n",
            "    num_steps_trained: 1664\n",
            "    sample_time_ms: 4115.484\n",
            "    update_time_ms: 78.085\n",
            "  iterations_since_restore: 13\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.85151515151514\n",
            "    ram_util_percent: 21.2\n",
            "  pid: 1834\n",
            "  policy_reward_max:\n",
            "    policy_0: 233985.0\n",
            "    policy_1: -138678.0\n",
            "    policy_2: 257620.0\n",
            "    policy_3: 169934.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 22085.5\n",
            "    policy_1: -188209.0\n",
            "    policy_2: 157081.0\n",
            "    policy_3: 9042.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -189814.0\n",
            "    policy_1: -237740.0\n",
            "    policy_2: 56542.0\n",
            "    policy_3: -151849.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 49.055883394065475\n",
            "    mean_inference_ms: 14.316562219714442\n",
            "    mean_processing_ms: 2.174516096182749\n",
            "  time_since_restore: 344.16218280792236\n",
            "  time_this_iter_s: 23.457977533340454\n",
            "  time_total_s: 344.16218280792236\n",
            "  timestamp: 1583407204\n",
            "  timesteps_since_restore: 1664\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1664\n",
            "  training_iteration: 13\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         344.162</td><td style=\"text-align: right;\">1664</td><td style=\"text-align: right;\">    13</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-20-28\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 20097.778\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.027559280395508\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.02420186623930931\n",
            "        policy_loss: -0.12512977421283722\n",
            "        total_loss: 2454575360.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 2454575360.0\n",
            "    load_time_ms: 1.375\n",
            "    num_steps_sampled: 1792\n",
            "    num_steps_trained: 1792\n",
            "    sample_time_ms: 4110.427\n",
            "    update_time_ms: 78.764\n",
            "  iterations_since_restore: 14\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 92.15428571428572\n",
            "    ram_util_percent: 21.399999999999995\n",
            "  pid: 1834\n",
            "  policy_reward_max:\n",
            "    policy_0: 233985.0\n",
            "    policy_1: -138678.0\n",
            "    policy_2: 257620.0\n",
            "    policy_3: 169934.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 22085.5\n",
            "    policy_1: -188209.0\n",
            "    policy_2: 157081.0\n",
            "    policy_3: 9042.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -189814.0\n",
            "    policy_1: -237740.0\n",
            "    policy_2: 56542.0\n",
            "    policy_3: -151849.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 49.055883394065475\n",
            "    mean_inference_ms: 14.316562219714442\n",
            "    mean_processing_ms: 2.174516096182749\n",
            "  time_since_restore: 368.6941957473755\n",
            "  time_this_iter_s: 24.532012939453125\n",
            "  time_total_s: 368.6941957473755\n",
            "  timestamp: 1583407228\n",
            "  timesteps_since_restore: 1792\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1792\n",
            "  training_iteration: 14\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         368.694</td><td style=\"text-align: right;\">1792</td><td style=\"text-align: right;\">    14</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-20-52\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 20038.939\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.946898460388184\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.013376566581428051\n",
            "        policy_loss: -0.09668725728988647\n",
            "        total_loss: 3957296896.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 3957296896.0\n",
            "    load_time_ms: 1.392\n",
            "    num_steps_sampled: 1920\n",
            "    num_steps_trained: 1920\n",
            "    sample_time_ms: 4104.656\n",
            "    update_time_ms: 79.088\n",
            "  iterations_since_restore: 15\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.48235294117646\n",
            "    ram_util_percent: 21.700000000000003\n",
            "  pid: 1834\n",
            "  policy_reward_max:\n",
            "    policy_0: 233985.0\n",
            "    policy_1: -138678.0\n",
            "    policy_2: 257620.0\n",
            "    policy_3: 169934.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 22085.5\n",
            "    policy_1: -188209.0\n",
            "    policy_2: 157081.0\n",
            "    policy_3: 9042.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -189814.0\n",
            "    policy_1: -237740.0\n",
            "    policy_2: 56542.0\n",
            "    policy_3: -151849.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 49.055883394065475\n",
            "    mean_inference_ms: 14.316562219714442\n",
            "    mean_processing_ms: 2.174516096182749\n",
            "  time_since_restore: 392.51664662361145\n",
            "  time_this_iter_s: 23.822450876235962\n",
            "  time_total_s: 392.51664662361145\n",
            "  timestamp: 1583407252\n",
            "  timesteps_since_restore: 1920\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1920\n",
            "  training_iteration: 15\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         392.517</td><td style=\"text-align: right;\">1920</td><td style=\"text-align: right;\">    15</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-21-16\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 19980.038\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.930268287658691\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.014988604933023453\n",
            "        policy_loss: -0.08741146326065063\n",
            "        total_loss: 2971297024.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 2971297024.0\n",
            "    load_time_ms: 1.415\n",
            "    num_steps_sampled: 2048\n",
            "    num_steps_trained: 2048\n",
            "    sample_time_ms: 4103.8\n",
            "    update_time_ms: 79.387\n",
            "  iterations_since_restore: 16\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.59117647058824\n",
            "    ram_util_percent: 21.9\n",
            "  pid: 1834\n",
            "  policy_reward_max:\n",
            "    policy_0: 233985.0\n",
            "    policy_1: -138678.0\n",
            "    policy_2: 257620.0\n",
            "    policy_3: 169934.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 22085.5\n",
            "    policy_1: -188209.0\n",
            "    policy_2: 157081.0\n",
            "    policy_3: 9042.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -189814.0\n",
            "    policy_1: -237740.0\n",
            "    policy_2: 56542.0\n",
            "    policy_3: -151849.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 49.055883394065475\n",
            "    mean_inference_ms: 14.316562219714442\n",
            "    mean_processing_ms: 2.174516096182749\n",
            "  time_since_restore: 416.36485719680786\n",
            "  time_this_iter_s: 23.84821057319641\n",
            "  time_total_s: 416.36485719680786\n",
            "  timestamp: 1583407276\n",
            "  timesteps_since_restore: 2048\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2048\n",
            "  training_iteration: 16\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         416.365</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">    16</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-21-40\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 19939.483\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.974937438964844\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.011054407805204391\n",
            "        policy_loss: -0.08955048769712448\n",
            "        total_loss: 1701769472.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 1701769472.0\n",
            "    load_time_ms: 1.404\n",
            "    num_steps_sampled: 2176\n",
            "    num_steps_trained: 2176\n",
            "    sample_time_ms: 4126.85\n",
            "    update_time_ms: 76.92\n",
            "  iterations_since_restore: 17\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.72285714285714\n",
            "    ram_util_percent: 22.171428571428578\n",
            "  pid: 1834\n",
            "  policy_reward_max:\n",
            "    policy_0: 233985.0\n",
            "    policy_1: -138678.0\n",
            "    policy_2: 257620.0\n",
            "    policy_3: 169934.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 22085.5\n",
            "    policy_1: -188209.0\n",
            "    policy_2: 157081.0\n",
            "    policy_3: 9042.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -189814.0\n",
            "    policy_1: -237740.0\n",
            "    policy_2: 56542.0\n",
            "    policy_3: -151849.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 49.055883394065475\n",
            "    mean_inference_ms: 14.316562219714442\n",
            "    mean_processing_ms: 2.174516096182749\n",
            "  time_since_restore: 440.73598408699036\n",
            "  time_this_iter_s: 24.371126890182495\n",
            "  time_total_s: 440.73598408699036\n",
            "  timestamp: 1583407300\n",
            "  timesteps_since_restore: 2176\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2176\n",
            "  training_iteration: 17\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         440.736</td><td style=\"text-align: right;\">2176</td><td style=\"text-align: right;\">    17</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-22-05\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 19928.481\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.95266056060791\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.00991404801607132\n",
            "        policy_loss: -0.06565410643815994\n",
            "        total_loss: 3199649792.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 3199649792.0\n",
            "    load_time_ms: 1.414\n",
            "    num_steps_sampled: 2304\n",
            "    num_steps_trained: 2304\n",
            "    sample_time_ms: 4131.333\n",
            "    update_time_ms: 76.696\n",
            "  iterations_since_restore: 18\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.72\n",
            "    ram_util_percent: 22.399999999999995\n",
            "  pid: 1834\n",
            "  policy_reward_max:\n",
            "    policy_0: 233985.0\n",
            "    policy_1: -138678.0\n",
            "    policy_2: 257620.0\n",
            "    policy_3: 169934.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 22085.5\n",
            "    policy_1: -188209.0\n",
            "    policy_2: 157081.0\n",
            "    policy_3: 9042.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -189814.0\n",
            "    policy_1: -237740.0\n",
            "    policy_2: 56542.0\n",
            "    policy_3: -151849.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 49.055883394065475\n",
            "    mean_inference_ms: 14.316562219714442\n",
            "    mean_processing_ms: 2.174516096182749\n",
            "  time_since_restore: 465.2342035770416\n",
            "  time_this_iter_s: 24.49821949005127\n",
            "  time_total_s: 465.2342035770416\n",
            "  timestamp: 1583407325\n",
            "  timesteps_since_restore: 2304\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2304\n",
            "  training_iteration: 18\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.9/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         465.234</td><td style=\"text-align: right;\">2304</td><td style=\"text-align: right;\">    18</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-22-30\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 19944.636\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.010228157043457\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.016159020364284515\n",
            "        policy_loss: -0.1041269451379776\n",
            "        total_loss: 2128034816.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 2128034816.0\n",
            "    load_time_ms: 1.336\n",
            "    num_steps_sampled: 2432\n",
            "    num_steps_trained: 2432\n",
            "    sample_time_ms: 4130.431\n",
            "    update_time_ms: 76.519\n",
            "  iterations_since_restore: 19\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.46285714285716\n",
            "    ram_util_percent: 22.691428571428574\n",
            "  pid: 1834\n",
            "  policy_reward_max:\n",
            "    policy_0: 233985.0\n",
            "    policy_1: -138678.0\n",
            "    policy_2: 257620.0\n",
            "    policy_3: 169934.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 22085.5\n",
            "    policy_1: -188209.0\n",
            "    policy_2: 157081.0\n",
            "    policy_3: 9042.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -189814.0\n",
            "    policy_1: -237740.0\n",
            "    policy_2: 56542.0\n",
            "    policy_3: -151849.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 49.055883394065475\n",
            "    mean_inference_ms: 14.316562219714442\n",
            "    mean_processing_ms: 2.174516096182749\n",
            "  time_since_restore: 489.78077006340027\n",
            "  time_this_iter_s: 24.546566486358643\n",
            "  time_total_s: 489.78077006340027\n",
            "  timestamp: 1583407350\n",
            "  timesteps_since_restore: 2432\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2432\n",
            "  training_iteration: 19\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.9/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         489.781</td><td style=\"text-align: right;\">2432</td><td style=\"text-align: right;\">    19</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-22-54\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 19949.178\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.954870223999023\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.009623068384826183\n",
            "        policy_loss: -0.07512757182121277\n",
            "        total_loss: 2555288576.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 2555288576.0\n",
            "    load_time_ms: 1.373\n",
            "    num_steps_sampled: 2560\n",
            "    num_steps_trained: 2560\n",
            "    sample_time_ms: 4135.133\n",
            "    update_time_ms: 76.609\n",
            "  iterations_since_restore: 20\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.57714285714286\n",
            "    ram_util_percent: 22.899999999999995\n",
            "  pid: 1834\n",
            "  policy_reward_max:\n",
            "    policy_0: 233985.0\n",
            "    policy_1: -138678.0\n",
            "    policy_2: 257620.0\n",
            "    policy_3: 169934.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 22085.5\n",
            "    policy_1: -188209.0\n",
            "    policy_2: 157081.0\n",
            "    policy_3: 9042.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -189814.0\n",
            "    policy_1: -237740.0\n",
            "    policy_2: 56542.0\n",
            "    policy_3: -151849.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 49.055883394065475\n",
            "    mean_inference_ms: 14.316562219714442\n",
            "    mean_processing_ms: 2.174516096182749\n",
            "  time_since_restore: 514.4102265834808\n",
            "  time_this_iter_s: 24.629456520080566\n",
            "  time_total_s: 514.4102265834808\n",
            "  timestamp: 1583407374\n",
            "  timesteps_since_restore: 2560\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2560\n",
            "  training_iteration: 20\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.9/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">          514.41</td><td style=\"text-align: right;\">2560</td><td style=\"text-align: right;\">    20</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-23-19\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 19954.01\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.9588623046875\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.012893831357359886\n",
            "        policy_loss: -0.0773068517446518\n",
            "        total_loss: 1131985792.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 1131985792.0\n",
            "    load_time_ms: 1.412\n",
            "    num_steps_sampled: 2688\n",
            "    num_steps_trained: 2688\n",
            "    sample_time_ms: 4161.469\n",
            "    update_time_ms: 75.984\n",
            "  iterations_since_restore: 21\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.71388888888889\n",
            "    ram_util_percent: 23.152777777777782\n",
            "  pid: 1834\n",
            "  policy_reward_max:\n",
            "    policy_0: 233985.0\n",
            "    policy_1: -138678.0\n",
            "    policy_2: 257620.0\n",
            "    policy_3: 169934.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 22085.5\n",
            "    policy_1: -188209.0\n",
            "    policy_2: 157081.0\n",
            "    policy_3: 9042.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -189814.0\n",
            "    policy_1: -237740.0\n",
            "    policy_2: 56542.0\n",
            "    policy_3: -151849.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 49.055883394065475\n",
            "    mean_inference_ms: 14.316562219714442\n",
            "    mean_processing_ms: 2.174516096182749\n",
            "  time_since_restore: 539.0314102172852\n",
            "  time_this_iter_s: 24.62118363380432\n",
            "  time_total_s: 539.0314102172852\n",
            "  timestamp: 1583407399\n",
            "  timesteps_since_restore: 2688\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2688\n",
            "  training_iteration: 21\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.9/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         539.031</td><td style=\"text-align: right;\">2688</td><td style=\"text-align: right;\">    21</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-23-44\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 2\n",
            "  episodes_total: 4\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 20037.848\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.947781562805176\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.010122728534042835\n",
            "        policy_loss: -0.08705423772335052\n",
            "        total_loss: 813847168.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 813847168.0\n",
            "    load_time_ms: 1.438\n",
            "    num_steps_sampled: 2816\n",
            "    num_steps_trained: 2816\n",
            "    sample_time_ms: 4208.914\n",
            "    update_time_ms: 78.442\n",
            "  iterations_since_restore: 22\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.59142857142858\n",
            "    ram_util_percent: 23.399999999999995\n",
            "  pid: 1834\n",
            "  policy_reward_max:\n",
            "    policy_0: 233985.0\n",
            "    policy_1: 90130.0\n",
            "    policy_2: 257620.0\n",
            "    policy_3: 169934.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 55398.25\n",
            "    policy_1: -52415.0\n",
            "    policy_2: 14199.75\n",
            "    policy_3: -17183.0\n",
            "  policy_reward_min:\n",
            "    policy_0: -189814.0\n",
            "    policy_1: -237740.0\n",
            "    policy_2: -136506.0\n",
            "    policy_3: -151849.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 49.165552834406625\n",
            "    mean_inference_ms: 13.947006316436749\n",
            "    mean_processing_ms: 2.1776593280092142\n",
            "  time_since_restore: 564.0254890918732\n",
            "  time_this_iter_s: 24.994078874588013\n",
            "  time_total_s: 564.0254890918732\n",
            "  timestamp: 1583407424\n",
            "  timesteps_since_restore: 2816\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2816\n",
            "  training_iteration: 22\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         564.025</td><td style=\"text-align: right;\">2816</td><td style=\"text-align: right;\">    22</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-24-08\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 20139.659\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.936327934265137\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.014880742877721786\n",
            "        policy_loss: -0.13314585387706757\n",
            "        total_loss: 546189696.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 546189696.0\n",
            "    load_time_ms: 1.434\n",
            "    num_steps_sampled: 2944\n",
            "    num_steps_trained: 2944\n",
            "    sample_time_ms: 4187.883\n",
            "    update_time_ms: 78.263\n",
            "  iterations_since_restore: 23\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 92.55714285714286\n",
            "    ram_util_percent: 23.688571428571432\n",
            "  pid: 1834\n",
            "  policy_reward_max:\n",
            "    policy_0: 233985.0\n",
            "    policy_1: 90130.0\n",
            "    policy_2: 257620.0\n",
            "    policy_3: 169934.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 55398.25\n",
            "    policy_1: -52415.0\n",
            "    policy_2: 14199.75\n",
            "    policy_3: -17183.0\n",
            "  policy_reward_min:\n",
            "    policy_0: -189814.0\n",
            "    policy_1: -237740.0\n",
            "    policy_2: -136506.0\n",
            "    policy_3: -151849.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 49.16555283440663\n",
            "    mean_inference_ms: 13.947006316436749\n",
            "    mean_processing_ms: 2.177659328009214\n",
            "  time_since_restore: 588.2891561985016\n",
            "  time_this_iter_s: 24.263667106628418\n",
            "  time_total_s: 588.2891561985016\n",
            "  timestamp: 1583407448\n",
            "  timesteps_since_restore: 2944\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2944\n",
            "  training_iteration: 23\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         588.289</td><td style=\"text-align: right;\">2944</td><td style=\"text-align: right;\">    23</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-24-33\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 20131.381\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.895956039428711\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.011894438415765762\n",
            "        policy_loss: -0.0847788155078888\n",
            "        total_loss: 525458848.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 525458848.0\n",
            "    load_time_ms: 1.458\n",
            "    num_steps_sampled: 3072\n",
            "    num_steps_trained: 3072\n",
            "    sample_time_ms: 4183.621\n",
            "    update_time_ms: 77.398\n",
            "  iterations_since_restore: 24\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 92.13428571428572\n",
            "    ram_util_percent: 23.899999999999995\n",
            "  pid: 1834\n",
            "  policy_reward_max:\n",
            "    policy_0: 233985.0\n",
            "    policy_1: 90130.0\n",
            "    policy_2: 257620.0\n",
            "    policy_3: 169934.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 55398.25\n",
            "    policy_1: -52415.0\n",
            "    policy_2: 14199.75\n",
            "    policy_3: -17183.0\n",
            "  policy_reward_min:\n",
            "    policy_0: -189814.0\n",
            "    policy_1: -237740.0\n",
            "    policy_2: -136506.0\n",
            "    policy_3: -151849.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 49.16555283440663\n",
            "    mean_inference_ms: 13.947006316436749\n",
            "    mean_processing_ms: 2.177659328009214\n",
            "  time_since_restore: 612.6854822635651\n",
            "  time_this_iter_s: 24.396326065063477\n",
            "  time_total_s: 612.6854822635651\n",
            "  timestamp: 1583407473\n",
            "  timesteps_since_restore: 3072\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3072\n",
            "  training_iteration: 24\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         612.685</td><td style=\"text-align: right;\">3072</td><td style=\"text-align: right;\">    24</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-24-57\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 20137.423\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.876214027404785\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.015245914459228516\n",
            "        policy_loss: -0.11624953150749207\n",
            "        total_loss: 390694848.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 390694848.0\n",
            "    load_time_ms: 1.441\n",
            "    num_steps_sampled: 3200\n",
            "    num_steps_trained: 3200\n",
            "    sample_time_ms: 4187.062\n",
            "    update_time_ms: 76.909\n",
            "  iterations_since_restore: 25\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.47941176470587\n",
            "    ram_util_percent: 24.1\n",
            "  pid: 1834\n",
            "  policy_reward_max:\n",
            "    policy_0: 233985.0\n",
            "    policy_1: 90130.0\n",
            "    policy_2: 257620.0\n",
            "    policy_3: 169934.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 55398.25\n",
            "    policy_1: -52415.0\n",
            "    policy_2: 14199.75\n",
            "    policy_3: -17183.0\n",
            "  policy_reward_min:\n",
            "    policy_0: -189814.0\n",
            "    policy_1: -237740.0\n",
            "    policy_2: -136506.0\n",
            "    policy_3: -151849.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 49.16555283440663\n",
            "    mean_inference_ms: 13.947006316436749\n",
            "    mean_processing_ms: 2.177659328009214\n",
            "  time_since_restore: 636.5984725952148\n",
            "  time_this_iter_s: 23.91299033164978\n",
            "  time_total_s: 636.5984725952148\n",
            "  timestamp: 1583407497\n",
            "  timesteps_since_restore: 3200\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3200\n",
            "  training_iteration: 25\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.1/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         636.598</td><td style=\"text-align: right;\">3200</td><td style=\"text-align: right;\">    25</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-25-21\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 20187.023\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.8928542137146\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.009368586353957653\n",
            "        policy_loss: -0.04453887417912483\n",
            "        total_loss: 27189720.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 27189720.0\n",
            "    load_time_ms: 1.404\n",
            "    num_steps_sampled: 3328\n",
            "    num_steps_trained: 3328\n",
            "    sample_time_ms: 4189.047\n",
            "    update_time_ms: 77.438\n",
            "  iterations_since_restore: 26\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.7057142857143\n",
            "    ram_util_percent: 24.399999999999995\n",
            "  pid: 1834\n",
            "  policy_reward_max:\n",
            "    policy_0: 233985.0\n",
            "    policy_1: 90130.0\n",
            "    policy_2: 257620.0\n",
            "    policy_3: 169934.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 55398.25\n",
            "    policy_1: -52415.0\n",
            "    policy_2: 14199.75\n",
            "    policy_3: -17183.0\n",
            "  policy_reward_min:\n",
            "    policy_0: -189814.0\n",
            "    policy_1: -237740.0\n",
            "    policy_2: -136506.0\n",
            "    policy_3: -151849.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 49.16555283440663\n",
            "    mean_inference_ms: 13.947006316436749\n",
            "    mean_processing_ms: 2.177659328009214\n",
            "  time_since_restore: 660.9672675132751\n",
            "  time_this_iter_s: 24.368794918060303\n",
            "  time_total_s: 660.9672675132751\n",
            "  timestamp: 1583407521\n",
            "  timesteps_since_restore: 3328\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3328\n",
            "  training_iteration: 26\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.1/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         660.967</td><td style=\"text-align: right;\">3328</td><td style=\"text-align: right;\">    26</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-25-46\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 20275.057\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.911438941955566\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.013036882504820824\n",
            "        policy_loss: -0.07488369941711426\n",
            "        total_loss: 25527776.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 25527776.0\n",
            "    load_time_ms: 1.412\n",
            "    num_steps_sampled: 3456\n",
            "    num_steps_trained: 3456\n",
            "    sample_time_ms: 4186.787\n",
            "    update_time_ms: 78.028\n",
            "  iterations_since_restore: 27\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.57777777777778\n",
            "    ram_util_percent: 24.600000000000005\n",
            "  pid: 1834\n",
            "  policy_reward_max:\n",
            "    policy_0: 233985.0\n",
            "    policy_1: 90130.0\n",
            "    policy_2: 257620.0\n",
            "    policy_3: 169934.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 55398.25\n",
            "    policy_1: -52415.0\n",
            "    policy_2: 14199.75\n",
            "    policy_3: -17183.0\n",
            "  policy_reward_min:\n",
            "    policy_0: -189814.0\n",
            "    policy_1: -237740.0\n",
            "    policy_2: -136506.0\n",
            "    policy_3: -151849.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 49.16555283440663\n",
            "    mean_inference_ms: 13.947006316436749\n",
            "    mean_processing_ms: 2.177659328009214\n",
            "  time_since_restore: 686.2020659446716\n",
            "  time_this_iter_s: 25.234798431396484\n",
            "  time_total_s: 686.2020659446716\n",
            "  timestamp: 1583407546\n",
            "  timesteps_since_restore: 3456\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3456\n",
            "  training_iteration: 27\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.1/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>RUNNING </td><td>172.28.0.2:1834</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         686.202</td><td style=\"text-align: right;\">3456</td><td style=\"text-align: right;\">    27</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_5b8fb402:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-05_11-26-11\n",
            "  done: true\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 0fe50dfa42c744d989f7bc04b87ea835\n",
            "  experiment_tag: '0'\n",
            "  hostname: e4eefd8a6e43\n",
            "  info:\n",
            "    grad_time_ms: 20237.497\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.912782669067383\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.012627401389181614\n",
            "        policy_loss: -0.061819061636924744\n",
            "        total_loss: 92019088.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 92019088.0\n",
            "    load_time_ms: 1.476\n",
            "    num_steps_sampled: 3584\n",
            "    num_steps_trained: 3584\n",
            "    sample_time_ms: 4192.439\n",
            "    update_time_ms: 78.294\n",
            "  iterations_since_restore: 28\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.66285714285713\n",
            "    ram_util_percent: 24.799999999999997\n",
            "  pid: 1834\n",
            "  policy_reward_max:\n",
            "    policy_0: 233985.0\n",
            "    policy_1: 90130.0\n",
            "    policy_2: 257620.0\n",
            "    policy_3: 169934.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 55398.25\n",
            "    policy_1: -52415.0\n",
            "    policy_2: 14199.75\n",
            "    policy_3: -17183.0\n",
            "  policy_reward_min:\n",
            "    policy_0: -189814.0\n",
            "    policy_1: -237740.0\n",
            "    policy_2: -136506.0\n",
            "    policy_3: -151849.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 49.16555283440663\n",
            "    mean_inference_ms: 13.947006316436749\n",
            "    mean_processing_ms: 2.177659328009214\n",
            "  time_since_restore: 710.3850796222687\n",
            "  time_this_iter_s: 24.183013677597046\n",
            "  time_total_s: 710.3850796222687\n",
            "  timestamp: 1583407571\n",
            "  timesteps_since_restore: 3584\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3584\n",
            "  training_iteration: 28\n",
            "  trial_id: 5b8fb402\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.1/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         710.385</td><td style=\"text-align: right;\">3584</td><td style=\"text-align: right;\">    28</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.1/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_5b8fb402</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         710.385</td><td style=\"text-align: right;\">3584</td><td style=\"text-align: right;\">    28</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2020-03-05 11:26:11,396\tINFO tune.py:352 -- Returning an analysis object by default. You can call `analysis.trials` to retrieve a list of trials. This message will be removed in future versions of Tune.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f0e514809e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}