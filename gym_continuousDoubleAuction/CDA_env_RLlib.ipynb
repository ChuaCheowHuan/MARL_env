{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CDA_env_RLlib.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAqVG2cqjLXM",
        "colab_type": "code",
        "outputId": "f57fa457-f601-4635-933e-aabca862deb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Needed to switch directory in Google drive so as to import MARL env.\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd \"/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction/\"\n",
        "#%cd \"/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction/gym_continuousDoubleAuction/\"\n",
        "\n",
        "!pwd\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "!pip show tensorflow\n",
        "!pip show ray"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction\n",
            "/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction\n",
            "Requirement already satisfied: absl-py==0.9.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.9.0)\n",
            "Requirement already satisfied: aiohttp==3.6.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (3.6.2)\n",
            "Requirement already satisfied: astor==0.8.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.8.1)\n",
            "Requirement already satisfied: async-timeout==3.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (3.0.1)\n",
            "Requirement already satisfied: atari-py==0.2.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.2.6)\n",
            "Requirement already satisfied: attrs==19.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (19.3.0)\n",
            "Requirement already satisfied: beautifulsoup4==4.8.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (4.8.2)\n",
            "Requirement already satisfied: cachetools==4.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (4.0.0)\n",
            "Requirement already satisfied: certifi==2019.11.28 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (2019.11.28)\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (3.0.4)\n",
            "Requirement already satisfied: Click==7.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (7.0)\n",
            "Requirement already satisfied: cloudpickle==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: colorama==0.4.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 13)) (0.4.3)\n",
            "Requirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 14)) (3.0.12)\n",
            "Requirement already satisfied: funcsigs==1.0.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 15)) (1.0.2)\n",
            "Requirement already satisfied: future==0.18.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 16)) (0.18.2)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 17)) (0.2.2)\n",
            "Requirement already satisfied: google==2.0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 18)) (2.0.3)\n",
            "Requirement already satisfied: google-auth==1.11.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 19)) (1.11.2)\n",
            "Requirement already satisfied: google-auth-oauthlib==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 20)) (0.4.1)\n",
            "Requirement already satisfied: google-pasta==0.1.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 21)) (0.1.8)\n",
            "Requirement already satisfied: grpcio==1.27.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 22)) (1.27.2)\n",
            "Requirement already satisfied: gym==0.17.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 23)) (0.17.0)\n",
            "Obtaining gym_continuousDoubleAuction from git+https://github.com/ChuaCheowHuan/gym-continuousDoubleAuction.git@c897137cbcc93ca71cbd51c27e683c3298f6562d#egg=gym_continuousDoubleAuction (from -r requirements.txt (line 24))\n",
            "  Skipping because already up-to-date.\n",
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 25)) (2.10.0)\n",
            "Requirement already satisfied: idna==2.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 26)) (2.9)\n",
            "Requirement already satisfied: importlib-metadata==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 27)) (1.5.0)\n",
            "Requirement already satisfied: joblib==0.14.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 28)) (0.14.1)\n",
            "Requirement already satisfied: jsonschema==3.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 29)) (3.2.0)\n",
            "Requirement already satisfied: Keras-Applications==1.0.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 30)) (1.0.8)\n",
            "Requirement already satisfied: Keras-Preprocessing==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 31)) (1.1.0)\n",
            "Requirement already satisfied: lz4==3.0.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 32)) (3.0.2)\n",
            "Requirement already satisfied: Markdown==3.2.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 33)) (3.2.1)\n",
            "Requirement already satisfied: more-itertools==8.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 34)) (8.2.0)\n",
            "Requirement already satisfied: multidict==4.7.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 35)) (4.7.5)\n",
            "Requirement already satisfied: numpy==1.18.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 36)) (1.18.1)\n",
            "Requirement already satisfied: oauthlib==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 37)) (3.1.0)\n",
            "Requirement already satisfied: opencv-python==4.2.0.32 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 38)) (4.2.0.32)\n",
            "Requirement already satisfied: opencv-python-headless==4.2.0.32 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 39)) (4.2.0.32)\n",
            "Requirement already satisfied: opt-einsum==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 40)) (3.1.0)\n",
            "Requirement already satisfied: packaging==20.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 41)) (20.1)\n",
            "Requirement already satisfied: pandas==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 42)) (1.0.1)\n",
            "Requirement already satisfied: Pillow==7.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 43)) (7.0.0)\n",
            "Requirement already satisfied: pluggy==0.13.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 44)) (0.13.1)\n",
            "Requirement already satisfied: protobuf==3.11.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 45)) (3.11.3)\n",
            "Requirement already satisfied: py==1.8.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 46)) (1.8.1)\n",
            "Requirement already satisfied: py-spy==0.3.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 47)) (0.3.3)\n",
            "Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 48)) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 49)) (0.2.8)\n",
            "Requirement already satisfied: pyglet==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 50)) (1.5.0)\n",
            "Requirement already satisfied: pyparsing==2.4.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 51)) (2.4.6)\n",
            "Requirement already satisfied: pyrsistent==0.15.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 52)) (0.15.7)\n",
            "Requirement already satisfied: pytest==5.3.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 53)) (5.3.5)\n",
            "Requirement already satisfied: python-dateutil==2.8.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 54)) (2.8.1)\n",
            "Requirement already satisfied: pytz==2019.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 55)) (2019.3)\n",
            "Requirement already satisfied: PyYAML==5.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 56)) (5.3)\n",
            "Requirement already satisfied: ray==0.8.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 57)) (0.8.2)\n",
            "Requirement already satisfied: redis==3.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 58)) (3.4.1)\n",
            "Requirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 59)) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 60)) (1.3.0)\n",
            "Requirement already satisfied: rsa==4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 61)) (4.0)\n",
            "Requirement already satisfied: scikit-learn==0.22.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 62)) (0.22.2)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 63)) (1.4.1)\n",
            "Requirement already satisfied: six==1.14.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 64)) (1.14.0)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 65)) (0.0)\n",
            "Requirement already satisfied: sortedcontainers==2.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 66)) (2.1.0)\n",
            "Requirement already satisfied: soupsieve==2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 67)) (2.0)\n",
            "Requirement already satisfied: tabulate==0.8.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 68)) (0.8.6)\n",
            "Requirement already satisfied: tensorboard==2.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 69)) (2.1.0)\n",
            "Requirement already satisfied: tensorboardX==2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 70)) (2.0)\n",
            "Requirement already satisfied: tensorflow==2.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 71)) (2.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator==2.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 72)) (2.1.0)\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 73)) (1.1.0)\n",
            "Requirement already satisfied: urllib3==1.25.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 74)) (1.25.8)\n",
            "Requirement already satisfied: wcwidth==0.1.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 75)) (0.1.8)\n",
            "Requirement already satisfied: Werkzeug==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 76)) (1.0.0)\n",
            "Requirement already satisfied: wrapt==1.12.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 77)) (1.12.0)\n",
            "Requirement already satisfied: yarl==1.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 78)) (1.4.2)\n",
            "Requirement already satisfied: zipp==3.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 79)) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp==3.6.2->-r requirements.txt (line 2)) (3.6.6)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp==3.6.2->-r requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth==1.11.2->-r requirements.txt (line 19)) (45.2.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.1.0->-r requirements.txt (line 69)) (0.34.2)\n",
            "Installing collected packages: gym-continuousDoubleAuction\n",
            "  Found existing installation: gym-continuousDoubleAuction 0.0.1\n",
            "    Can't uninstall 'gym-continuousDoubleAuction'. No files were found to uninstall.\n",
            "  Running setup.py develop for gym-continuousDoubleAuction\n",
            "Successfully installed gym-continuousDoubleAuction\n",
            "Name: tensorflow\n",
            "Version: 2.1.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: six, tensorboard, wrapt, scipy, numpy, opt-einsum, protobuf, absl-py, google-pasta, tensorflow-estimator, grpcio, astor, keras-preprocessing, termcolor, gast, keras-applications, wheel\n",
            "Required-by: tensorflow-federated, stable-baselines, magenta, fancyimpute\n",
            "Name: ray\n",
            "Version: 0.8.2\n",
            "Summary: A system for parallel and distributed Python that unifies the ML ecosystem.\n",
            "Home-page: https://github.com/ray-project/ray\n",
            "Author: Ray Team\n",
            "Author-email: ray-dev@googlegroups.com\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: cloudpickle, numpy, click, pytest, packaging, colorama, py-spy, pyyaml, jsonschema, protobuf, aiohttp, grpcio, filelock, funcsigs, google, six, redis\n",
            "Required-by: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UW3INjDipTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CDA_env_disc_RLlib\n",
        "\n",
        "# https://github.com/ray-project/ray/blob/master/python/ray/rllib/examples/multiagent_cartpole.py\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\"\"\"Simple example of setting up a multi-agent policy mapping.\n",
        "Control the number of agents and policies via --num-agents and --num-policies.\n",
        "This works with hundreds of agents and policies, but note that initializing\n",
        "many TF policies will take some time.\n",
        "Also, TF evals might slow down with large numbers of policies. To debug TF\n",
        "execution, set the TF_TIMELINE_DIR environment variable.\n",
        "\"\"\"\n",
        "\n",
        "# rllib rollout ~/ray_results/PPO/PPO_MMenv-v0_0_2019-08-09_00-06-10t4b1lscr/checkpoint-1 --run PPO --env MMenv-v0 --steps 100\n",
        "\n",
        "import os\n",
        "os.environ['RAY_DEBUG_DISABLE_MEMORY_MONITOR'] = \"True\"\n",
        "\n",
        "import argparse\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.models import Model, ModelCatalog\n",
        "\n",
        "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
        "from ray.rllib.models.tf.fcnet_v2 import FullyConnectedNetwork\n",
        "\n",
        "from ray.tune.registry import register_env\n",
        "from ray.rllib.utils import try_import_tf\n",
        "\n",
        "\n",
        "from ray.rllib.policy.policy import Policy\n",
        "#from ray.rllib.policy.tf_policy import TFPolicy\n",
        "#from ray.rllib.policy import Policy\n",
        "\n",
        "\n",
        "#from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
        "\n",
        "\n",
        "import sys\n",
        "if \"../\" not in sys.path:\n",
        "    sys.path.append(\"../\")\n",
        "#from exchg.x.y import z\n",
        "\n",
        "#from envs.continuousDoubleAuction_env import continuousDoubleAuctionEnv\n",
        "from gym_continuousDoubleAuction.envs.continuousDoubleAuction_env import continuousDoubleAuctionEnv\n",
        "\n",
        "tf = try_import_tf()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFWgG3bhiwux",
        "colab_type": "code",
        "outputId": "169e28b5-2218-4233-ac23-b32a0c94dc12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--num-agents\", type=int, default=4)\n",
        "parser.add_argument(\"--num-policies\", type=int, default=4)\n",
        "parser.add_argument(\"--num-iters\", type=int, default=10)\n",
        "parser.add_argument(\"--simple\", action=\"store_true\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreTrueAction(option_strings=['--simple'], dest='simple', nargs=0, const=True, default=False, type=None, choices=None, help=None, metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIYT1UTxiQcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomModel_1(Model):\n",
        "    def _lstm(self, Inputs, cell_size):\n",
        "        s = tf.expand_dims(Inputs, axis=1, name='time_major')  # [time_step, feature] => [time_step, batch, feature]\n",
        "        lstm_cell = tf.nn.rnn_cell.LSTMCell(cell_size)\n",
        "        self.init_state = lstm_cell.zero_state(batch_size=1, dtype=tf.float32)\n",
        "        # time_major means [time_step, batch, feature] while batch major means [batch, time_step, feature]\n",
        "        outputs, self.final_state = tf.nn.dynamic_rnn(cell=lstm_cell, inputs=s, initial_state=self.init_state, time_major=True)\n",
        "        lstm_out = tf.reshape(outputs, [-1, cell_size], name='flatten_rnn_outputs')  # joined state representation\n",
        "        return lstm_out\n",
        "\n",
        "    def _build_layers_v2(self, input_dict, num_outputs, options):\n",
        "        hidden = 512\n",
        "        cell_size = 256\n",
        "        #S = input_dict[\"obs\"]\n",
        "        S = tf.layers.flatten(input_dict[\"obs\"])\n",
        "        with tf.variable_scope(tf.VariableScope(tf.AUTO_REUSE, \"shared\"),\n",
        "                               reuse=tf.AUTO_REUSE,\n",
        "                               auxiliary_name_scope=False):\n",
        "            last_layer = tf.layers.dense(S, hidden, activation=tf.nn.relu, name=\"fc1\")\n",
        "        last_layer = tf.layers.dense(last_layer, hidden, activation=tf.nn.relu, name=\"fc2\")\n",
        "        last_layer = tf.layers.dense(last_layer, hidden, activation=tf.nn.relu, name=\"fc3\")\n",
        "\n",
        "        last_layer = self._lstm(last_layer, cell_size)\n",
        "\n",
        "        output = tf.layers.dense(last_layer, num_outputs, activation=tf.nn.softmax, name=\"mu\")\n",
        "\n",
        "        return output, last_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56uO7LyPMQ4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomModel(TFModelV2):\n",
        "    \"\"\"Example of a custom model that just delegates to a fc-net.\"\"\"\n",
        "\n",
        "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
        "        \n",
        "        super(CustomModel, self).__init__(obs_space, action_space, num_outputs, model_config, name)\n",
        "        \n",
        "        self.model = FullyConnectedNetwork(obs_space, action_space, num_outputs, model_config, name)\n",
        "        #print('obs_space', obs_space)\n",
        "\n",
        "        self.register_variables(self.model.variables())\n",
        "\n",
        "    def forward(self, input_dict, state, seq_lens):\n",
        "        return self.model.forward(input_dict, state, seq_lens)\n",
        "\n",
        "    def value_function(self):\n",
        "        return self.model.value_function()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cijiMv-ti1SK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_RandomPolicy(_seed):\n",
        "\n",
        "    # a hand-coded policy that acts at random in the env (doesn't learn)\n",
        "    class RandomPolicy(Policy):\n",
        "        \"\"\"Hand-coded policy that returns random actions.\"\"\"\n",
        "        def __init__(self, observation_space, action_space, config):\n",
        "            self.observation_space = observation_space\n",
        "            self.action_space = action_space\n",
        "            self.action_space.seed(_seed)\n",
        "\n",
        "        def compute_actions(self,\n",
        "                            obs_batch,\n",
        "                            state_batches,\n",
        "                            prev_action_batch=None,\n",
        "                            prev_reward_batch=None,\n",
        "                            info_batch=None,\n",
        "                            episodes=None,\n",
        "                            **kwargs):\n",
        "            \"\"\"Compute actions on a batch of observations.\"\"\"\n",
        "            return [self.action_space.sample() for _ in obs_batch], [], {}\n",
        "\n",
        "        def learn_on_batch(self, samples):\n",
        "            \"\"\"No learning.\"\"\"\n",
        "            #return {}\n",
        "            pass\n",
        "\n",
        "        def get_weights(self):\n",
        "            pass\n",
        "\n",
        "        def set_weights(self, weights):\n",
        "            pass\n",
        "\n",
        "    return RandomPolicy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7ASMamrPoh3",
        "colab_type": "code",
        "outputId": "3a87db8d-b61b-4cb9-9c09-91e2e49cecfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "#args = parser.parse_args()\n",
        "\n",
        "# set log_to_driver=False to off render output so as to prevent browser from hanging.\n",
        "#ray.init(ignore_reinit_error=True)\n",
        "#ray.init(ignore_reinit_error=True, num_cpus=2)\n",
        "#ray.init(ignore_reinit_error=True, webui_host='127.0.0.1', num_cpus=2)\n",
        "ray.init(ignore_reinit_error=True, log_to_driver=False, webui_host='127.0.0.1', num_cpus=2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-17 07:28:49,653\tWARNING services.py:586 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
            "2020-03-17 07:28:49,655\tINFO resource_spec.py:212 -- Starting Ray with 6.74 GiB memory available for workers and up to 3.38 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
            "2020-03-17 07:28:50,156\tINFO services.py:1078 -- View the Ray dashboard at \u001b[1m\u001b[32m127.0.0.1:8265\u001b[39m\u001b[22m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'node_ip_address': '172.28.0.2',\n",
              " 'object_store_address': '/tmp/ray/session_2020-03-17_07-28-49_652205_2354/sockets/plasma_store',\n",
              " 'raylet_socket_name': '/tmp/ray/session_2020-03-17_07-28-49_652205_2354/sockets/raylet',\n",
              " 'redis_address': '172.28.0.2:42788',\n",
              " 'session_dir': '/tmp/ray/session_2020-03-17_07-28-49_652205_2354',\n",
              " 'webui_url': '127.0.0.1:8265'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqzjVWUsPykm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_agents = 4\n",
        "num_policies = num_agents\n",
        "num_iters = 3\n",
        "simple = False #store_true\n",
        "#num_of_traders = args.num_agents\n",
        "num_of_traders = num_agents\n",
        "tape_display_length = 10 #100\n",
        "tick_size = 1\n",
        "init_cash = 1000000\n",
        "max_step = 700 # per episode #700\n",
        "episode = 5 #9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To995IZanbGx",
        "colab_type": "code",
        "outputId": "bd61fea4-d18d-4a80-e20b-cab1d4cf241a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "single_CDA_env = continuousDoubleAuctionEnv(num_of_traders, init_cash, tick_size, tape_display_length, max_step)\n",
        "obs_space = single_CDA_env.observation_space\n",
        "act_space = single_CDA_env.action_space\n",
        "register_env(\"continuousDoubleAuction-v0\", lambda _: continuousDoubleAuctionEnv(num_of_traders, init_cash, tick_size, tape_display_length, max_step))\n",
        "ModelCatalog.register_custom_model(\"model_disc\", CustomModel_1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN5IfMMvP4VA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Each policy can have a different configuration (including custom model)\n",
        "def gen_policy(i):\n",
        "    config = {\"model\": {\"custom_model\": \"model_disc\"},\n",
        "              \"gamma\": 0.99,}\n",
        "    return (None, obs_space, act_space, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7KFjAxGP6en",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def policy_mapper(agent_id):\n",
        "    for i in range(num_agents):\n",
        "        if agent_id == i:\n",
        "            return \"policy_{}\".format(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXrTPRQDP8of",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup PPO with an ensemble of `num_policies` different policies\n",
        "\n",
        "# Dictionary of policies\n",
        "#policies = {\"policy_{}\".format(i): gen_policy(i) for i in range(args.num_policies)}\n",
        "policies = {\"policy_{}\".format(i): gen_policy(i) for i in range(num_policies)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsbblOn-P_eA",
        "colab_type": "code",
        "outputId": "921ac934-7f53-412f-e224-ec48f9be2b25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# override policy with random policy\n",
        "\n",
        "def set_RandomPolicy(policies):\n",
        "    \"\"\"\n",
        "    Set 1st policy as PPO & override all other policies as RandomPolicy with\n",
        "    different seed.\n",
        "    \"\"\"\n",
        "\n",
        "    for i in range(num_agents):\n",
        "        if i == num_agents-1:\n",
        "            break\n",
        "        x = i + 1\n",
        "        policies[\"policy_{}\".format(num_policies-x)] = (make_RandomPolicy(num_policies-x), obs_space, act_space, {})\n",
        "\n",
        "    print('policies:', policies)\n",
        "    return 0\n",
        "\n",
        "set_RandomPolicy(policies)\n",
        "\n",
        "policy_ids = list(policies.keys())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "policies: {'policy_0': (None, Box(4, 10), Tuple(Discrete(3), Discrete(4), Box(1,), Box(1,), Discrete(12)), {'model': {'custom_model': 'model_disc'}, 'gamma': 0.99}), 'policy_1': (<class '__main__.make_RandomPolicy.<locals>.RandomPolicy'>, Box(4, 10), Tuple(Discrete(3), Discrete(4), Box(1,), Box(1,), Discrete(12)), {}), 'policy_2': (<class '__main__.make_RandomPolicy.<locals>.RandomPolicy'>, Box(4, 10), Tuple(Discrete(3), Discrete(4), Box(1,), Box(1,), Discrete(12)), {}), 'policy_3': (<class '__main__.make_RandomPolicy.<locals>.RandomPolicy'>, Box(4, 10), Tuple(Discrete(3), Discrete(4), Box(1,), Box(1,), Discrete(12)), {})}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVr7hhEni7u7",
        "colab_type": "code",
        "outputId": "00190614-4c78-4167-f822-ee9d28c57a0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tune.run(#PPOTrainer,\n",
        "          \"PPO\",\n",
        "          #\"PG\",\n",
        "          #queue_trials=False,\n",
        "          #resources_per_trial={\"cpu\": 2,\n",
        "          #                     \"gpu\": 0},\n",
        "\n",
        "          #stop={\"training_iteration\": args.num_iters},\n",
        "          #stop={\"timesteps_total\": max_step,\n",
        "          #      \"training_iteration\": num_iters},\n",
        "          stop={\"timesteps_total\": max_step * episode},\n",
        "\n",
        "          config={\"env\": \"continuousDoubleAuction-v0\",\n",
        "\n",
        "                  #\"log_level\": \"DEBUG\",\n",
        "                  #\"simple_optimizer\": args.simple,\n",
        "                  #\"simple_optimizer\": True,\n",
        "                  #\"num_sgd_iter\": 10,\n",
        "\n",
        "                  #\"gamma\": 0.9,\n",
        "\n",
        "                  # Number of rollout worker actors to create for parallel sampling.\n",
        "                  # Setting to 0 will force rollouts to be done in the trainer actor.\n",
        "                  \"num_workers\": 1, # Colab (only 2 CPUs or 1 GPU) #1\n",
        "                  \"num_envs_per_worker\": 2, #2\n",
        "\n",
        "                  #\"timesteps_per_iteration\": max_step,\n",
        "\n",
        "                  # https://github.com/ray-project/ray/issues/4628\n",
        "                  \"sample_batch_size\": 32, # number of environment steps sampled from each environment\n",
        "                  \"train_batch_size\": 128, # minibatch size must be >= 128, number of environment steps sampled from all available environments\n",
        "\n",
        "                  \"multiagent\": {\"policies_to_train\": [\"policy_0\"],\n",
        "                                 \"policies\": policies,\n",
        "                                 #\"policy_mapping_fn\": tune.function(lambda agent_id: random.choice(policy_ids)),\n",
        "                                 #\"policy_mapping_fn\": tune.function(policy_mapper),\n",
        "                                 \"policy_mapping_fn\": policy_mapper,\n",
        "                                },\n",
        "                  },\n",
        "          )\n",
        "\n",
        "#ray.shutdown()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 1.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc  </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>     </td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-29-29\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 14915.606\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.20000000298023224\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.934357643127441\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.015403380617499352\n",
            "        policy_loss: -0.12289988994598389\n",
            "        total_loss: 268038784.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 268038784.0\n",
            "    load_time_ms: 120.425\n",
            "    num_steps_sampled: 128\n",
            "    num_steps_trained: 128\n",
            "    sample_time_ms: 3176.368\n",
            "    update_time_ms: 1529.422\n",
            "  iterations_since_restore: 1\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 85.78965517241382\n",
            "    ram_util_percent: 17.45862068965517\n",
            "  pid: 2443\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 19.871483087539673\n",
            "  time_this_iter_s: 19.871483087539673\n",
            "  time_total_s: 19.871483087539673\n",
            "  timestamp: 1584430169\n",
            "  timesteps_since_restore: 128\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 128\n",
            "  training_iteration: 1\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.2/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         19.8715</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">     1</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-29-46\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 14465.062\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.20000000298023224\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.939593315124512\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.01406663004308939\n",
            "        policy_loss: -0.07625341415405273\n",
            "        total_loss: 16426016.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 16426018.0\n",
            "    load_time_ms: 60.873\n",
            "    num_steps_sampled: 256\n",
            "    num_steps_trained: 256\n",
            "    sample_time_ms: 3202.707\n",
            "    update_time_ms: 777.485\n",
            "  iterations_since_restore: 2\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.2\n",
            "    ram_util_percent: 17.512\n",
            "  pid: 2443\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 37.145246744155884\n",
            "  time_this_iter_s: 17.27376365661621\n",
            "  time_total_s: 37.145246744155884\n",
            "  timestamp: 1584430186\n",
            "  timesteps_since_restore: 256\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 256\n",
            "  training_iteration: 2\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.2/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         37.1452</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">     2</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-30-03\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 14350.464\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.20000000298023224\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.90464973449707\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.010826986283063889\n",
            "        policy_loss: -0.07699980586767197\n",
            "        total_loss: 79289952.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 79289952.0\n",
            "    load_time_ms: 40.931\n",
            "    num_steps_sampled: 384\n",
            "    num_steps_trained: 384\n",
            "    sample_time_ms: 3226.446\n",
            "    update_time_ms: 527.698\n",
            "  iterations_since_restore: 3\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.056\n",
            "    ram_util_percent: 17.604\n",
            "  pid: 2443\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 54.57444190979004\n",
            "  time_this_iter_s: 17.429195165634155\n",
            "  time_total_s: 54.57444190979004\n",
            "  timestamp: 1584430203\n",
            "  timesteps_since_restore: 384\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 384\n",
            "  training_iteration: 3\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.2/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         54.5744</td><td style=\"text-align: right;\"> 384</td><td style=\"text-align: right;\">     3</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-30-21\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 14238.57\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.20000000298023224\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.925111293792725\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.009825093671679497\n",
            "        policy_loss: -0.10892008990049362\n",
            "        total_loss: 205448512.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 205448512.0\n",
            "    load_time_ms: 30.994\n",
            "    num_steps_sampled: 512\n",
            "    num_steps_trained: 512\n",
            "    sample_time_ms: 3260.718\n",
            "    update_time_ms: 402.557\n",
            "  iterations_since_restore: 4\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.236\n",
            "    ram_util_percent: 17.7\n",
            "  pid: 2443\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 71.87302732467651\n",
            "  time_this_iter_s: 17.298585414886475\n",
            "  time_total_s: 71.87302732467651\n",
            "  timestamp: 1584430221\n",
            "  timesteps_since_restore: 512\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 512\n",
            "  training_iteration: 4\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">          71.873</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">     4</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-30-38\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 14157.631\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.20000000298023224\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.945356369018555\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.011196572333574295\n",
            "        policy_loss: -0.0768723115324974\n",
            "        total_loss: 111084040.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 111084040.0\n",
            "    load_time_ms: 24.994\n",
            "    num_steps_sampled: 640\n",
            "    num_steps_trained: 640\n",
            "    sample_time_ms: 3285.148\n",
            "    update_time_ms: 327.045\n",
            "  iterations_since_restore: 5\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.55416666666667\n",
            "    ram_util_percent: 17.8\n",
            "  pid: 2443\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 89.11925458908081\n",
            "  time_this_iter_s: 17.246227264404297\n",
            "  time_total_s: 89.11925458908081\n",
            "  timestamp: 1584430238\n",
            "  timesteps_since_restore: 640\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 640\n",
            "  training_iteration: 5\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         89.1193</td><td style=\"text-align: right;\"> 640</td><td style=\"text-align: right;\">     5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-30-56\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 14127.764\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.20000000298023224\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.941017150878906\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.012235958129167557\n",
            "        policy_loss: -0.10188319534063339\n",
            "        total_loss: 29369762.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 29369762.0\n",
            "    load_time_ms: 21.033\n",
            "    num_steps_sampled: 768\n",
            "    num_steps_trained: 768\n",
            "    sample_time_ms: 3312.275\n",
            "    update_time_ms: 277.134\n",
            "  iterations_since_restore: 6\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.656\n",
            "    ram_util_percent: 17.9\n",
            "  pid: 2443\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 106.57938933372498\n",
            "  time_this_iter_s: 17.460134744644165\n",
            "  time_total_s: 106.57938933372498\n",
            "  timestamp: 1584430256\n",
            "  timesteps_since_restore: 768\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 768\n",
            "  training_iteration: 6\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         106.579</td><td style=\"text-align: right;\"> 768</td><td style=\"text-align: right;\">     6</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-31-13\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 14095.977\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.20000000298023224\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.026924133300781\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.02479381114244461\n",
            "        policy_loss: -0.10185523331165314\n",
            "        total_loss: 2303275.5\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 2303275.75\n",
            "    load_time_ms: 18.189\n",
            "    num_steps_sampled: 896\n",
            "    num_steps_trained: 896\n",
            "    sample_time_ms: 3330.252\n",
            "    update_time_ms: 241.099\n",
            "  iterations_since_restore: 7\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.892\n",
            "    ram_util_percent: 18.0\n",
            "  pid: 2443\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 123.95282006263733\n",
            "  time_this_iter_s: 17.373430728912354\n",
            "  time_total_s: 123.95282006263733\n",
            "  timestamp: 1584430273\n",
            "  timesteps_since_restore: 896\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 896\n",
            "  training_iteration: 7\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         123.953</td><td style=\"text-align: right;\"> 896</td><td style=\"text-align: right;\">     7</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-31-31\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 14099.587\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.93223762512207\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.01311160996556282\n",
            "        policy_loss: -0.07703135907649994\n",
            "        total_loss: 2777878.25\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 2777878.25\n",
            "    load_time_ms: 16.065\n",
            "    num_steps_sampled: 1024\n",
            "    num_steps_trained: 1024\n",
            "    sample_time_ms: 3388.378\n",
            "    update_time_ms: 214.26\n",
            "  iterations_since_restore: 8\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.15384615384616\n",
            "    ram_util_percent: 18.1\n",
            "  pid: 2443\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 141.90444803237915\n",
            "  time_this_iter_s: 17.95162796974182\n",
            "  time_total_s: 141.90444803237915\n",
            "  timestamp: 1584430291\n",
            "  timesteps_since_restore: 1024\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1024\n",
            "  training_iteration: 8\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         141.904</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">     8</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-31-48\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 14046.166\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.9931511878967285\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.019921209663152695\n",
            "        policy_loss: -0.06764984130859375\n",
            "        total_loss: 2020071.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 2020071.25\n",
            "    load_time_ms: 14.466\n",
            "    num_steps_sampled: 1152\n",
            "    num_steps_trained: 1152\n",
            "    sample_time_ms: 3396.343\n",
            "    update_time_ms: 194.694\n",
            "  iterations_since_restore: 9\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 88.99166666666667\n",
            "    ram_util_percent: 18.2\n",
            "  pid: 2443\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 159.02706146240234\n",
            "  time_this_iter_s: 17.122613430023193\n",
            "  time_total_s: 159.02706146240234\n",
            "  timestamp: 1584430308\n",
            "  timesteps_since_restore: 1152\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1152\n",
            "  training_iteration: 9\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         159.027</td><td style=\"text-align: right;\">1152</td><td style=\"text-align: right;\">     9</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-32-05\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 14028.004\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.936367511749268\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.01377885602414608\n",
            "        policy_loss: -0.0796303004026413\n",
            "        total_loss: 5164616.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 5164616.0\n",
            "    load_time_ms: 13.109\n",
            "    num_steps_sampled: 1280\n",
            "    num_steps_trained: 1280\n",
            "    sample_time_ms: 3399.891\n",
            "    update_time_ms: 177.592\n",
            "  iterations_since_restore: 10\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.752\n",
            "    ram_util_percent: 18.3\n",
            "  pid: 2443\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 176.35192251205444\n",
            "  time_this_iter_s: 17.3248610496521\n",
            "  time_total_s: 176.35192251205444\n",
            "  timestamp: 1584430325\n",
            "  timesteps_since_restore: 1280\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1280\n",
            "  training_iteration: 10\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         176.352</td><td style=\"text-align: right;\">1280</td><td style=\"text-align: right;\">    10</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-32-23\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 2\n",
            "  episodes_total: 2\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 13920.426\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.894207954406738\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.009772809222340584\n",
            "        policy_loss: -0.09190350770950317\n",
            "        total_loss: 2845449.5\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 2845449.5\n",
            "    load_time_ms: 1.167\n",
            "    num_steps_sampled: 1408\n",
            "    num_steps_trained: 1408\n",
            "    sample_time_ms: 3426.458\n",
            "    update_time_ms: 26.896\n",
            "  iterations_since_restore: 11\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.904\n",
            "    ram_util_percent: 18.4\n",
            "  pid: 2443\n",
            "  policy_reward_max:\n",
            "    policy_0: 21489.0\n",
            "    policy_1: 25138.0\n",
            "    policy_2: -6098.0\n",
            "    policy_3: -4490.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 3469.5\n",
            "    policy_1: 21534.5\n",
            "    policy_2: -20217.0\n",
            "    policy_3: -4787.0\n",
            "  policy_reward_min:\n",
            "    policy_0: -14550.0\n",
            "    policy_1: 17931.0\n",
            "    policy_2: -34336.0\n",
            "    policy_3: -5084.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 46.99554849178233\n",
            "    mean_inference_ms: 4.321964750898645\n",
            "    mean_processing_ms: 1.4193856124336839\n",
            "  time_since_restore: 193.6612253189087\n",
            "  time_this_iter_s: 17.309302806854248\n",
            "  time_total_s: 193.6612253189087\n",
            "  timestamp: 1584430343\n",
            "  timesteps_since_restore: 1408\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1408\n",
            "  training_iteration: 11\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         193.661</td><td style=\"text-align: right;\">1408</td><td style=\"text-align: right;\">    11</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-32-40\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 13926.872\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.001075744628906\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.024732954800128937\n",
            "        policy_loss: -0.12114179134368896\n",
            "        total_loss: 37089512.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 37089512.0\n",
            "    load_time_ms: 1.138\n",
            "    num_steps_sampled: 1536\n",
            "    num_steps_trained: 1536\n",
            "    sample_time_ms: 3402.916\n",
            "    update_time_ms: 27.132\n",
            "  iterations_since_restore: 12\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.46249999999999\n",
            "    ram_util_percent: 18.5\n",
            "  pid: 2443\n",
            "  policy_reward_max:\n",
            "    policy_0: 21489.0\n",
            "    policy_1: 25138.0\n",
            "    policy_2: -6098.0\n",
            "    policy_3: -4490.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 3469.5\n",
            "    policy_1: 21534.5\n",
            "    policy_2: -20217.0\n",
            "    policy_3: -4787.0\n",
            "  policy_reward_min:\n",
            "    policy_0: -14550.0\n",
            "    policy_1: 17931.0\n",
            "    policy_2: -34336.0\n",
            "    policy_3: -5084.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 46.99554849178233\n",
            "    mean_inference_ms: 4.321964750898645\n",
            "    mean_processing_ms: 1.4193856124336839\n",
            "  time_since_restore: 210.76667261123657\n",
            "  time_this_iter_s: 17.10544729232788\n",
            "  time_total_s: 210.76667261123657\n",
            "  timestamp: 1584430360\n",
            "  timesteps_since_restore: 1536\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1536\n",
            "  training_iteration: 12\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         210.767</td><td style=\"text-align: right;\">1536</td><td style=\"text-align: right;\">    12</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-32-57\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 13895.968\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.906530380249023\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.011111855506896973\n",
            "        policy_loss: -0.10118065774440765\n",
            "        total_loss: 71335784.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 71335784.0\n",
            "    load_time_ms: 1.133\n",
            "    num_steps_sampled: 1664\n",
            "    num_steps_trained: 1664\n",
            "    sample_time_ms: 3406.426\n",
            "    update_time_ms: 27.381\n",
            "  iterations_since_restore: 13\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.956\n",
            "    ram_util_percent: 18.6\n",
            "  pid: 2443\n",
            "  policy_reward_max:\n",
            "    policy_0: 21489.0\n",
            "    policy_1: 25138.0\n",
            "    policy_2: -6098.0\n",
            "    policy_3: -4490.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 3469.5\n",
            "    policy_1: 21534.5\n",
            "    policy_2: -20217.0\n",
            "    policy_3: -4787.0\n",
            "  policy_reward_min:\n",
            "    policy_0: -14550.0\n",
            "    policy_1: 17931.0\n",
            "    policy_2: -34336.0\n",
            "    policy_3: -5084.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 46.99554849178233\n",
            "    mean_inference_ms: 4.321964750898645\n",
            "    mean_processing_ms: 1.4193856124336839\n",
            "  time_since_restore: 227.92412543296814\n",
            "  time_this_iter_s: 17.157452821731567\n",
            "  time_total_s: 227.92412543296814\n",
            "  timestamp: 1584430377\n",
            "  timesteps_since_restore: 1664\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1664\n",
            "  training_iteration: 13\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         227.924</td><td style=\"text-align: right;\">1664</td><td style=\"text-align: right;\">    13</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-33-14\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 13888.9\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.898251056671143\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.012430120259523392\n",
            "        policy_loss: -0.08832475543022156\n",
            "        total_loss: 61256644.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 61256644.0\n",
            "    load_time_ms: 1.129\n",
            "    num_steps_sampled: 1792\n",
            "    num_steps_trained: 1792\n",
            "    sample_time_ms: 3396.905\n",
            "    update_time_ms: 27.44\n",
            "  iterations_since_restore: 14\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.08333333333333\n",
            "    ram_util_percent: 18.691666666666666\n",
            "  pid: 2443\n",
            "  policy_reward_max:\n",
            "    policy_0: 21489.0\n",
            "    policy_1: 25138.0\n",
            "    policy_2: -6098.0\n",
            "    policy_3: -4490.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 3469.5\n",
            "    policy_1: 21534.5\n",
            "    policy_2: -20217.0\n",
            "    policy_3: -4787.0\n",
            "  policy_reward_min:\n",
            "    policy_0: -14550.0\n",
            "    policy_1: 17931.0\n",
            "    policy_2: -34336.0\n",
            "    policy_3: -5084.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 46.99554849178233\n",
            "    mean_inference_ms: 4.321964750898645\n",
            "    mean_processing_ms: 1.4193856124336839\n",
            "  time_since_restore: 245.05704545974731\n",
            "  time_this_iter_s: 17.132920026779175\n",
            "  time_total_s: 245.05704545974731\n",
            "  timestamp: 1584430394\n",
            "  timesteps_since_restore: 1792\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1792\n",
            "  training_iteration: 14\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         245.057</td><td style=\"text-align: right;\">1792</td><td style=\"text-align: right;\">    14</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-33-31\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 13882.84\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.938316345214844\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.014324215240776539\n",
            "        policy_loss: -0.08613629639148712\n",
            "        total_loss: 3748944.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 3748944.0\n",
            "    load_time_ms: 1.129\n",
            "    num_steps_sampled: 1920\n",
            "    num_steps_trained: 1920\n",
            "    sample_time_ms: 3385.484\n",
            "    update_time_ms: 27.939\n",
            "  iterations_since_restore: 15\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.168\n",
            "    ram_util_percent: 18.788\n",
            "  pid: 2443\n",
            "  policy_reward_max:\n",
            "    policy_0: 21489.0\n",
            "    policy_1: 25138.0\n",
            "    policy_2: -6098.0\n",
            "    policy_3: -4490.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 3469.5\n",
            "    policy_1: 21534.5\n",
            "    policy_2: -20217.0\n",
            "    policy_3: -4787.0\n",
            "  policy_reward_min:\n",
            "    policy_0: -14550.0\n",
            "    policy_1: 17931.0\n",
            "    policy_2: -34336.0\n",
            "    policy_3: -5084.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 46.99554849178233\n",
            "    mean_inference_ms: 4.321964750898645\n",
            "    mean_processing_ms: 1.4193856124336839\n",
            "  time_since_restore: 262.1336281299591\n",
            "  time_this_iter_s: 17.076582670211792\n",
            "  time_total_s: 262.1336281299591\n",
            "  timestamp: 1584430411\n",
            "  timesteps_since_restore: 1920\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1920\n",
            "  training_iteration: 15\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         262.134</td><td style=\"text-align: right;\">1920</td><td style=\"text-align: right;\">    15</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-33-49\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 13865.185\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.995109558105469\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.015849340707063675\n",
            "        policy_loss: -0.10807034373283386\n",
            "        total_loss: 519598.03125\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 519598.125\n",
            "    load_time_ms: 1.125\n",
            "    num_steps_sampled: 2048\n",
            "    num_steps_trained: 2048\n",
            "    sample_time_ms: 3384.261\n",
            "    update_time_ms: 28.239\n",
            "  iterations_since_restore: 16\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.34583333333335\n",
            "    ram_util_percent: 18.883333333333333\n",
            "  pid: 2443\n",
            "  policy_reward_max:\n",
            "    policy_0: 21489.0\n",
            "    policy_1: 25138.0\n",
            "    policy_2: -6098.0\n",
            "    policy_3: -4490.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 3469.5\n",
            "    policy_1: 21534.5\n",
            "    policy_2: -20217.0\n",
            "    policy_3: -4787.0\n",
            "  policy_reward_min:\n",
            "    policy_0: -14550.0\n",
            "    policy_1: 17931.0\n",
            "    policy_2: -34336.0\n",
            "    policy_3: -5084.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 46.99554849178233\n",
            "    mean_inference_ms: 4.321964750898645\n",
            "    mean_processing_ms: 1.4193856124336839\n",
            "  time_since_restore: 279.4067735671997\n",
            "  time_this_iter_s: 17.2731454372406\n",
            "  time_total_s: 279.4067735671997\n",
            "  timestamp: 1584430429\n",
            "  timesteps_since_restore: 2048\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2048\n",
            "  training_iteration: 16\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         279.407</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">    16</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-34-06\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 13862.524\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.978761672973633\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.007903424091637135\n",
            "        policy_loss: -0.08460040390491486\n",
            "        total_loss: 5142799.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 5142799.5\n",
            "    load_time_ms: 1.132\n",
            "    num_steps_sampled: 2176\n",
            "    num_steps_trained: 2176\n",
            "    sample_time_ms: 3371.931\n",
            "    update_time_ms: 28.448\n",
            "  iterations_since_restore: 17\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.132\n",
            "    ram_util_percent: 18.96\n",
            "  pid: 2443\n",
            "  policy_reward_max:\n",
            "    policy_0: 21489.0\n",
            "    policy_1: 25138.0\n",
            "    policy_2: -6098.0\n",
            "    policy_3: -4490.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 3469.5\n",
            "    policy_1: 21534.5\n",
            "    policy_2: -20217.0\n",
            "    policy_3: -4787.0\n",
            "  policy_reward_min:\n",
            "    policy_0: -14550.0\n",
            "    policy_1: 17931.0\n",
            "    policy_2: -34336.0\n",
            "    policy_3: -5084.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 46.99554849178233\n",
            "    mean_inference_ms: 4.321964750898645\n",
            "    mean_processing_ms: 1.4193856124336839\n",
            "  time_since_restore: 296.6325590610504\n",
            "  time_this_iter_s: 17.225785493850708\n",
            "  time_total_s: 296.6325590610504\n",
            "  timestamp: 1584430446\n",
            "  timesteps_since_restore: 2176\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2176\n",
            "  training_iteration: 17\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         296.633</td><td style=\"text-align: right;\">2176</td><td style=\"text-align: right;\">    17</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-34-23\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 13833.073\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.974634170532227\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.011417113244533539\n",
            "        policy_loss: -0.05793872848153114\n",
            "        total_loss: 9766984.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 9766984.0\n",
            "    load_time_ms: 1.104\n",
            "    num_steps_sampled: 2304\n",
            "    num_steps_trained: 2304\n",
            "    sample_time_ms: 3327.931\n",
            "    update_time_ms: 28.563\n",
            "  iterations_since_restore: 18\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.97999999999998\n",
            "    ram_util_percent: 19.0\n",
            "  pid: 2443\n",
            "  policy_reward_max:\n",
            "    policy_0: 21489.0\n",
            "    policy_1: 25138.0\n",
            "    policy_2: -6098.0\n",
            "    policy_3: -4490.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 3469.5\n",
            "    policy_1: 21534.5\n",
            "    policy_2: -20217.0\n",
            "    policy_3: -4787.0\n",
            "  policy_reward_min:\n",
            "    policy_0: -14550.0\n",
            "    policy_1: 17931.0\n",
            "    policy_2: -34336.0\n",
            "    policy_3: -5084.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 46.99554849178233\n",
            "    mean_inference_ms: 4.321964750898645\n",
            "    mean_processing_ms: 1.4193856124336839\n",
            "  time_since_restore: 313.8504099845886\n",
            "  time_this_iter_s: 17.217850923538208\n",
            "  time_total_s: 313.8504099845886\n",
            "  timestamp: 1584430463\n",
            "  timesteps_since_restore: 2304\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2304\n",
            "  training_iteration: 18\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">          313.85</td><td style=\"text-align: right;\">2304</td><td style=\"text-align: right;\">    18</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-34-40\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 13812.495\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.997467041015625\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.012246869504451752\n",
            "        policy_loss: -0.07788492739200592\n",
            "        total_loss: 4786839.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 4786839.0\n",
            "    load_time_ms: 1.08\n",
            "    num_steps_sampled: 2432\n",
            "    num_steps_trained: 2432\n",
            "    sample_time_ms: 3318.887\n",
            "    update_time_ms: 27.527\n",
            "  iterations_since_restore: 19\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.52916666666665\n",
            "    ram_util_percent: 19.1\n",
            "  pid: 2443\n",
            "  policy_reward_max:\n",
            "    policy_0: 21489.0\n",
            "    policy_1: 25138.0\n",
            "    policy_2: -6098.0\n",
            "    policy_3: -4490.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 3469.5\n",
            "    policy_1: 21534.5\n",
            "    policy_2: -20217.0\n",
            "    policy_3: -4787.0\n",
            "  policy_reward_min:\n",
            "    policy_0: -14550.0\n",
            "    policy_1: 17931.0\n",
            "    policy_2: -34336.0\n",
            "    policy_3: -5084.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 46.99554849178233\n",
            "    mean_inference_ms: 4.321964750898645\n",
            "    mean_processing_ms: 1.4193856124336839\n",
            "  time_since_restore: 330.6675124168396\n",
            "  time_this_iter_s: 16.817102432250977\n",
            "  time_total_s: 330.6675124168396\n",
            "  timestamp: 1584430480\n",
            "  timesteps_since_restore: 2432\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2432\n",
            "  training_iteration: 19\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         330.668</td><td style=\"text-align: right;\">2432</td><td style=\"text-align: right;\">    19</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-34-57\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 13742.83\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.9894328117370605\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.009430812671780586\n",
            "        policy_loss: -0.06332877278327942\n",
            "        total_loss: 4654604.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 4654604.0\n",
            "    load_time_ms: 1.094\n",
            "    num_steps_sampled: 2560\n",
            "    num_steps_trained: 2560\n",
            "    sample_time_ms: 3327.315\n",
            "    update_time_ms: 27.827\n",
            "  iterations_since_restore: 20\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 88.96249999999999\n",
            "    ram_util_percent: 19.2\n",
            "  pid: 2443\n",
            "  policy_reward_max:\n",
            "    policy_0: 21489.0\n",
            "    policy_1: 25138.0\n",
            "    policy_2: -6098.0\n",
            "    policy_3: -4490.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 3469.5\n",
            "    policy_1: 21534.5\n",
            "    policy_2: -20217.0\n",
            "    policy_3: -4787.0\n",
            "  policy_reward_min:\n",
            "    policy_0: -14550.0\n",
            "    policy_1: 17931.0\n",
            "    policy_2: -34336.0\n",
            "    policy_3: -5084.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 46.99554849178233\n",
            "    mean_inference_ms: 4.321964750898645\n",
            "    mean_processing_ms: 1.4193856124336839\n",
            "  time_since_restore: 347.3838474750519\n",
            "  time_this_iter_s: 16.71633505821228\n",
            "  time_total_s: 347.3838474750519\n",
            "  timestamp: 1584430497\n",
            "  timesteps_since_restore: 2560\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2560\n",
            "  training_iteration: 20\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         347.384</td><td style=\"text-align: right;\">2560</td><td style=\"text-align: right;\">    20</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-35-14\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 13680.99\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.906023979187012\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.009734943509101868\n",
            "        policy_loss: -0.048010941594839096\n",
            "        total_loss: 2859692.5\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 2859692.75\n",
            "    load_time_ms: 1.087\n",
            "    num_steps_sampled: 2688\n",
            "    num_steps_trained: 2688\n",
            "    sample_time_ms: 3325.123\n",
            "    update_time_ms: 28.269\n",
            "  iterations_since_restore: 21\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 88.97391304347825\n",
            "    ram_util_percent: 19.300000000000004\n",
            "  pid: 2443\n",
            "  policy_reward_max:\n",
            "    policy_0: 21489.0\n",
            "    policy_1: 25138.0\n",
            "    policy_2: -6098.0\n",
            "    policy_3: -4490.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 3469.5\n",
            "    policy_1: 21534.5\n",
            "    policy_2: -20217.0\n",
            "    policy_3: -4787.0\n",
            "  policy_reward_min:\n",
            "    policy_0: -14550.0\n",
            "    policy_1: 17931.0\n",
            "    policy_2: -34336.0\n",
            "    policy_3: -5084.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 46.99554849178233\n",
            "    mean_inference_ms: 4.321964750898645\n",
            "    mean_processing_ms: 1.4193856124336839\n",
            "  time_since_restore: 364.0572760105133\n",
            "  time_this_iter_s: 16.673428535461426\n",
            "  time_total_s: 364.0572760105133\n",
            "  timestamp: 1584430514\n",
            "  timesteps_since_restore: 2688\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2688\n",
            "  training_iteration: 21\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         364.057</td><td style=\"text-align: right;\">2688</td><td style=\"text-align: right;\">    21</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-35-30\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 2\n",
            "  episodes_total: 4\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 13589.904\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.94659423828125\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.013940216973423958\n",
            "        policy_loss: -0.0942511260509491\n",
            "        total_loss: 5585262.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 5585262.5\n",
            "    load_time_ms: 1.101\n",
            "    num_steps_sampled: 2816\n",
            "    num_steps_trained: 2816\n",
            "    sample_time_ms: 3350.368\n",
            "    update_time_ms: 28.164\n",
            "  iterations_since_restore: 22\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.10833333333335\n",
            "    ram_util_percent: 19.4\n",
            "  pid: 2443\n",
            "  policy_reward_max:\n",
            "    policy_0: 21489.0\n",
            "    policy_1: 25138.0\n",
            "    policy_2: 1772.0\n",
            "    policy_3: 31202.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 2336.25\n",
            "    policy_1: 7311.5\n",
            "    policy_2: -10402.25\n",
            "    policy_3: 754.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -14550.0\n",
            "    policy_1: -21599.0\n",
            "    policy_2: -34336.0\n",
            "    policy_3: -18610.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 46.834392728915425\n",
            "    mean_inference_ms: 4.20081187055646\n",
            "    mean_processing_ms: 1.4079571163889724\n",
            "  time_since_restore: 380.5032000541687\n",
            "  time_this_iter_s: 16.445924043655396\n",
            "  time_total_s: 380.5032000541687\n",
            "  timestamp: 1584430530\n",
            "  timesteps_since_restore: 2816\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2816\n",
            "  training_iteration: 22\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         380.503</td><td style=\"text-align: right;\">2816</td><td style=\"text-align: right;\">    22</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-35-46\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 13545.259\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.025140762329102\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.02535039559006691\n",
            "        policy_loss: -0.13220955431461334\n",
            "        total_loss: 291463776.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 291463776.0\n",
            "    load_time_ms: 1.105\n",
            "    num_steps_sampled: 2944\n",
            "    num_steps_trained: 2944\n",
            "    sample_time_ms: 3309.584\n",
            "    update_time_ms: 27.988\n",
            "  iterations_since_restore: 23\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.83043478260869\n",
            "    ram_util_percent: 19.5\n",
            "  pid: 2443\n",
            "  policy_reward_max:\n",
            "    policy_0: 21489.0\n",
            "    policy_1: 25138.0\n",
            "    policy_2: 1772.0\n",
            "    policy_3: 31202.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 2336.25\n",
            "    policy_1: 7311.5\n",
            "    policy_2: -10402.25\n",
            "    policy_3: 754.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -14550.0\n",
            "    policy_1: -21599.0\n",
            "    policy_2: -34336.0\n",
            "    policy_3: -18610.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 46.83439272891542\n",
            "    mean_inference_ms: 4.2008118705564605\n",
            "    mean_processing_ms: 1.4079571163889724\n",
            "  time_since_restore: 396.8049771785736\n",
            "  time_this_iter_s: 16.301777124404907\n",
            "  time_total_s: 396.8049771785736\n",
            "  timestamp: 1584430546\n",
            "  timesteps_since_restore: 2944\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2944\n",
            "  training_iteration: 23\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         396.805</td><td style=\"text-align: right;\">2944</td><td style=\"text-align: right;\">    23</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-36-03\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 13498.825\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.981637954711914\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.01573270373046398\n",
            "        policy_loss: -0.11757886409759521\n",
            "        total_loss: 314417664.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 314417664.0\n",
            "    load_time_ms: 1.098\n",
            "    num_steps_sampled: 3072\n",
            "    num_steps_trained: 3072\n",
            "    sample_time_ms: 3279.905\n",
            "    update_time_ms: 27.984\n",
            "  iterations_since_restore: 24\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.39166666666667\n",
            "    ram_util_percent: 19.6\n",
            "  pid: 2443\n",
            "  policy_reward_max:\n",
            "    policy_0: 21489.0\n",
            "    policy_1: 25138.0\n",
            "    policy_2: 1772.0\n",
            "    policy_3: 31202.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 2336.25\n",
            "    policy_1: 7311.5\n",
            "    policy_2: -10402.25\n",
            "    policy_3: 754.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -14550.0\n",
            "    policy_1: -21599.0\n",
            "    policy_2: -34336.0\n",
            "    policy_3: -18610.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 46.83439272891542\n",
            "    mean_inference_ms: 4.2008118705564605\n",
            "    mean_processing_ms: 1.4079571163889724\n",
            "  time_since_restore: 413.1777865886688\n",
            "  time_this_iter_s: 16.372809410095215\n",
            "  time_total_s: 413.1777865886688\n",
            "  timestamp: 1584430563\n",
            "  timesteps_since_restore: 3072\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3072\n",
            "  training_iteration: 24\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         413.178</td><td style=\"text-align: right;\">3072</td><td style=\"text-align: right;\">    24</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-36-19\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 13459.311\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.984760284423828\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.017769677564501762\n",
            "        policy_loss: -0.108391173183918\n",
            "        total_loss: 237512688.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 237512688.0\n",
            "    load_time_ms: 1.115\n",
            "    num_steps_sampled: 3200\n",
            "    num_steps_trained: 3200\n",
            "    sample_time_ms: 3254.731\n",
            "    update_time_ms: 27.642\n",
            "  iterations_since_restore: 25\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.01739130434783\n",
            "    ram_util_percent: 19.699999999999996\n",
            "  pid: 2443\n",
            "  policy_reward_max:\n",
            "    policy_0: 21489.0\n",
            "    policy_1: 25138.0\n",
            "    policy_2: 1772.0\n",
            "    policy_3: 31202.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 2336.25\n",
            "    policy_1: 7311.5\n",
            "    policy_2: -10402.25\n",
            "    policy_3: 754.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -14550.0\n",
            "    policy_1: -21599.0\n",
            "    policy_2: -34336.0\n",
            "    policy_3: -18610.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 46.83439272891542\n",
            "    mean_inference_ms: 4.2008118705564605\n",
            "    mean_processing_ms: 1.4079571163889724\n",
            "  time_since_restore: 429.6045935153961\n",
            "  time_this_iter_s: 16.426806926727295\n",
            "  time_total_s: 429.6045935153961\n",
            "  timestamp: 1584430579\n",
            "  timesteps_since_restore: 3200\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3200\n",
            "  training_iteration: 25\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         429.605</td><td style=\"text-align: right;\">3200</td><td style=\"text-align: right;\">    25</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-36-36\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 13448.0\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.020910263061523\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.015687795355916023\n",
            "        policy_loss: -0.09711763262748718\n",
            "        total_loss: 66408200.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 66408200.0\n",
            "    load_time_ms: 1.175\n",
            "    num_steps_sampled: 3328\n",
            "    num_steps_trained: 3328\n",
            "    sample_time_ms: 3239.479\n",
            "    update_time_ms: 27.346\n",
            "  iterations_since_restore: 26\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.79166666666667\n",
            "    ram_util_percent: 19.8\n",
            "  pid: 2443\n",
            "  policy_reward_max:\n",
            "    policy_0: 21489.0\n",
            "    policy_1: 25138.0\n",
            "    policy_2: 1772.0\n",
            "    policy_3: 31202.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 2336.25\n",
            "    policy_1: 7311.5\n",
            "    policy_2: -10402.25\n",
            "    policy_3: 754.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -14550.0\n",
            "    policy_1: -21599.0\n",
            "    policy_2: -34336.0\n",
            "    policy_3: -18610.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 46.83439272891542\n",
            "    mean_inference_ms: 4.2008118705564605\n",
            "    mean_processing_ms: 1.4079571163889724\n",
            "  time_since_restore: 446.6098690032959\n",
            "  time_this_iter_s: 17.00527548789978\n",
            "  time_total_s: 446.6098690032959\n",
            "  timestamp: 1584430596\n",
            "  timesteps_since_restore: 3328\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3328\n",
            "  training_iteration: 26\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">          446.61</td><td style=\"text-align: right;\">3328</td><td style=\"text-align: right;\">    26</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-36-53\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 13408.466\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.018653869628906\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.013097397983074188\n",
            "        policy_loss: -0.06669045984745026\n",
            "        total_loss: 23236422.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 23236422.0\n",
            "    load_time_ms: 1.145\n",
            "    num_steps_sampled: 3456\n",
            "    num_steps_trained: 3456\n",
            "    sample_time_ms: 3236.195\n",
            "    update_time_ms: 28.254\n",
            "  iterations_since_restore: 27\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.7\n",
            "    ram_util_percent: 19.895833333333332\n",
            "  pid: 2443\n",
            "  policy_reward_max:\n",
            "    policy_0: 21489.0\n",
            "    policy_1: 25138.0\n",
            "    policy_2: 1772.0\n",
            "    policy_3: 31202.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 2336.25\n",
            "    policy_1: 7311.5\n",
            "    policy_2: -10402.25\n",
            "    policy_3: 754.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -14550.0\n",
            "    policy_1: -21599.0\n",
            "    policy_2: -34336.0\n",
            "    policy_3: -18610.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 46.83439272891542\n",
            "    mean_inference_ms: 4.2008118705564605\n",
            "    mean_processing_ms: 1.4079571163889724\n",
            "  time_since_restore: 463.4165999889374\n",
            "  time_this_iter_s: 16.80673098564148\n",
            "  time_total_s: 463.4165999889374\n",
            "  timestamp: 1584430613\n",
            "  timesteps_since_restore: 3456\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3456\n",
            "  training_iteration: 27\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>RUNNING </td><td>172.28.0.2:2443</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         463.417</td><td style=\"text-align: right;\">3456</td><td style=\"text-align: right;\">    27</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_f2ebbe2a:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_07-37-10\n",
            "  done: true\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 5863ef4ca418454cb449ce9f78f4c8ba\n",
            "  experiment_tag: '0'\n",
            "  hostname: 9e785dd5f6c5\n",
            "  info:\n",
            "    grad_time_ms: 13342.175\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.05618667602539\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.013894252479076385\n",
            "        policy_loss: -0.08181355893611908\n",
            "        total_loss: 9196808.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 9196808.0\n",
            "    load_time_ms: 1.167\n",
            "    num_steps_sampled: 3584\n",
            "    num_steps_trained: 3584\n",
            "    sample_time_ms: 3229.931\n",
            "    update_time_ms: 28.198\n",
            "  iterations_since_restore: 28\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.63749999999999\n",
            "    ram_util_percent: 19.995833333333334\n",
            "  pid: 2443\n",
            "  policy_reward_max:\n",
            "    policy_0: 21489.0\n",
            "    policy_1: 25138.0\n",
            "    policy_2: 1772.0\n",
            "    policy_3: 31202.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 2336.25\n",
            "    policy_1: 7311.5\n",
            "    policy_2: -10402.25\n",
            "    policy_3: 754.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -14550.0\n",
            "    policy_1: -21599.0\n",
            "    policy_2: -34336.0\n",
            "    policy_3: -18610.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 46.83439272891542\n",
            "    mean_inference_ms: 4.2008118705564605\n",
            "    mean_processing_ms: 1.4079571163889724\n",
            "  time_since_restore: 479.90894198417664\n",
            "  time_this_iter_s: 16.492341995239258\n",
            "  time_total_s: 479.90894198417664\n",
            "  timestamp: 1584430630\n",
            "  timesteps_since_restore: 3584\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3584\n",
            "  training_iteration: 28\n",
            "  trial_id: f2ebbe2a\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         479.909</td><td style=\"text-align: right;\">3584</td><td style=\"text-align: right;\">    28</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_f2ebbe2a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         479.909</td><td style=\"text-align: right;\">3584</td><td style=\"text-align: right;\">    28</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2020-03-17 07:37:10,440\tINFO tune.py:352 -- Returning an analysis object by default. You can call `analysis.trials` to retrieve a list of trials. This message will be removed in future versions of Tune.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f2638e479e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}