{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CDA_env_RLlib.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAqVG2cqjLXM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1a919cc-f140-492f-ca45-38be9e0a4436"
      },
      "source": [
        "# Needed to switch directory in Google drive so as to import MARL env.\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd \"/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction/\"\n",
        "#%cd \"/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction/gym_continuousDoubleAuction/\"\n",
        "\n",
        "!pwd\n",
        "\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction\n",
            "/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction\n",
            "Requirement already satisfied: absl-py==0.9.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.9.0)\n",
            "Requirement already satisfied: aiohttp==3.6.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (3.6.2)\n",
            "Requirement already satisfied: astor==0.8.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.8.1)\n",
            "Requirement already satisfied: async-timeout==3.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (3.0.1)\n",
            "Requirement already satisfied: atari-py==0.2.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.2.6)\n",
            "Requirement already satisfied: attrs==19.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (19.3.0)\n",
            "Requirement already satisfied: beautifulsoup4==4.8.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (4.8.2)\n",
            "Requirement already satisfied: cachetools==4.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (4.0.0)\n",
            "Requirement already satisfied: certifi==2019.11.28 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (2019.11.28)\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (3.0.4)\n",
            "Requirement already satisfied: Click==7.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (7.0)\n",
            "Requirement already satisfied: cloudpickle==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: colorama==0.4.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 13)) (0.4.3)\n",
            "Requirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 14)) (3.0.12)\n",
            "Requirement already satisfied: funcsigs==1.0.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 15)) (1.0.2)\n",
            "Requirement already satisfied: future==0.18.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 16)) (0.18.2)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 17)) (0.2.2)\n",
            "Requirement already satisfied: google==2.0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 18)) (2.0.3)\n",
            "Requirement already satisfied: google-auth==1.11.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 19)) (1.11.2)\n",
            "Requirement already satisfied: google-auth-oauthlib==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 20)) (0.4.1)\n",
            "Requirement already satisfied: google-pasta==0.1.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 21)) (0.1.8)\n",
            "Requirement already satisfied: grpcio==1.27.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 22)) (1.27.2)\n",
            "Requirement already satisfied: gym==0.17.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 23)) (0.17.0)\n",
            "Obtaining gym_continuousDoubleAuction from git+https://github.com/ChuaCheowHuan/gym-continuousDoubleAuction.git@c897137cbcc93ca71cbd51c27e683c3298f6562d#egg=gym_continuousDoubleAuction (from -r requirements.txt (line 24))\n",
            "  Cloning https://github.com/ChuaCheowHuan/gym-continuousDoubleAuction.git (to revision c897137cbcc93ca71cbd51c27e683c3298f6562d) to ./src/gym-continuousdoubleauction\n",
            "  Running command git clone -q https://github.com/ChuaCheowHuan/gym-continuousDoubleAuction.git '/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction/src/gym-continuousdoubleauction'\n",
            "  Running command git checkout -q c897137cbcc93ca71cbd51c27e683c3298f6562d\n",
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 25)) (2.10.0)\n",
            "Requirement already satisfied: idna==2.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 26)) (2.9)\n",
            "Requirement already satisfied: importlib-metadata==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 27)) (1.5.0)\n",
            "Requirement already satisfied: joblib==0.14.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 28)) (0.14.1)\n",
            "Requirement already satisfied: jsonschema==3.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 29)) (3.2.0)\n",
            "Requirement already satisfied: Keras-Applications==1.0.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 30)) (1.0.8)\n",
            "Requirement already satisfied: Keras-Preprocessing==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 31)) (1.1.0)\n",
            "Requirement already satisfied: lz4==3.0.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 32)) (3.0.2)\n",
            "Requirement already satisfied: Markdown==3.2.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 33)) (3.2.1)\n",
            "Requirement already satisfied: more-itertools==8.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 34)) (8.2.0)\n",
            "Requirement already satisfied: multidict==4.7.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 35)) (4.7.5)\n",
            "Requirement already satisfied: numpy==1.18.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 36)) (1.18.1)\n",
            "Requirement already satisfied: oauthlib==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 37)) (3.1.0)\n",
            "Requirement already satisfied: opencv-python==4.2.0.32 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 38)) (4.2.0.32)\n",
            "Requirement already satisfied: opencv-python-headless==4.2.0.32 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 39)) (4.2.0.32)\n",
            "Requirement already satisfied: opt-einsum==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 40)) (3.1.0)\n",
            "Requirement already satisfied: packaging==20.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 41)) (20.1)\n",
            "Requirement already satisfied: pandas==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 42)) (1.0.1)\n",
            "Requirement already satisfied: Pillow==7.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 43)) (7.0.0)\n",
            "Requirement already satisfied: pluggy==0.13.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 44)) (0.13.1)\n",
            "Requirement already satisfied: protobuf==3.11.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 45)) (3.11.3)\n",
            "Requirement already satisfied: py==1.8.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 46)) (1.8.1)\n",
            "Requirement already satisfied: py-spy==0.3.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 47)) (0.3.3)\n",
            "Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 48)) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 49)) (0.2.8)\n",
            "Requirement already satisfied: pyglet==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 50)) (1.5.0)\n",
            "Requirement already satisfied: pyparsing==2.4.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 51)) (2.4.6)\n",
            "Requirement already satisfied: pyrsistent==0.15.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 52)) (0.15.7)\n",
            "Requirement already satisfied: pytest==5.3.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 53)) (5.3.5)\n",
            "Requirement already satisfied: python-dateutil==2.8.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 54)) (2.8.1)\n",
            "Requirement already satisfied: pytz==2019.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 55)) (2019.3)\n",
            "Requirement already satisfied: PyYAML==5.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 56)) (5.3)\n",
            "Requirement already satisfied: ray==0.8.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 57)) (0.8.2)\n",
            "Requirement already satisfied: redis==3.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 58)) (3.4.1)\n",
            "Requirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 59)) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 60)) (1.3.0)\n",
            "Requirement already satisfied: rsa==4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 61)) (4.0)\n",
            "Requirement already satisfied: scikit-learn==0.22.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 62)) (0.22.2)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 63)) (1.4.1)\n",
            "Requirement already satisfied: six==1.14.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 64)) (1.14.0)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 65)) (0.0)\n",
            "Requirement already satisfied: sortedcontainers==2.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 66)) (2.1.0)\n",
            "Requirement already satisfied: soupsieve==2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 67)) (2.0)\n",
            "Requirement already satisfied: tabulate==0.8.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 68)) (0.8.6)\n",
            "Collecting tensorboard==2.1.0\n",
            "  Using cached https://files.pythonhosted.org/packages/40/23/53ffe290341cd0855d595b0a2e7485932f473798af173bbe3a584b99bb06/tensorboard-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: tensorboardX==2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 70)) (2.0)\n",
            "Requirement already satisfied: tensorflow==2.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 71)) (2.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator==2.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 72)) (2.1.0)\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 73)) (1.1.0)\n",
            "Requirement already satisfied: urllib3==1.25.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 74)) (1.25.8)\n",
            "Requirement already satisfied: wcwidth==0.1.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 75)) (0.1.8)\n",
            "Requirement already satisfied: Werkzeug==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 76)) (1.0.0)\n",
            "Requirement already satisfied: wrapt==1.12.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 77)) (1.12.0)\n",
            "Requirement already satisfied: yarl==1.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 78)) (1.4.2)\n",
            "Requirement already satisfied: zipp==3.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 79)) (3.0.0)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp==3.6.2->-r requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp==3.6.2->-r requirements.txt (line 2)) (3.6.6)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth==1.11.2->-r requirements.txt (line 19)) (45.2.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.1.0->-r requirements.txt (line 69)) (0.34.2)\n",
            "\u001b[31mERROR: tensorflow-federated 0.12.0 has requirement cachetools~=3.1.1, but you'll have cachetools 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-federated 0.12.0 has requirement grpcio~=1.24.3, but you'll have grpcio 1.27.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-federated 0.12.0 has requirement tensorflow-addons~=0.7.0, but you'll have tensorflow-addons 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gym-continuousDoubleAuction, tensorboard\n",
            "  Found existing installation: gym-continuousDoubleAuction 0.0.1\n",
            "    Can't uninstall 'gym-continuousDoubleAuction'. No files were found to uninstall.\n",
            "  Running setup.py develop for gym-continuousDoubleAuction\n",
            "  Found existing installation: tensorboard 2.1.1\n",
            "    Uninstalling tensorboard-2.1.1:\n",
            "      Successfully uninstalled tensorboard-2.1.1\n",
            "Successfully installed gym-continuousDoubleAuction tensorboard-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_GHZ31rWbYh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "5fe953e9-5bae-4ab6-fcee-5578b81e7e8f"
      },
      "source": [
        "!pip show tensorflow\n",
        "!pip show ray"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.1.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: wrapt, absl-py, keras-preprocessing, keras-applications, grpcio, tensorflow-estimator, termcolor, scipy, astor, google-pasta, wheel, opt-einsum, gast, protobuf, six, numpy, tensorboard\n",
            "Required-by: tensorflow-federated, stable-baselines, magenta, fancyimpute\n",
            "Name: ray\n",
            "Version: 0.8.2\n",
            "Summary: A system for parallel and distributed Python that unifies the ML ecosystem.\n",
            "Home-page: https://github.com/ray-project/ray\n",
            "Author: Ray Team\n",
            "Author-email: ray-dev@googlegroups.com\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: funcsigs, py-spy, click, cloudpickle, aiohttp, packaging, numpy, pyyaml, protobuf, redis, google, grpcio, filelock, pytest, six, jsonschema, colorama\n",
            "Required-by: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UW3INjDipTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CDA_env_disc_RLlib\n",
        "\n",
        "# https://github.com/ray-project/ray/blob/master/python/ray/rllib/examples/multiagent_cartpole.py\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\"\"\"Simple example of setting up a multi-agent policy mapping.\n",
        "Control the number of agents and policies via --num-agents and --num-policies.\n",
        "This works with hundreds of agents and policies, but note that initializing\n",
        "many TF policies will take some time.\n",
        "Also, TF evals might slow down with large numbers of policies. To debug TF\n",
        "execution, set the TF_TIMELINE_DIR environment variable.\n",
        "\"\"\"\n",
        "\n",
        "# rllib rollout ~/ray_results/PPO/PPO_MMenv-v0_0_2019-08-09_00-06-10t4b1lscr/checkpoint-1 --run PPO --env MMenv-v0 --steps 100\n",
        "\n",
        "import os\n",
        "os.environ['RAY_DEBUG_DISABLE_MEMORY_MONITOR'] = \"True\"\n",
        "\n",
        "import argparse\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.models import Model, ModelCatalog\n",
        "\n",
        "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
        "from ray.rllib.models.tf.fcnet_v2 import FullyConnectedNetwork\n",
        "\n",
        "from ray.tune.registry import register_env\n",
        "from ray.rllib.utils import try_import_tf\n",
        "\n",
        "\n",
        "from ray.rllib.policy.policy import Policy\n",
        "#from ray.rllib.policy.tf_policy import TFPolicy\n",
        "#from ray.rllib.policy import Policy\n",
        "\n",
        "\n",
        "#from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
        "\n",
        "\n",
        "import sys\n",
        "if \"../\" not in sys.path:\n",
        "    sys.path.append(\"../\")\n",
        "#from exchg.x.y import z\n",
        "\n",
        "#from envs.continuousDoubleAuction_env import continuousDoubleAuctionEnv\n",
        "from gym_continuousDoubleAuction.envs.continuousDoubleAuction_env import continuousDoubleAuctionEnv\n",
        "\n",
        "tf = try_import_tf()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFWgG3bhiwux",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c21b8fdb-39b0-4377-f3e8-bb2a56f5e528"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--num-agents\", type=int, default=4)\n",
        "parser.add_argument(\"--num-policies\", type=int, default=4)\n",
        "parser.add_argument(\"--num-iters\", type=int, default=10)\n",
        "parser.add_argument(\"--simple\", action=\"store_true\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreTrueAction(option_strings=['--simple'], dest='simple', nargs=0, const=True, default=False, type=None, choices=None, help=None, metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIYT1UTxiQcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomModel_disc(Model):\n",
        "    def _lstm(self, Inputs, cell_size):\n",
        "        s = tf.expand_dims(Inputs, axis=1, name='time_major')  # [time_step, feature] => [time_step, batch, feature]\n",
        "        lstm_cell = tf.nn.rnn_cell.LSTMCell(cell_size)\n",
        "        self.init_state = lstm_cell.zero_state(batch_size=1, dtype=tf.float32)\n",
        "        # time_major means [time_step, batch, feature] while batch major means [batch, time_step, feature]\n",
        "        outputs, self.final_state = tf.nn.dynamic_rnn(cell=lstm_cell, inputs=s, initial_state=self.init_state, time_major=True)\n",
        "        lstm_out = tf.reshape(outputs, [-1, cell_size], name='flatten_rnn_outputs')  # joined state representation\n",
        "        return lstm_out\n",
        "\n",
        "    def _build_layers_v2(self, input_dict, num_outputs, options):\n",
        "        hidden = 512\n",
        "        cell_size = 256\n",
        "        #S = input_dict[\"obs\"]\n",
        "        S = tf.layers.flatten(input_dict[\"obs\"])\n",
        "        with tf.variable_scope(tf.VariableScope(tf.AUTO_REUSE, \"shared\"),\n",
        "                               reuse=tf.AUTO_REUSE,\n",
        "                               auxiliary_name_scope=False):\n",
        "            last_layer = tf.layers.dense(S, hidden, activation=tf.nn.relu, name=\"fc1\")\n",
        "        last_layer = tf.layers.dense(last_layer, hidden, activation=tf.nn.relu, name=\"fc2\")\n",
        "        last_layer = tf.layers.dense(last_layer, hidden, activation=tf.nn.relu, name=\"fc3\")\n",
        "\n",
        "        last_layer = self._lstm(last_layer, cell_size)\n",
        "\n",
        "        output = tf.layers.dense(last_layer, num_outputs, activation=tf.nn.softmax, name=\"mu\")\n",
        "\n",
        "        return output, last_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56uO7LyPMQ4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomModel(TFModelV2):\n",
        "    \"\"\"Example of a custom model that just delegates to a fc-net.\"\"\"\n",
        "\n",
        "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
        "        \n",
        "        super(CustomModel, self).__init__(obs_space, action_space, num_outputs, model_config, name)\n",
        "        \n",
        "        self.model = FullyConnectedNetwork(obs_space, action_space, num_outputs, model_config, name)\n",
        "        #print('obs_space', obs_space)\n",
        "\n",
        "        self.register_variables(self.model.variables())\n",
        "\n",
        "    def forward(self, input_dict, state, seq_lens):\n",
        "        return self.model.forward(input_dict, state, seq_lens)\n",
        "\n",
        "    def value_function(self):\n",
        "        return self.model.value_function()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cijiMv-ti1SK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_RandomPolicy(_seed):\n",
        "\n",
        "    # a hand-coded policy that acts at random in the env (doesn't learn)\n",
        "    class RandomPolicy(Policy):\n",
        "        \"\"\"Hand-coded policy that returns random actions.\"\"\"\n",
        "        def __init__(self, observation_space, action_space, config):\n",
        "            self.observation_space = observation_space\n",
        "            self.action_space = action_space\n",
        "            self.action_space.seed(_seed)\n",
        "\n",
        "        def compute_actions(self,\n",
        "                            obs_batch,\n",
        "                            state_batches,\n",
        "                            prev_action_batch=None,\n",
        "                            prev_reward_batch=None,\n",
        "                            info_batch=None,\n",
        "                            episodes=None,\n",
        "                            **kwargs):\n",
        "            \"\"\"Compute actions on a batch of observations.\"\"\"\n",
        "            return [self.action_space.sample() for _ in obs_batch], [], {}\n",
        "\n",
        "        def learn_on_batch(self, samples):\n",
        "            \"\"\"No learning.\"\"\"\n",
        "            #return {}\n",
        "            pass\n",
        "\n",
        "        def get_weights(self):\n",
        "            pass\n",
        "\n",
        "        def set_weights(self, weights):\n",
        "            pass\n",
        "\n",
        "    return RandomPolicy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7ASMamrPoh3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "807b950e-7c6a-4873-d0f4-20a233ad5b70"
      },
      "source": [
        "#args = parser.parse_args()\n",
        "\n",
        "# set log_to_driver=False to off render output so as to prevent browser from hanging.\n",
        "#ray.init(ignore_reinit_error=True)\n",
        "#ray.init(ignore_reinit_error=True, num_cpus=2)\n",
        "#ray.init(ignore_reinit_error=True, webui_host='127.0.0.1', num_cpus=2)\n",
        "ray.init(ignore_reinit_error=True, log_to_driver=False, webui_host='127.0.0.1', num_cpus=2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-17 05:04:10,109\tWARNING services.py:586 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
            "2020-03-17 05:04:10,112\tINFO resource_spec.py:212 -- Starting Ray with 6.74 GiB memory available for workers and up to 3.37 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
            "2020-03-17 05:04:10,631\tINFO services.py:1078 -- View the Ray dashboard at \u001b[1m\u001b[32m127.0.0.1:8265\u001b[39m\u001b[22m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'node_ip_address': '172.28.0.2',\n",
              " 'object_store_address': '/tmp/ray/session_2020-03-17_05-04-10_108965_615/sockets/plasma_store',\n",
              " 'raylet_socket_name': '/tmp/ray/session_2020-03-17_05-04-10_108965_615/sockets/raylet',\n",
              " 'redis_address': '172.28.0.2:21092',\n",
              " 'session_dir': '/tmp/ray/session_2020-03-17_05-04-10_108965_615',\n",
              " 'webui_url': '127.0.0.1:8265'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqzjVWUsPykm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_agents = 4\n",
        "num_policies = num_agents\n",
        "num_iters = 3\n",
        "simple = False #store_true\n",
        "#num_of_traders = args.num_agents\n",
        "num_of_traders = num_agents\n",
        "tape_display_length = 10 #100\n",
        "tick_size = 1\n",
        "init_cash = 1000000\n",
        "max_step = 700 # per episode #700\n",
        "episode = 5 #9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To995IZanbGx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c844258e-716a-464b-9cb5-faca527ce3a3"
      },
      "source": [
        "single_CDA_env = continuousDoubleAuctionEnv(num_of_traders, init_cash, tick_size, tape_display_length, max_step)\n",
        "obs_space = single_CDA_env.observation_space\n",
        "act_space = single_CDA_env.action_space\n",
        "register_env(\"continuousDoubleAuction-v0\", lambda _: continuousDoubleAuctionEnv(num_of_traders, init_cash, tick_size, tape_display_length, max_step))\n",
        "ModelCatalog.register_custom_model(\"model_disc\", CustomModel_disc)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN5IfMMvP4VA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Each policy can have a different configuration (including custom model)\n",
        "def gen_policy(i):\n",
        "    config = {\"model\": {\"custom_model\": \"model_disc\"},\n",
        "              \"gamma\": 0.99,}\n",
        "    return (None, obs_space, act_space, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7KFjAxGP6en",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "def policy_mapper_0(agent_id):\n",
        "    if agent_id == 0:\n",
        "        return \"policy_0\" # PPO\n",
        "    elif agent_id == 1:\n",
        "        return \"policy_1\" # RandomPolicy\n",
        "    elif agent_id == 2:\n",
        "        return \"policy_2\" # RandomPolicy\n",
        "    else:\n",
        "        return \"policy_3\" # RandomPolicy\n",
        "\"\"\"\n",
        "def policy_mapper(agent_id):\n",
        "    for i in range(num_agents):\n",
        "        if agent_id == i:\n",
        "            return \"policy_{}\".format(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXrTPRQDP8of",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup PPO with an ensemble of `num_policies` different policies\n",
        "\n",
        "# Dictionary of policies\n",
        "#policies = {\"policy_{}\".format(i): gen_policy(i) for i in range(args.num_policies)}\n",
        "policies = {\"policy_{}\".format(i): gen_policy(i) for i in range(num_policies)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsbblOn-P_eA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "fc814191-9139-4bd0-c554-4aa2941dd8f4"
      },
      "source": [
        "# override policy with random policy\n",
        "\"\"\"\n",
        "policies[\"policy_{}\".format(num_policies-3)] = (make_RandomPolicy(1), obs_space, act_space, {}) # random policy stored as the last item in policies dictionary\n",
        "policies[\"policy_{}\".format(num_policies-2)] = (make_RandomPolicy(2), obs_space, act_space, {}) # random policy stored as the last item in policies dictionary\n",
        "policies[\"policy_{}\".format(num_policies-1)] = (make_RandomPolicy(3), obs_space, act_space, {}) # random policy stored as the last item in policies dictionary\n",
        "\"\"\"\n",
        "def set_RandomPolicy(policies):\n",
        "    for i in range(num_agents):\n",
        "        # random policy stored as the last item in policies dictionary\n",
        "        policies[\"policy_{}\".format(num_policies-1)] = (make_RandomPolicy(3), obs_space, act_space, {})\n",
        "\n",
        "    print('policies:', policies)\n",
        "\n",
        "    return 0\n",
        "set_RandomPolicy(policies)\n",
        "\n",
        "policy_ids = list(policies.keys())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "policies: {'policy_0': (None, Box(4, 10), Tuple(Discrete(3), Discrete(4), Box(1,), Box(1,), Discrete(12)), {'model': {'custom_model': 'model_disc'}, 'gamma': 0.99}), 'policy_1': (None, Box(4, 10), Tuple(Discrete(3), Discrete(4), Box(1,), Box(1,), Discrete(12)), {'model': {'custom_model': 'model_disc'}, 'gamma': 0.99}), 'policy_2': (None, Box(4, 10), Tuple(Discrete(3), Discrete(4), Box(1,), Box(1,), Discrete(12)), {'model': {'custom_model': 'model_disc'}, 'gamma': 0.99}), 'policy_3': (<class '__main__.make_RandomPolicy.<locals>.RandomPolicy'>, Box(4, 10), Tuple(Discrete(3), Discrete(4), Box(1,), Box(1,), Discrete(12)), {})}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVr7hhEni7u7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "976828ea-ea50-4229-a1f1-b86cc001870d"
      },
      "source": [
        "tune.run(#PPOTrainer,\n",
        "          \"PPO\",\n",
        "          #\"PG\",\n",
        "          #queue_trials=False,\n",
        "          #resources_per_trial={\"cpu\": 2,\n",
        "          #                     \"gpu\": 0},\n",
        "\n",
        "          #stop={\"training_iteration\": args.num_iters},\n",
        "          #stop={\"timesteps_total\": max_step,\n",
        "          #      \"training_iteration\": num_iters},\n",
        "          stop={\"timesteps_total\": max_step * episode},\n",
        "\n",
        "          config={\"env\": \"continuousDoubleAuction-v0\",\n",
        "\n",
        "                  #\"log_level\": \"DEBUG\",\n",
        "                  #\"simple_optimizer\": args.simple,\n",
        "                  #\"simple_optimizer\": True,\n",
        "                  #\"num_sgd_iter\": 10,\n",
        "\n",
        "                  #\"gamma\": 0.9,\n",
        "\n",
        "                  # Number of rollout worker actors to create for parallel sampling.\n",
        "                  # Setting to 0 will force rollouts to be done in the trainer actor.\n",
        "                  \"num_workers\": 1, # Colab (only 2 CPUs or 1 GPU) #1\n",
        "                  \"num_envs_per_worker\": 2, #2\n",
        "\n",
        "                  #\"timesteps_per_iteration\": max_step,\n",
        "\n",
        "                  # https://github.com/ray-project/ray/issues/4628\n",
        "                  \"sample_batch_size\": 32, # number of environment steps sampled from each environment\n",
        "                  \"train_batch_size\": 128, # minibatch size must be >= 128, number of environment steps sampled from all available environments\n",
        "\n",
        "                  \"multiagent\": {\"policies_to_train\": [\"policy_0\"],\n",
        "                                 \"policies\": policies,\n",
        "                                 #\"policy_mapping_fn\": tune.function(lambda agent_id: random.choice(policy_ids)),\n",
        "                                 #\"policy_mapping_fn\": tune.function(policy_mapper),\n",
        "                                 \"policy_mapping_fn\": policy_mapper,\n",
        "                                },\n",
        "                  },\n",
        "          )\n",
        "\n",
        "#ray.shutdown()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 1.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc  </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>     </td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-05-30\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 20929.984\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.20000000298023224\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.970001220703125\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.010703801177442074\n",
            "        policy_loss: -0.13240456581115723\n",
            "        total_loss: 1153154944.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 1153154944.0\n",
            "    load_time_ms: 306.026\n",
            "    num_steps_sampled: 128\n",
            "    num_steps_trained: 128\n",
            "    sample_time_ms: 5266.336\n",
            "    update_time_ms: 16751.961\n",
            "  iterations_since_restore: 1\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 84.1134328358209\n",
            "    ram_util_percent: 18.004477611940295\n",
            "  pid: 720\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 43.57675242424011\n",
            "  time_this_iter_s: 43.57675242424011\n",
            "  time_total_s: 43.57675242424011\n",
            "  timestamp: 1584421530\n",
            "  timesteps_since_restore: 128\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 128\n",
            "  training_iteration: 1\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         43.5768</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">     1</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-05-54\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 20098.692\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.20000000298023224\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.010251998901367\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.009549417532980442\n",
            "        policy_loss: -0.06775331497192383\n",
            "        total_loss: 63225132.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 63225132.0\n",
            "    load_time_ms: 153.556\n",
            "    num_steps_sampled: 256\n",
            "    num_steps_trained: 256\n",
            "    sample_time_ms: 4749.611\n",
            "    update_time_ms: 8416.306\n",
            "  iterations_since_restore: 2\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.89090909090909\n",
            "    ram_util_percent: 18.303030303030305\n",
            "  pid: 720\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 67.16256380081177\n",
            "  time_this_iter_s: 23.585811376571655\n",
            "  time_total_s: 67.16256380081177\n",
            "  timestamp: 1584421554\n",
            "  timesteps_since_restore: 256\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 256\n",
            "  training_iteration: 2\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.3/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         67.1626</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">     2</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-06-17\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 19651.426\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.20000000298023224\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.971488952636719\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.005780811421573162\n",
            "        policy_loss: -0.04506651684641838\n",
            "        total_loss: 22440618.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 22440618.0\n",
            "    load_time_ms: 102.736\n",
            "    num_steps_sampled: 384\n",
            "    num_steps_trained: 384\n",
            "    sample_time_ms: 4565.041\n",
            "    update_time_ms: 5636.798\n",
            "  iterations_since_restore: 3\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.91818181818182\n",
            "    ram_util_percent: 18.560606060606062\n",
            "  pid: 720\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 90.19918584823608\n",
            "  time_this_iter_s: 23.036622047424316\n",
            "  time_total_s: 90.19918584823608\n",
            "  timestamp: 1584421577\n",
            "  timesteps_since_restore: 384\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 384\n",
            "  training_iteration: 3\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         90.1992</td><td style=\"text-align: right;\"> 384</td><td style=\"text-align: right;\">     3</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-06-41\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 19729.014\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.20000000298023224\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.07459831237793\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.030455516651272774\n",
            "        policy_loss: -0.09129857271909714\n",
            "        total_loss: 4241261.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 4241261.0\n",
            "    load_time_ms: 77.407\n",
            "    num_steps_sampled: 512\n",
            "    num_steps_trained: 512\n",
            "    sample_time_ms: 4586.965\n",
            "    update_time_ms: 4246.445\n",
            "  iterations_since_restore: 4\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.79142857142858\n",
            "    ram_util_percent: 18.799999999999997\n",
            "  pid: 720\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 114.89459562301636\n",
            "  time_this_iter_s: 24.695409774780273\n",
            "  time_total_s: 114.89459562301636\n",
            "  timestamp: 1584421601\n",
            "  timesteps_since_restore: 512\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 512\n",
            "  training_iteration: 4\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         114.895</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">     4</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-07-06\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 19766.487\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.999919891357422\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.017113983631134033\n",
            "        policy_loss: -0.08082030713558197\n",
            "        total_loss: 3353288.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 3353288.0\n",
            "    load_time_ms: 62.15\n",
            "    num_steps_sampled: 640\n",
            "    num_steps_trained: 640\n",
            "    sample_time_ms: 4584.334\n",
            "    update_time_ms: 3413.143\n",
            "  iterations_since_restore: 5\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.22222222222224\n",
            "    ram_util_percent: 18.994444444444444\n",
            "  pid: 720\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 139.47010278701782\n",
            "  time_this_iter_s: 24.575507164001465\n",
            "  time_total_s: 139.47010278701782\n",
            "  timestamp: 1584421626\n",
            "  timesteps_since_restore: 640\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 640\n",
            "  training_iteration: 5\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">          139.47</td><td style=\"text-align: right;\"> 640</td><td style=\"text-align: right;\">     5</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-07-31\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 19814.751\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.021442413330078\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.017843497917056084\n",
            "        policy_loss: -0.09335649013519287\n",
            "        total_loss: 12664294.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 12664294.0\n",
            "    load_time_ms: 51.988\n",
            "    num_steps_sampled: 768\n",
            "    num_steps_trained: 768\n",
            "    sample_time_ms: 4551.167\n",
            "    update_time_ms: 2857.779\n",
            "  iterations_since_restore: 6\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.34857142857143\n",
            "    ram_util_percent: 19.297142857142852\n",
            "  pid: 720\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 163.99796056747437\n",
            "  time_this_iter_s: 24.527857780456543\n",
            "  time_total_s: 163.99796056747437\n",
            "  timestamp: 1584421651\n",
            "  timesteps_since_restore: 768\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 768\n",
            "  training_iteration: 6\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.4/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         163.998</td><td style=\"text-align: right;\"> 768</td><td style=\"text-align: right;\">     6</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-07-55\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 19795.363\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.073933601379395\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.019067661836743355\n",
            "        policy_loss: -0.0828419104218483\n",
            "        total_loss: 6322969.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 6322968.5\n",
            "    load_time_ms: 44.725\n",
            "    num_steps_sampled: 896\n",
            "    num_steps_trained: 896\n",
            "    sample_time_ms: 4538.899\n",
            "    update_time_ms: 2460.881\n",
            "  iterations_since_restore: 7\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.81470588235295\n",
            "    ram_util_percent: 19.5\n",
            "  pid: 720\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 188.2271568775177\n",
            "  time_this_iter_s: 24.229196310043335\n",
            "  time_total_s: 188.2271568775177\n",
            "  timestamp: 1584421675\n",
            "  timesteps_since_restore: 896\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 896\n",
            "  training_iteration: 7\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         188.227</td><td style=\"text-align: right;\"> 896</td><td style=\"text-align: right;\">     7</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-08-19\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 19816.335\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.88787841796875\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.018577799201011658\n",
            "        policy_loss: -0.09292592108249664\n",
            "        total_loss: 5574246.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 5574246.5\n",
            "    load_time_ms: 39.267\n",
            "    num_steps_sampled: 1024\n",
            "    num_steps_trained: 1024\n",
            "    sample_time_ms: 4527.768\n",
            "    update_time_ms: 2163.14\n",
            "  iterations_since_restore: 8\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.25428571428571\n",
            "    ram_util_percent: 19.759999999999998\n",
            "  pid: 720\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 212.72515058517456\n",
            "  time_this_iter_s: 24.49799370765686\n",
            "  time_total_s: 212.72515058517456\n",
            "  timestamp: 1584421699\n",
            "  timesteps_since_restore: 1024\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1024\n",
            "  training_iteration: 8\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         212.725</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">     8</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-08-44\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 19819.613\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.006608009338379\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.01320258341729641\n",
            "        policy_loss: -0.08293458819389343\n",
            "        total_loss: 2415703.75\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 2415703.75\n",
            "    load_time_ms: 35.045\n",
            "    num_steps_sampled: 1152\n",
            "    num_steps_trained: 1152\n",
            "    sample_time_ms: 4535.89\n",
            "    update_time_ms: 1931.223\n",
            "  iterations_since_restore: 9\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.05428571428571\n",
            "    ram_util_percent: 20.0\n",
            "  pid: 720\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 237.25435256958008\n",
            "  time_this_iter_s: 24.529201984405518\n",
            "  time_total_s: 237.25435256958008\n",
            "  timestamp: 1584421724\n",
            "  timesteps_since_restore: 1152\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1152\n",
            "  training_iteration: 9\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         237.254</td><td style=\"text-align: right;\">1152</td><td style=\"text-align: right;\">     9</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n",
            "WARNING:root:NaN or Inf found in input tensor.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-09-09\n",
            "  done: false\n",
            "  episode_len_mean: .nan\n",
            "  episode_reward_max: .nan\n",
            "  episode_reward_mean: .nan\n",
            "  episode_reward_min: .nan\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 0\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 19850.278\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.98395299911499\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.009151849895715714\n",
            "        policy_loss: -0.04943016916513443\n",
            "        total_loss: 3727455.5\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 3727455.5\n",
            "    load_time_ms: 31.651\n",
            "    num_steps_sampled: 1280\n",
            "    num_steps_trained: 1280\n",
            "    sample_time_ms: 4545.49\n",
            "    update_time_ms: 1745.597\n",
            "  iterations_since_restore: 10\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.08333333333333\n",
            "    ram_util_percent: 20.202777777777783\n",
            "  pid: 720\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf: {}\n",
            "  time_since_restore: 262.0958249568939\n",
            "  time_this_iter_s: 24.841472387313843\n",
            "  time_total_s: 262.0958249568939\n",
            "  timestamp: 1584421749\n",
            "  timesteps_since_restore: 1280\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1280\n",
            "  training_iteration: 10\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.6/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">         262.096</td><td style=\"text-align: right;\">1280</td><td style=\"text-align: right;\">    10</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-09-33\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 2\n",
            "  episodes_total: 2\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 19736.049\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.024125099182129\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.015276610851287842\n",
            "        policy_loss: -0.08783513307571411\n",
            "        total_loss: 4321638.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 4321638.0\n",
            "    load_time_ms: 1.165\n",
            "    num_steps_sampled: 1408\n",
            "    num_steps_trained: 1408\n",
            "    sample_time_ms: 4467.438\n",
            "    update_time_ms: 78.912\n",
            "  iterations_since_restore: 11\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.20285714285716\n",
            "    ram_util_percent: 20.5\n",
            "  pid: 720\n",
            "  policy_reward_max:\n",
            "    policy_0: 71261.0\n",
            "    policy_1: -12490.0\n",
            "    policy_2: -7768.0\n",
            "    policy_3: -24569.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 63950.0\n",
            "    policy_1: -12585.5\n",
            "    policy_2: -13578.5\n",
            "    policy_3: -37786.0\n",
            "  policy_reward_min:\n",
            "    policy_0: 56639.0\n",
            "    policy_1: -12681.0\n",
            "    policy_2: -19389.0\n",
            "    policy_3: -51003.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 50.94624343493306\n",
            "    mean_inference_ms: 16.243108789971533\n",
            "    mean_processing_ms: 2.1882459626975637\n",
            "  time_since_restore: 286.46058988571167\n",
            "  time_this_iter_s: 24.36476492881775\n",
            "  time_total_s: 286.46058988571167\n",
            "  timestamp: 1584421773\n",
            "  timesteps_since_restore: 1408\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1408\n",
            "  training_iteration: 11\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.6/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         286.461</td><td style=\"text-align: right;\">1408</td><td style=\"text-align: right;\">    11</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-09-57\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 19779.014\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.94110107421875\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.027436088770627975\n",
            "        policy_loss: -0.13577800989151\n",
            "        total_loss: 97487120.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 97487120.0\n",
            "    load_time_ms: 1.169\n",
            "    num_steps_sampled: 1536\n",
            "    num_steps_trained: 1536\n",
            "    sample_time_ms: 4442.5\n",
            "    update_time_ms: 79.117\n",
            "  iterations_since_restore: 12\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.6\n",
            "    ram_util_percent: 20.700000000000003\n",
            "  pid: 720\n",
            "  policy_reward_max:\n",
            "    policy_0: 71261.0\n",
            "    policy_1: -12490.0\n",
            "    policy_2: -7768.0\n",
            "    policy_3: -24569.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 63950.0\n",
            "    policy_1: -12585.5\n",
            "    policy_2: -13578.5\n",
            "    policy_3: -37786.0\n",
            "  policy_reward_min:\n",
            "    policy_0: 56639.0\n",
            "    policy_1: -12681.0\n",
            "    policy_2: -19389.0\n",
            "    policy_3: -51003.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 50.94624343493306\n",
            "    mean_inference_ms: 16.243108789971533\n",
            "    mean_processing_ms: 2.1882459626975637\n",
            "  time_since_restore: 310.2300488948822\n",
            "  time_this_iter_s: 23.769459009170532\n",
            "  time_total_s: 310.2300488948822\n",
            "  timestamp: 1584421797\n",
            "  timesteps_since_restore: 1536\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1536\n",
            "  training_iteration: 12\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.6/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">          310.23</td><td style=\"text-align: right;\">1536</td><td style=\"text-align: right;\">    12</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-10-21\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 19839.634\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.95182991027832\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.018878493458032608\n",
            "        policy_loss: -0.06517640501260757\n",
            "        total_loss: 69856304.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 69856304.0\n",
            "    load_time_ms: 1.206\n",
            "    num_steps_sampled: 1664\n",
            "    num_steps_trained: 1664\n",
            "    sample_time_ms: 4449.497\n",
            "    update_time_ms: 78.857\n",
            "  iterations_since_restore: 13\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.28484848484848\n",
            "    ram_util_percent: 20.996969696969696\n",
            "  pid: 720\n",
            "  policy_reward_max:\n",
            "    policy_0: 71261.0\n",
            "    policy_1: -12490.0\n",
            "    policy_2: -7768.0\n",
            "    policy_3: -24569.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 63950.0\n",
            "    policy_1: -12585.5\n",
            "    policy_2: -13578.5\n",
            "    policy_3: -37786.0\n",
            "  policy_reward_min:\n",
            "    policy_0: 56639.0\n",
            "    policy_1: -12681.0\n",
            "    policy_2: -19389.0\n",
            "    policy_3: -51003.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 50.94624343493306\n",
            "    mean_inference_ms: 16.243108789971533\n",
            "    mean_processing_ms: 2.1882459626975637\n",
            "  time_since_restore: 333.9399185180664\n",
            "  time_this_iter_s: 23.709869623184204\n",
            "  time_total_s: 333.9399185180664\n",
            "  timestamp: 1584421821\n",
            "  timesteps_since_restore: 1664\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1664\n",
            "  training_iteration: 13\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">          333.94</td><td style=\"text-align: right;\">1664</td><td style=\"text-align: right;\">    13</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-10-45\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 19809.316\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.983755111694336\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.01056887861341238\n",
            "        policy_loss: -0.0629388689994812\n",
            "        total_loss: 18729040.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 18729040.0\n",
            "    load_time_ms: 1.193\n",
            "    num_steps_sampled: 1792\n",
            "    num_steps_trained: 1792\n",
            "    sample_time_ms: 4430.004\n",
            "    update_time_ms: 80.501\n",
            "  iterations_since_restore: 14\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.50857142857143\n",
            "    ram_util_percent: 21.19428571428572\n",
            "  pid: 720\n",
            "  policy_reward_max:\n",
            "    policy_0: 71261.0\n",
            "    policy_1: -12490.0\n",
            "    policy_2: -7768.0\n",
            "    policy_3: -24569.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 63950.0\n",
            "    policy_1: -12585.5\n",
            "    policy_2: -13578.5\n",
            "    policy_3: -37786.0\n",
            "  policy_reward_min:\n",
            "    policy_0: 56639.0\n",
            "    policy_1: -12681.0\n",
            "    policy_2: -19389.0\n",
            "    policy_3: -51003.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 50.94624343493306\n",
            "    mean_inference_ms: 16.243108789971533\n",
            "    mean_processing_ms: 2.1882459626975637\n",
            "  time_since_restore: 358.1542727947235\n",
            "  time_this_iter_s: 24.214354276657104\n",
            "  time_total_s: 358.1542727947235\n",
            "  timestamp: 1584421845\n",
            "  timesteps_since_restore: 1792\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1792\n",
            "  training_iteration: 14\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         358.154</td><td style=\"text-align: right;\">1792</td><td style=\"text-align: right;\">    14</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-11-10\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 19802.648\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.016807556152344\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.011928904801607132\n",
            "        policy_loss: -0.07155415415763855\n",
            "        total_loss: 16371816.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 16371816.0\n",
            "    load_time_ms: 1.212\n",
            "    num_steps_sampled: 1920\n",
            "    num_steps_trained: 1920\n",
            "    sample_time_ms: 4420.771\n",
            "    update_time_ms: 81.47\n",
            "  iterations_since_restore: 15\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.99714285714288\n",
            "    ram_util_percent: 21.45142857142857\n",
            "  pid: 720\n",
            "  policy_reward_max:\n",
            "    policy_0: 71261.0\n",
            "    policy_1: -12490.0\n",
            "    policy_2: -7768.0\n",
            "    policy_3: -24569.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 63950.0\n",
            "    policy_1: -12585.5\n",
            "    policy_2: -13578.5\n",
            "    policy_3: -37786.0\n",
            "  policy_reward_min:\n",
            "    policy_0: 56639.0\n",
            "    policy_1: -12681.0\n",
            "    policy_2: -19389.0\n",
            "    policy_3: -51003.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 50.94624343493306\n",
            "    mean_inference_ms: 16.243108789971533\n",
            "    mean_processing_ms: 2.1882459626975637\n",
            "  time_since_restore: 382.583847284317\n",
            "  time_this_iter_s: 24.429574489593506\n",
            "  time_total_s: 382.583847284317\n",
            "  timestamp: 1584421870\n",
            "  timesteps_since_restore: 1920\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 1920\n",
            "  training_iteration: 15\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         382.584</td><td style=\"text-align: right;\">1920</td><td style=\"text-align: right;\">    15</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-11-34\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 19793.874\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.143149375915527\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.019928384572267532\n",
            "        policy_loss: -0.06745973974466324\n",
            "        total_loss: 3077823.5\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 3077823.5\n",
            "    load_time_ms: 1.204\n",
            "    num_steps_sampled: 2048\n",
            "    num_steps_trained: 2048\n",
            "    sample_time_ms: 4435.623\n",
            "    update_time_ms: 81.272\n",
            "  iterations_since_restore: 16\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.99428571428572\n",
            "    ram_util_percent: 21.700000000000003\n",
            "  pid: 720\n",
            "  policy_reward_max:\n",
            "    policy_0: 71261.0\n",
            "    policy_1: -12490.0\n",
            "    policy_2: -7768.0\n",
            "    policy_3: -24569.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 63950.0\n",
            "    policy_1: -12585.5\n",
            "    policy_2: -13578.5\n",
            "    policy_3: -37786.0\n",
            "  policy_reward_min:\n",
            "    policy_0: 56639.0\n",
            "    policy_1: -12681.0\n",
            "    policy_2: -19389.0\n",
            "    policy_3: -51003.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 50.94624343493306\n",
            "    mean_inference_ms: 16.243108789971533\n",
            "    mean_processing_ms: 2.1882459626975637\n",
            "  time_since_restore: 407.1720633506775\n",
            "  time_this_iter_s: 24.588216066360474\n",
            "  time_total_s: 407.1720633506775\n",
            "  timestamp: 1584421894\n",
            "  timesteps_since_restore: 2048\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2048\n",
            "  training_iteration: 16\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         407.172</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">    16</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-11-59\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 19810.344\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.071861267089844\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.016532285138964653\n",
            "        policy_loss: -0.08468527346849442\n",
            "        total_loss: 483737.125\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 483737.1875\n",
            "    load_time_ms: 1.216\n",
            "    num_steps_sampled: 2176\n",
            "    num_steps_trained: 2176\n",
            "    sample_time_ms: 4447.444\n",
            "    update_time_ms: 80.921\n",
            "  iterations_since_restore: 17\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.89714285714285\n",
            "    ram_util_percent: 21.899999999999995\n",
            "  pid: 720\n",
            "  policy_reward_max:\n",
            "    policy_0: 71261.0\n",
            "    policy_1: -12490.0\n",
            "    policy_2: -7768.0\n",
            "    policy_3: -24569.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 63950.0\n",
            "    policy_1: -12585.5\n",
            "    policy_2: -13578.5\n",
            "    policy_3: -37786.0\n",
            "  policy_reward_min:\n",
            "    policy_0: 56639.0\n",
            "    policy_1: -12681.0\n",
            "    policy_2: -19389.0\n",
            "    policy_3: -51003.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 50.94624343493306\n",
            "    mean_inference_ms: 16.243108789971533\n",
            "    mean_processing_ms: 2.1882459626975637\n",
            "  time_since_restore: 431.68201899528503\n",
            "  time_this_iter_s: 24.509955644607544\n",
            "  time_total_s: 431.68201899528503\n",
            "  timestamp: 1584421919\n",
            "  timesteps_since_restore: 2176\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2176\n",
            "  training_iteration: 17\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         431.682</td><td style=\"text-align: right;\">2176</td><td style=\"text-align: right;\">    17</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-12-23\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 19802.847\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.090469360351562\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.012462864629924297\n",
            "        policy_loss: -0.06576329469680786\n",
            "        total_loss: 789475.875\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 789475.9375\n",
            "    load_time_ms: 1.261\n",
            "    num_steps_sampled: 2304\n",
            "    num_steps_trained: 2304\n",
            "    sample_time_ms: 4473.351\n",
            "    update_time_ms: 81.055\n",
            "  iterations_since_restore: 18\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.76857142857143\n",
            "    ram_util_percent: 22.200000000000003\n",
            "  pid: 720\n",
            "  policy_reward_max:\n",
            "    policy_0: 71261.0\n",
            "    policy_1: -12490.0\n",
            "    policy_2: -7768.0\n",
            "    policy_3: -24569.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 63950.0\n",
            "    policy_1: -12585.5\n",
            "    policy_2: -13578.5\n",
            "    policy_3: -37786.0\n",
            "  policy_reward_min:\n",
            "    policy_0: 56639.0\n",
            "    policy_1: -12681.0\n",
            "    policy_2: -19389.0\n",
            "    policy_3: -51003.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 50.94624343493306\n",
            "    mean_inference_ms: 16.243108789971533\n",
            "    mean_processing_ms: 2.1882459626975637\n",
            "  time_since_restore: 456.3654727935791\n",
            "  time_this_iter_s: 24.683453798294067\n",
            "  time_total_s: 456.3654727935791\n",
            "  timestamp: 1584421943\n",
            "  timesteps_since_restore: 2304\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2304\n",
            "  training_iteration: 18\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         456.365</td><td style=\"text-align: right;\">2304</td><td style=\"text-align: right;\">    18</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-12-48\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 19794.593\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.148468017578125\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.02006814256310463\n",
            "        policy_loss: -0.07475955784320831\n",
            "        total_loss: 624016.75\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 624016.875\n",
            "    load_time_ms: 1.26\n",
            "    num_steps_sampled: 2432\n",
            "    num_steps_trained: 2432\n",
            "    sample_time_ms: 4474.093\n",
            "    update_time_ms: 82.865\n",
            "  iterations_since_restore: 19\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.9457142857143\n",
            "    ram_util_percent: 22.47714285714286\n",
            "  pid: 720\n",
            "  policy_reward_max:\n",
            "    policy_0: 71261.0\n",
            "    policy_1: -12490.0\n",
            "    policy_2: -7768.0\n",
            "    policy_3: -24569.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 63950.0\n",
            "    policy_1: -12585.5\n",
            "    policy_2: -13578.5\n",
            "    policy_3: -37786.0\n",
            "  policy_reward_min:\n",
            "    policy_0: 56639.0\n",
            "    policy_1: -12681.0\n",
            "    policy_2: -19389.0\n",
            "    policy_3: -51003.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 50.94624343493306\n",
            "    mean_inference_ms: 16.243108789971533\n",
            "    mean_processing_ms: 2.1882459626975637\n",
            "  time_since_restore: 480.8375635147095\n",
            "  time_this_iter_s: 24.47209072113037\n",
            "  time_total_s: 480.8375635147095\n",
            "  timestamp: 1584421968\n",
            "  timesteps_since_restore: 2432\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2432\n",
            "  training_iteration: 19\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.9/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         480.838</td><td style=\"text-align: right;\">2432</td><td style=\"text-align: right;\">    19</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-13-12\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 19746.386\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.17591667175293\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.010880311019718647\n",
            "        policy_loss: -0.04697229713201523\n",
            "        total_loss: 135921.09375\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 135921.125\n",
            "    load_time_ms: 1.288\n",
            "    num_steps_sampled: 2560\n",
            "    num_steps_trained: 2560\n",
            "    sample_time_ms: 4473.263\n",
            "    update_time_ms: 83.95\n",
            "  iterations_since_restore: 20\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.88857142857141\n",
            "    ram_util_percent: 22.700000000000003\n",
            "  pid: 720\n",
            "  policy_reward_max:\n",
            "    policy_0: 71261.0\n",
            "    policy_1: -12490.0\n",
            "    policy_2: -7768.0\n",
            "    policy_3: -24569.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 63950.0\n",
            "    policy_1: -12585.5\n",
            "    policy_2: -13578.5\n",
            "    policy_3: -37786.0\n",
            "  policy_reward_min:\n",
            "    policy_0: 56639.0\n",
            "    policy_1: -12681.0\n",
            "    policy_2: -19389.0\n",
            "    policy_3: -51003.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 50.94624343493306\n",
            "    mean_inference_ms: 16.243108789971533\n",
            "    mean_processing_ms: 2.1882459626975637\n",
            "  time_since_restore: 505.19665813446045\n",
            "  time_this_iter_s: 24.359094619750977\n",
            "  time_total_s: 505.19665813446045\n",
            "  timestamp: 1584421992\n",
            "  timesteps_since_restore: 2560\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2560\n",
            "  training_iteration: 20\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.9/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         505.197</td><td style=\"text-align: right;\">2560</td><td style=\"text-align: right;\">    20</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-13-37\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 2\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 19770.966\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.107719421386719\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.010819962248206139\n",
            "        policy_loss: -0.04770096018910408\n",
            "        total_loss: 289174.96875\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 289175.03125\n",
            "    load_time_ms: 1.282\n",
            "    num_steps_sampled: 2688\n",
            "    num_steps_trained: 2688\n",
            "    sample_time_ms: 4483.316\n",
            "    update_time_ms: 83.522\n",
            "  iterations_since_restore: 21\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.76857142857143\n",
            "    ram_util_percent: 22.997142857142858\n",
            "  pid: 720\n",
            "  policy_reward_max:\n",
            "    policy_0: 71261.0\n",
            "    policy_1: -12490.0\n",
            "    policy_2: -7768.0\n",
            "    policy_3: -24569.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 63950.0\n",
            "    policy_1: -12585.5\n",
            "    policy_2: -13578.5\n",
            "    policy_3: -37786.0\n",
            "  policy_reward_min:\n",
            "    policy_0: 56639.0\n",
            "    policy_1: -12681.0\n",
            "    policy_2: -19389.0\n",
            "    policy_3: -51003.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 50.94624343493306\n",
            "    mean_inference_ms: 16.243108789971533\n",
            "    mean_processing_ms: 2.1882459626975637\n",
            "  time_since_restore: 529.9034740924835\n",
            "  time_this_iter_s: 24.70681595802307\n",
            "  time_total_s: 529.9034740924835\n",
            "  timestamp: 1584422017\n",
            "  timesteps_since_restore: 2688\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2688\n",
            "  training_iteration: 21\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.9/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         529.903</td><td style=\"text-align: right;\">2688</td><td style=\"text-align: right;\">    21</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-14-02\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 2\n",
            "  episodes_total: 4\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 19794.615\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.144630432128906\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.013859033584594727\n",
            "        policy_loss: -0.0799495056271553\n",
            "        total_loss: 1498313.5\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 1498313.5\n",
            "    load_time_ms: 1.337\n",
            "    num_steps_sampled: 2816\n",
            "    num_steps_trained: 2816\n",
            "    sample_time_ms: 4530.924\n",
            "    update_time_ms: 83.544\n",
            "  iterations_since_restore: 22\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.12\n",
            "    ram_util_percent: 23.19428571428572\n",
            "  pid: 720\n",
            "  policy_reward_max:\n",
            "    policy_0: 71261.0\n",
            "    policy_1: 10943.0\n",
            "    policy_2: -787.0\n",
            "    policy_3: 10255.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 30510.5\n",
            "    policy_1: -3903.75\n",
            "    policy_2: -9884.25\n",
            "    policy_3: -16722.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -9605.0\n",
            "    policy_1: -12681.0\n",
            "    policy_2: -19389.0\n",
            "    policy_3: -51003.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 51.21529296642299\n",
            "    mean_inference_ms: 15.968142329405332\n",
            "    mean_processing_ms: 2.183122515695807\n",
            "  time_since_restore: 554.3864591121674\n",
            "  time_this_iter_s: 24.482985019683838\n",
            "  time_total_s: 554.3864591121674\n",
            "  timestamp: 1584422042\n",
            "  timesteps_since_restore: 2816\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2816\n",
            "  training_iteration: 22\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         554.386</td><td style=\"text-align: right;\">2816</td><td style=\"text-align: right;\">    22</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-14-26\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 19891.229\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.013494491577148\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.012691779062151909\n",
            "        policy_loss: -0.12156576663255692\n",
            "        total_loss: 138856112.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 138856112.0\n",
            "    load_time_ms: 1.37\n",
            "    num_steps_sampled: 2944\n",
            "    num_steps_trained: 2944\n",
            "    sample_time_ms: 4517.974\n",
            "    update_time_ms: 84.541\n",
            "  iterations_since_restore: 23\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.78333333333335\n",
            "    ram_util_percent: 23.477777777777778\n",
            "  pid: 720\n",
            "  policy_reward_max:\n",
            "    policy_0: 71261.0\n",
            "    policy_1: 10943.0\n",
            "    policy_2: -787.0\n",
            "    policy_3: 10255.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 30510.5\n",
            "    policy_1: -3903.75\n",
            "    policy_2: -9884.25\n",
            "    policy_3: -16722.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -9605.0\n",
            "    policy_1: -12681.0\n",
            "    policy_2: -19389.0\n",
            "    policy_3: -51003.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 51.215292966423\n",
            "    mean_inference_ms: 15.968142329405332\n",
            "    mean_processing_ms: 2.183122515695807\n",
            "  time_since_restore: 578.9440579414368\n",
            "  time_this_iter_s: 24.55759882926941\n",
            "  time_total_s: 578.9440579414368\n",
            "  timestamp: 1584422066\n",
            "  timesteps_since_restore: 2944\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 2944\n",
            "  training_iteration: 23\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         578.944</td><td style=\"text-align: right;\">2944</td><td style=\"text-align: right;\">    23</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-14-51\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 19979.244\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.9758148193359375\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.012799081392586231\n",
            "        policy_loss: -0.0886935442686081\n",
            "        total_loss: 94804272.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 94804272.0\n",
            "    load_time_ms: 1.372\n",
            "    num_steps_sampled: 3072\n",
            "    num_steps_trained: 3072\n",
            "    sample_time_ms: 4500.915\n",
            "    update_time_ms: 83.143\n",
            "  iterations_since_restore: 24\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.39142857142856\n",
            "    ram_util_percent: 23.700000000000003\n",
            "  pid: 720\n",
            "  policy_reward_max:\n",
            "    policy_0: 71261.0\n",
            "    policy_1: 10943.0\n",
            "    policy_2: -787.0\n",
            "    policy_3: 10255.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 30510.5\n",
            "    policy_1: -3903.75\n",
            "    policy_2: -9884.25\n",
            "    policy_3: -16722.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -9605.0\n",
            "    policy_1: -12681.0\n",
            "    policy_2: -19389.0\n",
            "    policy_3: -51003.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 51.215292966423\n",
            "    mean_inference_ms: 15.968142329405332\n",
            "    mean_processing_ms: 2.183122515695807\n",
            "  time_since_restore: 603.854282617569\n",
            "  time_this_iter_s: 24.910224676132202\n",
            "  time_total_s: 603.854282617569\n",
            "  timestamp: 1584422091\n",
            "  timesteps_since_restore: 3072\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3072\n",
            "  training_iteration: 24\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         603.854</td><td style=\"text-align: right;\">3072</td><td style=\"text-align: right;\">    24</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-15-16\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 20035.483\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.988360404968262\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.013073083013296127\n",
            "        policy_loss: -0.08284562826156616\n",
            "        total_loss: 59653360.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 59653360.0\n",
            "    load_time_ms: 1.385\n",
            "    num_steps_sampled: 3200\n",
            "    num_steps_trained: 3200\n",
            "    sample_time_ms: 4504.737\n",
            "    update_time_ms: 82.625\n",
            "  iterations_since_restore: 25\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.29444444444444\n",
            "    ram_util_percent: 23.98888888888889\n",
            "  pid: 720\n",
            "  policy_reward_max:\n",
            "    policy_0: 71261.0\n",
            "    policy_1: 10943.0\n",
            "    policy_2: -787.0\n",
            "    policy_3: 10255.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 30510.5\n",
            "    policy_1: -3903.75\n",
            "    policy_2: -9884.25\n",
            "    policy_3: -16722.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -9605.0\n",
            "    policy_1: -12681.0\n",
            "    policy_2: -19389.0\n",
            "    policy_3: -51003.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 51.215292966423\n",
            "    mean_inference_ms: 15.968142329405332\n",
            "    mean_processing_ms: 2.183122515695807\n",
            "  time_since_restore: 628.8769111633301\n",
            "  time_this_iter_s: 25.02262854576111\n",
            "  time_total_s: 628.8769111633301\n",
            "  timestamp: 1584422116\n",
            "  timesteps_since_restore: 3200\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3200\n",
            "  training_iteration: 25\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         628.877</td><td style=\"text-align: right;\">3200</td><td style=\"text-align: right;\">    25</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-15-41\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 20085.392\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.940023422241211\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.010367561131715775\n",
            "        policy_loss: -0.0577772818505764\n",
            "        total_loss: 26627506.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 26627508.0\n",
            "    load_time_ms: 1.392\n",
            "    num_steps_sampled: 3328\n",
            "    num_steps_trained: 3328\n",
            "    sample_time_ms: 4505.894\n",
            "    update_time_ms: 82.863\n",
            "  iterations_since_restore: 26\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.27222222222223\n",
            "    ram_util_percent: 24.200000000000003\n",
            "  pid: 720\n",
            "  policy_reward_max:\n",
            "    policy_0: 71261.0\n",
            "    policy_1: 10943.0\n",
            "    policy_2: -787.0\n",
            "    policy_3: 10255.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 30510.5\n",
            "    policy_1: -3903.75\n",
            "    policy_2: -9884.25\n",
            "    policy_3: -16722.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -9605.0\n",
            "    policy_1: -12681.0\n",
            "    policy_2: -19389.0\n",
            "    policy_3: -51003.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 51.215292966423\n",
            "    mean_inference_ms: 15.968142329405332\n",
            "    mean_processing_ms: 2.183122515695807\n",
            "  time_since_restore: 653.9776451587677\n",
            "  time_this_iter_s: 25.100733995437622\n",
            "  time_total_s: 653.9776451587677\n",
            "  timestamp: 1584422141\n",
            "  timesteps_since_restore: 3328\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3328\n",
            "  training_iteration: 26\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.1/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         653.978</td><td style=\"text-align: right;\">3328</td><td style=\"text-align: right;\">    26</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-16-07\n",
            "  done: false\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 20151.049\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 7.999573707580566\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.011779390275478363\n",
            "        policy_loss: -0.05411304533481598\n",
            "        total_loss: 19845692.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 19845692.0\n",
            "    load_time_ms: 1.4\n",
            "    num_steps_sampled: 3456\n",
            "    num_steps_trained: 3456\n",
            "    sample_time_ms: 4536.859\n",
            "    update_time_ms: 83.508\n",
            "  iterations_since_restore: 27\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.63055555555553\n",
            "    ram_util_percent: 24.475\n",
            "  pid: 720\n",
            "  policy_reward_max:\n",
            "    policy_0: 71261.0\n",
            "    policy_1: 10943.0\n",
            "    policy_2: -787.0\n",
            "    policy_3: 10255.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 30510.5\n",
            "    policy_1: -3903.75\n",
            "    policy_2: -9884.25\n",
            "    policy_3: -16722.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -9605.0\n",
            "    policy_1: -12681.0\n",
            "    policy_2: -19389.0\n",
            "    policy_3: -51003.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 51.215292966423\n",
            "    mean_inference_ms: 15.968142329405332\n",
            "    mean_processing_ms: 2.183122515695807\n",
            "  time_since_restore: 679.4594573974609\n",
            "  time_this_iter_s: 25.481812238693237\n",
            "  time_total_s: 679.4594573974609\n",
            "  timestamp: 1584422167\n",
            "  timesteps_since_restore: 3456\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3456\n",
            "  training_iteration: 27\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.1/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>RUNNING </td><td>172.28.0.2:720</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         679.459</td><td style=\"text-align: right;\">3456</td><td style=\"text-align: right;\">    27</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Result for PPO_continuousDoubleAuction-v0_bd9f9e26:\n",
            "  custom_metrics: {}\n",
            "  date: 2020-03-17_05-16-32\n",
            "  done: true\n",
            "  episode_len_mean: 701.0\n",
            "  episode_reward_max: 0.0\n",
            "  episode_reward_mean: 0.0\n",
            "  episode_reward_min: 0.0\n",
            "  episodes_this_iter: 0\n",
            "  episodes_total: 4\n",
            "  experiment_id: 97e8948c687044d18d08130dbcb41749\n",
            "  experiment_tag: '0'\n",
            "  hostname: 01ebcde0b5fb\n",
            "  info:\n",
            "    grad_time_ms: 20202.196\n",
            "    learner:\n",
            "      policy_0:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 8.040399551391602\n",
            "        entropy_coeff: 0.0\n",
            "        kl: 0.019970867782831192\n",
            "        policy_loss: -0.07380717992782593\n",
            "        total_loss: 7107307.0\n",
            "        vf_explained_var: 0.0\n",
            "        vf_loss: 7107307.0\n",
            "    load_time_ms: 1.369\n",
            "    num_steps_sampled: 3584\n",
            "    num_steps_trained: 3584\n",
            "    sample_time_ms: 4536.624\n",
            "    update_time_ms: 83.403\n",
            "  iterations_since_restore: 28\n",
            "  node_ip: 172.28.0.2\n",
            "  num_healthy_workers: 1\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.15277777777777\n",
            "    ram_util_percent: 24.700000000000003\n",
            "  pid: 720\n",
            "  policy_reward_max:\n",
            "    policy_0: 71261.0\n",
            "    policy_1: 10943.0\n",
            "    policy_2: -787.0\n",
            "    policy_3: 10255.0\n",
            "  policy_reward_mean:\n",
            "    policy_0: 30510.5\n",
            "    policy_1: -3903.75\n",
            "    policy_2: -9884.25\n",
            "    policy_3: -16722.5\n",
            "  policy_reward_min:\n",
            "    policy_0: -9605.0\n",
            "    policy_1: -12681.0\n",
            "    policy_2: -19389.0\n",
            "    policy_3: -51003.0\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 51.215292966423\n",
            "    mean_inference_ms: 15.968142329405332\n",
            "    mean_processing_ms: 2.183122515695807\n",
            "  time_since_restore: 704.6514701843262\n",
            "  time_this_iter_s: 25.192012786865234\n",
            "  time_total_s: 704.6514701843262\n",
            "  timestamp: 1584422192\n",
            "  timesteps_since_restore: 3584\n",
            "  timesteps_this_iter: 128\n",
            "  timesteps_total: 3584\n",
            "  training_iteration: 28\n",
            "  trial_id: bd9f9e26\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.1/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         704.651</td><td style=\"text-align: right;\">3584</td><td style=\"text-align: right;\">    28</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.1/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/2 CPUs, 0/0 GPUs, 0.0/6.74 GiB heap, 0.0/2.29 GiB objects<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                             </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  iter</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_continuousDoubleAuction-v0_bd9f9e26</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">         704.651</td><td style=\"text-align: right;\">3584</td><td style=\"text-align: right;\">    28</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2020-03-17 05:16:32,974\tINFO tune.py:352 -- Returning an analysis object by default. You can call `analysis.trials` to retrieve a list of trials. This message will be removed in future versions of Tune.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f67cf666c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}